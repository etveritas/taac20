{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if used pip install package\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install wget\n",
    "# !pip install gensim\n",
    "# !pip install catboost\n",
    "# !pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import wget\n",
    "import time\n",
    "import tarfile\n",
    "import zipfile\n",
    "import functools\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from itertools import product, combinations\n",
    "from scipy.special import comb, perm\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cbt\n",
    "from glove import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression,BayesianRidge,SGDClassifier,PassiveAggressiveClassifier,RidgeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC,NuSVC,SVC\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, LSTM, GRU  #, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import gensim\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# ray.init(object_store_memory=int(100e6))\n",
    "# import modin.pandas as pd\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, filename='default.log', stream=sys.stdout):\n",
    "        self.terminal = stream\n",
    "        self.log = open(filename, 'a')\n",
    "        \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        \n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.stdout = Logger(\"logs/default.log\", sys.stdout)\n",
    "# sys.stderr = Logger(\"logs/default_err.log\", sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDIR = \"data\"\n",
    "UDDIR = \"user_data\"\n",
    "UFEDIR = \"user_data/feat_data_v05\"\n",
    "UMDIR = \"user_data/model_data\"\n",
    "RESDIR = \"prediction_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UID = \"user_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (Only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/train_preliminary.zip\", out=DDIR)\n",
    "# test_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/test.zip\", out=DDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myunzip(filename):\n",
    "#     zFile = zipfile.ZipFile(filename, \"r\")\n",
    "#     for fileM in zFile.namelist(): \n",
    "#         zFile.extract(fileM, DDIR)\n",
    "#         print(fileM)\n",
    "#     zFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myunzip(train_fname)\n",
    "# myunzip(test_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bch_rencol(values, prefix=\"\", suffix=\"\"):\n",
    "    return list(map(lambda x: f\"{prefix}\"+\"_\".join(list(map(lambda y: str(y), x)))+f\"{suffix}\" \n",
    "                    if hasattr(x, \"__iter__\") and not isinstance(x, str) \n",
    "                    else f\"{prefix}\"+str(x)+f\"{suffix}\", values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynunique(values):\n",
    "    return values.nunique(dropna=False)\n",
    "def getidxmax(x):\n",
    "    return x.idxmax()[1]\n",
    "# for time series\n",
    "def at_len(x):\n",
    "    return len(x)\n",
    "\n",
    "def at_sum(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def at_max(x):\n",
    "    return np.max(x)\n",
    "\n",
    "def at_min(x):\n",
    "    return np.min(x)\n",
    "\n",
    "def at_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def at_range(x):\n",
    "    return at_max(x) - at_min(x)\n",
    "\n",
    "def at_nunq(x):\n",
    "    return len(set(x))\n",
    "\n",
    "def at_lenDrange(x):\n",
    "    return at_len(x)/(at_range(x)+1)\n",
    "\n",
    "def at_lenDnunq(x):\n",
    "    return at_len(x)/at_nunq(x)\n",
    "\n",
    "def at_percentile(n):\n",
    "    def at_percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    at_percentile_.__name__ = f\"at_percentile_{n}\"\n",
    "    return at_percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_SET = [\"sum\", \"max\", \"min\", \"mean\", \"std\"]\n",
    "OP_SET1 = [\"nunique\", \"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"skew\", at_percentile(0.25), at_percentile(0.75)]\n",
    "OP_SET2 = [\"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"skew\", at_percentile(0.25), at_percentile(0.75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_SET = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\", \"time\", \"click_times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting_level = 0\n",
    "is_start = None\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        self.history = [self.start]\n",
    "\n",
    "    def check(self, info):\n",
    "        current = time.time()\n",
    "        print(f\"[{info}] spend {current - self.history[-1]:0.2f} sec\")\n",
    "        self.history.append(current)\n",
    "\n",
    "def log(entry):\n",
    "    global nesting_level\n",
    "    space = \"-\" * (4 * nesting_level)\n",
    "    print(f\"{space}{entry}\")\n",
    "\n",
    "def timeit(method, start_log=None):\n",
    "    @functools.wraps(method)\n",
    "    def timed(*args, **kw):\n",
    "        global is_start\n",
    "        global nesting_level\n",
    "\n",
    "        if not is_start:\n",
    "            print()\n",
    "\n",
    "        is_start = True\n",
    "        log(f\"Start [{method.__name__}]:\" + (start_log if start_log else \"\"))\n",
    "        log(f'Start time: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        nesting_level += 1\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        end_time = time.time()\n",
    "\n",
    "        nesting_level -= 1\n",
    "        log(f\"End   [{method.__name__}]. Time elapsed: {end_time - start_time:0.2f} sec.\")\n",
    "        is_start = False\n",
    "\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration (todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train data\n",
    "train_click_log = pd.read_csv(f\"{DDIR}/train_preliminary/click_log.csv\")\n",
    "train_ad = pd.read_csv(f\"{DDIR}/train_preliminary/ad.csv\")\n",
    "# tag\n",
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "test_click_log = pd.read_csv(f\"{DDIR}/test/click_log.csv\")\n",
    "test_ad = pd.read_csv(f\"{DDIR}/test/ad.csv\")\n",
    "# pd.DataFrame(np.sort(test_click_log[UID].unique()), columns=[UID]).to_csv(f\"{DDIR}/test/user.csv\", index=False)\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_click_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_click_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ad[\"product_id\"] = train_ad[\"product_id\"].replace(\"\\\\N\", -1).astype(int)\n",
    "train_ad[\"industry\"] = train_ad[\"industry\"].replace(\"\\\\N\", -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ad[\"product_id\"] = test_ad[\"product_id\"].replace(\"\\\\N\", -1).astype(int)\n",
    "test_ad[\"industry\"] = test_ad[\"industry\"].replace(\"\\\\N\", -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creative id in train (creative id is unique in train_ad)\n",
    "len(train_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creative id in test (creative id is unique in test_ad)\n",
    "len(test_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check whether the same creative_id in train and test have same ad info\n",
    "insect1d = np.intersect1d(train_click_log.creative_id.unique(), test_click_log.creative_id.unique())\n",
    "print(\"Same creative id: \", insect1d.shape)\n",
    "print(\"Diff number: \", np.sum(train_ad[train_ad.creative_id.isin(insect1d)].values != test_ad[test_ad.creative_id.isin(insect1d)].values))\n",
    "# checked: they all have same ad info (result is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether click and ad have diff creative_id\n",
    "print(\"Diff list: \", np.setdiff1d(train_click_log.creative_id.unique(), train_ad.creative_id))\n",
    "print(\"Diff list: \", np.setdiff1d(train_ad.creative_id, train_click_log.creative_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether click and ad have diff creative_id\n",
    "print(\"Diff list: \", np.setdiff1d(test_click_log.creative_id.unique(), test_ad.creative_id))\n",
    "print(\"Diff list: \", np.setdiff1d(test_ad.creative_id, test_click_log.creative_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click time\n",
    "sns.lineplot(x=train_click_log.time.value_counts().index, y=train_click_log.time.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=test_click_log.time.value_counts().index, y=test_click_log.time.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_grouped = data.groupby(data.index)\n",
    "# results = Parallel(n_jobs=8)(delayed(key_func)(group) for name, group in data_grouped)\n",
    "# data = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ID sequence (Only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort by time, for time series\n",
    "# train_click_log.sort_values(by=\"time\", inplace=True)\n",
    "# test_click_log.sort_values(by=\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tol_train = pd.merge(train_click_log, train_ad, how=\"left\", on=\"creative_id\")\n",
    "# tol_test = pd.merge(test_click_log, test_ad, how=\"left\", on=\"creative_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_click_log, test_click_log\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeit\n",
    "# def gen_id_series(data, dtyp=\"train\"):\n",
    "#     for col in ID_SET:\n",
    "#         tmp = data.groupby([UID], sort=False)[[col]].agg(lambda x: [f\"word_{y}\" for y in x])\n",
    "#         tmp.columns = bch_rencol(tmp.columns)\n",
    "#         tmp.to_pickle(f\"{UDDIR}/imd/{dtyp}_{col}_seq.pkl\")\n",
    "#         tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_id_series(tol_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_id_series(tol_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqidx_dic = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ID_SET:\n",
    "#     train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "#     test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "#     tol_seq = pd.concat([train_seq, test_seq])\n",
    "#     seqidx_dic[col] = tol_seq.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{UDDIR}/imd/seqidx_dic.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(seqidx_dic), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF&Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_tfidf(col, nr_max=1):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    tol_seq[col] = tol_seq[col].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    tfidf_enc = TfidfVectorizer(ngram_range=(1, nr_max), min_df=30)\n",
    "    tfidf_vec = tfidf_enc.fit_transform(tol_seq[col].values)\n",
    "    log(f\"TF-IDF shape: {tfidf_vec.shape}\")\n",
    "\n",
    "    # save sparse matrix\n",
    "    scipy.sparse.save_npz(f\"{UDDIR}/imd/sparse_tfidf_{col}.npz\", tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_count(col, nr_max=1):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    tol_seq[col] = tol_seq[col].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    count_enc = CountVectorizer(ngram_range=(1, nr_max), min_df=30)\n",
    "    count_vec = count_enc.fit_transform(tol_seq[col].values)\n",
    "    log(f\"Count shape: {count_vec.shape}\")\n",
    "\n",
    "    # save sparse matrix\n",
    "    scipy.sparse.save_npz(f\"{UDDIR}/imd/sparse_count_{col}.npz\", count_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET[-2:]:\n",
    "    gen_tfidf(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET[-2:]:\n",
    "    gen_count(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_svd(col, index, prefix=\"tfidf\", n_cpt=64):\n",
    "    tfidf_vec = scipy.sparse.load_npz(f\"{UDDIR}/imd/sparse_{prefix}_{col}.npz\")\n",
    "    if tfidf_vec.shape[1] > n_cpt:\n",
    "        svd_enc = TruncatedSVD(n_components=n_cpt, n_iter=20, random_state=2020)\n",
    "        mode_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    else:\n",
    "        n_cpt = tfidf_vec.shape[1]\n",
    "        mode_svd = tfidf_vec.todense()\n",
    "    mode_svd = pd.DataFrame(mode_svd)\n",
    "    mode_svd.columns = [f\"{prefix}_svd_{col}_{i}\" for i in range(n_cpt)]\n",
    "    mode_svd.index = index\n",
    "    # save svd pkl\n",
    "    mode_svd.to_pickle(f\"{UFEDIR}/{prefix}_svd_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{UDDIR}/imd/seqidx_dic.pkl\", \"rb\") as f:\n",
    "    seqidx_dic = pickle.loads(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET:\n",
    "    gen_svd(col, seqidx_dic[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = None\n",
    "for col in tqdm(ID_SET):\n",
    "    print(\"current filename: \", f\"{UDDIR}/imd/sparse_tfidf_{col}.npz\", f\"{UDDIR}/imd/sparse_count_{col}.npz\")\n",
    "    tfidf_vec = scipy.sparse.load_npz(f\"{UDDIR}/imd/sparse_tfidf_{col}.npz\")\n",
    "    count_vec = scipy.sparse.load_npz(f\"{UDDIR}/imd/sparse_count_{col}.npz\")\n",
    "    sparse_matrix = scipy.sparse.csr_matrix(scipy.sparse.hstack([sparse_matrix, tfidf_vec]))\n",
    "    sparse_matrix = scipy.sparse.csr_matrix(scipy.sparse.hstack([sparse_matrix, count_vec]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{UDDIR}/imd/seqidx_dic.pkl\", \"rb\") as f:\n",
    "    seqidx_dic = pickle.loads(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_ids = list(combinations(ID_SET, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_eq = True\n",
    "for cp in comb_ids:\n",
    "    if (seqidx_dic[cp[0]] != seqidx_dic[cp[1]]).sum() != 0:\n",
    "        print(cp)\n",
    "        is_eq = False\n",
    "        break\n",
    "assert is_eq, \"Must True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sparse_matrix[seqidx_dic[\"creative_id\"].isin(train_user[UID])]\n",
    "X_test = sparse_matrix[~seqidx_dic[\"creative_id\"].isin(train_user[UID])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder index\n",
    "re_train_user = pd.DataFrame(index=seqidx_dic[\"creative_id\"][seqidx_dic[\"creative_id\"].isin(train_user[UID])])\n",
    "re_train_user = pd.merge(re_train_user, train_user, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder index\n",
    "re_test_user = pd.DataFrame(index=seqidx_dic[\"creative_id\"][~seqidx_dic[\"creative_id\"].isin(train_user[UID])])\n",
    "re_test_user = pd.merge(re_test_user, test_user, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(X_train.shape)\n",
    "log(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_clf = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_seed = 2020\n",
    "num_classes = 10\n",
    "n_splits = 5\n",
    "y = re_train_user[\"age\"]\n",
    "model_zoo = [SGDClassifier(n_jobs=-1,verbose=1), SGDClassifier(loss='log',n_jobs=-1,verbose=1),\n",
    "             SGDClassifier(loss='modified_huber',n_jobs=-1,verbose=1),\n",
    "             PassiveAggressiveClassifier(n_jobs=-1,verbose=1),LogisticRegression(C=10),\n",
    "             RidgeClassifier(solver='lsqr',fit_intercept=False),LinearSVC(verbose=1,max_iter=500),\n",
    "             BernoulliNB(),MultinomialNB()]\n",
    "oof = []\n",
    "count = 0\n",
    "columns = ['SGD_HINGE', 'SGD_LOG','SGD_HUBER','PAC','LR','RIDGE','LSVC','BNB','MNB']\n",
    "\n",
    "for model in model_zoo:\n",
    "    cv_pred_stack = np.zeros((X_train.shape[0],num_classes))\n",
    "    test_pred_stack = np.zeros((X_test.shape[0],num_classes))\n",
    "    skf = KFold(n_splits=n_splits,random_state=random_seed)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    if os.path.exists(f\"{UDDIR}/imd/tfidf_count_emb_age_{columns[count]}.pkl\"):\n",
    "        tmp = pd.read_pickle(f\"{UDDIR}/imd/tfidf_count_emb_age_{columns[count]}.pkl\")\n",
    "    else:\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "            print(index,model)\n",
    "            \n",
    "            train_x = test_x = train_y = test_y = None\n",
    "            gc.collect()\n",
    "            \n",
    "            train_x, test_x, train_y, test_y = X_train[train_index], X_train[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "            model.fit(train_x,train_y)\n",
    "            try:\n",
    "                y_val = model._predict_proba_lr(test_x)\n",
    "            except:\n",
    "                y_val = model.predict_proba(test_x)\n",
    "            cv_pred_stack[test_index] = y_val\n",
    "            print(y_val.shape)\n",
    "            try:\n",
    "                test_pred_stack += model._predict_proba_lr(X_test) / n_splits\n",
    "            except:\n",
    "                test_pred_stack += model.predict_proba(X_test) / n_splits\n",
    "            \n",
    "                \n",
    "        print(model,'score:',accuracy_score(y,np.argmax(cv_pred_stack,axis=1)))\n",
    "\n",
    "        a = pd.DataFrame(cv_pred_stack).add_prefix(columns[count]+\"_\")\n",
    "        a[UID] = re_train_user[UID].values\n",
    "        b = pd.DataFrame(test_pred_stack).add_prefix(columns[count]+\"_\")\n",
    "        b[UID] = re_test_user[UID].values\n",
    "        tmp = a.append(b)\n",
    "        tmp.to_pickle(f\"{UDDIR}/imd/tfidf_count_emb_age_{columns[count]}.pkl\")\n",
    "        \n",
    "    count += 1\n",
    "    oof.append(tmp)\n",
    "    \n",
    "\n",
    "df_agg = pd.DataFrame()\n",
    "for i in tqdm(oof):\n",
    "    df_agg[i.columns] = i\n",
    "df_agg.to_pickle(f\"{UFEDIR}/tfidf_count_emb_age_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_clf.check(\"clf embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_seed = 2020\n",
    "num_classes = 2\n",
    "n_splits = 5\n",
    "y = re_train_user[\"gender\"]\n",
    "model_zoo = [SGDClassifier(n_jobs=-1,verbose=1), SGDClassifier(loss='log',n_jobs=-1,verbose=1),\n",
    "             SGDClassifier(loss='modified_huber',n_jobs=-1,verbose=1),\n",
    "             PassiveAggressiveClassifier(n_jobs=-1,verbose=1),LogisticRegression(C=10),\n",
    "             RidgeClassifier(solver='lsqr',fit_intercept=False),LinearSVC(verbose=1,max_iter=500),\n",
    "             BernoulliNB(),MultinomialNB()]\n",
    "oof = []\n",
    "count = 0\n",
    "columns = ['SGD_HINGE', 'SGD_LOG','SGD_HUBER','PAC','LR','RIDGE','LSVC','BNB','MNB']\n",
    "\n",
    "for model in model_zoo:\n",
    "    cv_pred_stack = np.zeros((X_train.shape[0],num_classes))\n",
    "    test_pred_stack = np.zeros((X_test.shape[0],num_classes))\n",
    "    skf = KFold(n_splits=n_splits,random_state=random_seed)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    if os.path.exists(f\"{UDDIR}/imd/tfidf_count_emb_gender_{columns[count]}.pkl\"):\n",
    "        tmp = pd.read_pickle(f\"{UDDIR}/imd/tfidf_count_emb_gender_{columns[count]}.pkl\")\n",
    "    else:\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "            print(index,model)\n",
    "\n",
    "            train_x = test_x = train_y = test_y = None\n",
    "            gc.collect()\n",
    "\n",
    "            train_x, test_x, train_y, test_y = X_train[train_index], X_train[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "            model.fit(train_x,train_y)\n",
    "            try:\n",
    "                y_val = model._predict_proba_lr(test_x)\n",
    "            except:\n",
    "                y_val = model.predict_proba(test_x)\n",
    "            cv_pred_stack[test_index] = y_val\n",
    "            print(y_val.shape)\n",
    "            try:\n",
    "                test_pred_stack += model._predict_proba_lr(X_test) / n_splits\n",
    "            except:\n",
    "                test_pred_stack += model.predict_proba(X_test) / n_splits\n",
    "        print(model,'score:',accuracy_score(y,np.argmax(cv_pred_stack,axis=1)))\n",
    "\n",
    "        a = pd.DataFrame(cv_pred_stack).add_prefix(columns[count]+\"_\")\n",
    "        a[UID] = re_train_user[UID].values\n",
    "        b = pd.DataFrame(test_pred_stack).add_prefix(columns[count]+\"_\")\n",
    "        b[UID] = re_test_user[UID].values\n",
    "        tmp = a.append(b)\n",
    "        tmp.to_pickle(f\"{UDDIR}/imd/tfidf_count_emb_gender_{columns[count]}.pkl\")\n",
    "        \n",
    "    count += 1\n",
    "    oof.append(tmp)\n",
    "    \n",
    "\n",
    "df_agg = pd.DataFrame()\n",
    "for i in tqdm(oof):\n",
    "    df_agg[i.columns] = i\n",
    "df_agg.to_pickle(f\"{UFEDIR}/tfidf_count_emb_gender_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_clf.check(\"clf embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_w2v(col, vesize=300, win=20):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    if os.path.exists(f\"{UMDIR}/vectors/w2v_{col}.model\"):\n",
    "        model = gensim.models.Word2Vec.load(f\"{UMDIR}/vectors/w2v_{col}.model\")\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec(sentences=tol_seq[col], size=vesize, window=win, workers=32, sg=0, iter=10)\n",
    "        model.save(f\"{UMDIR}/vectors/w2v_{col}.model\")\n",
    "    \n",
    "    w2v_list = list()\n",
    "    \n",
    "    for it in tqdm(tol_seq[col]):\n",
    "        tmp = np.zeros(vesize)\n",
    "        cnt = 0\n",
    "        for wd in it:\n",
    "            cnt += 1\n",
    "            if wd in model:\n",
    "                tmp += model[wd]\n",
    "        w2v_list.append(list(tmp/cnt))\n",
    "    \n",
    "    w2v_avg = pd.DataFrame(w2v_list)\n",
    "    w2v_avg.index = tol_seq.index\n",
    "    w2v_avg.columns = [f\"w2v_avg_{col}_{i}\" for i in range(vesize)]\n",
    "    w2v_avg = reduce_mem_usage(w2v_avg)\n",
    "    w2v_avg.to_pickle(f\"{UFEDIR}/w2v_avg_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET:\n",
    "    gen_w2v(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_d2v(col, vesize=300, win=20):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    \n",
    "    if os.path.exists(f\"{UMDIR}/vectors/d2v_{col}.model\"):\n",
    "        model = gensim.models.Doc2Vec.load(f\"{UMDIR}/vectors/d2v_{col}.model\")\n",
    "    else:\n",
    "        docs = [gensim.models.doc2vec.TaggedDocument(words=i[1],tags=[str(i[0])]) for i in tol_seq[col].reset_index().values]\n",
    "        model = gensim.models.Doc2Vec(documents=docs, size=vesize, window=win, workers=32, iter=10)\n",
    "        model.save(f\"{UMDIR}/vectors/d2v_{col}.model\")\n",
    "    \n",
    "    d2v_list = list()\n",
    "    for it, cps in tqdm(tol_seq[col].reset_index().values):\n",
    "#         if it in model.docvecs:\n",
    "        d2v_list.append(model.docvecs[str(it)])\n",
    "#         else:\n",
    "#             d2v_list.append(model.infer_vector(cps))\n",
    "\n",
    "    d2v_avg = pd.DataFrame(d2v_list)\n",
    "    d2v_avg.index = tol_seq.index\n",
    "    d2v_avg.columns = [f\"d2v_avg_{col}_{i}\" for i in range(vesize)]\n",
    "    d2v_avg = reduce_mem_usage(d2v_avg)\n",
    "    d2v_avg.to_pickle(f\"{UFEDIR}/d2v_avg_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in ID_SET:\n",
    "    gen_d2v(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_glove(col, vesize=300, win=20):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    \n",
    "    if os.path.exists(f\"{UMDIR}/vectors/glove_{col}.model\"):\n",
    "        glove = Glove.load(f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "    else:\n",
    "        matrix =  Corpus()\n",
    "        matrix.fit(tol_seq[col].values)\n",
    "        glove = Glove(no_components=vesize, learning_rate=0.05)\n",
    "        glove.fit(matrix.matrix,epochs=10,no_threads=30,verbose=1)\n",
    "        glove.add_dictionary(matrix.dictionary)\n",
    "        glove.save(f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "        \n",
    "    ans = []\n",
    "    for i in tqdm(tol_seq[col].values):\n",
    "        line = []\n",
    "        for j in i:\n",
    "            line.append(glove.word_vectors[glove.dictionary[j]])\n",
    "        ans.append(np.mean(line,axis=0))\n",
    "    \n",
    "    glove_avg = pd.DataFrame(ans)\n",
    "    glove_avg.index = tol_seq.index\n",
    "    glove_avg.columns = [f\"glove_avg_{col}_{i}\" for i in range(vesize)]\n",
    "    glove_avg = reduce_mem_usage(glove_avg)\n",
    "    glove_avg.to_pickle(f\"{UFEDIR}/glove_avg_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET:\n",
    "    gen_glove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_train = pd.merge(train_click_log, train_ad, how=\"left\", on=\"creative_id\")\n",
    "tol_test = pd.merge(test_click_log, test_ad, how=\"left\", on=\"creative_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_click_log, train_ad\n",
    "del test_click_log, test_ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category map to Label (use KFold TODO)\n",
    "\n",
    "__a little overfitting age__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_catemlb(train_data, test_data, tag, col):\n",
    "    train_use = pd.merge(train_data[[UID, col]], tag, how=\"left\", on=UID)\n",
    "    test_use = test_data[[UID, col]]\n",
    "    \n",
    "    nfolds = 5\n",
    "    kfold = KFold(n_splits=nfolds)\n",
    "    \n",
    "    kf_map_df = pd.DataFrame()\n",
    "\n",
    "    re_train_use = pd.DataFrame()\n",
    "    re_test_use = pd.DataFrame()\n",
    "\n",
    "    for tr_idx, val_idx in kfold.split(train_use):\n",
    "        tr_ucid, val_ucid = train_use.iloc[tr_idx], train_use.iloc[val_idx]\n",
    "        kf_map = tr_ucid.groupby([col])[[\"age\", \"gender\"]].agg([\"mean\"])\n",
    "        kf_map.columns = bch_rencol(kf_map.columns, prefix=f\"{col}_\")\n",
    "        # only use intersect between train and test\n",
    "        kf_map.drop(np.setdiff1d(kf_map.index.unique(), test_use[col].unique()), inplace=True)\n",
    "        # get kf_cum\n",
    "        if kf_map_df.empty:\n",
    "            kf_map_df = kf_map\n",
    "        else:\n",
    "            kf_map_df = (kf_map_df + kf_map).fillna(0)\n",
    "        \n",
    "        val_ucid = pd.merge(val_ucid, kf_map, how=\"left\", on=col)\n",
    "        re_train_use = pd.concat([re_train_use, val_ucid])\n",
    "    \n",
    "    kf_map_df = kf_map_df/nfolds\n",
    "    re_test_use = pd.merge(test_use, kf_map_df, how=\"left\", on=col)\n",
    "\n",
    "    assert len(train_use) == len(re_train_use)\n",
    "    assert len(test_use) == len(re_test_use)\n",
    "\n",
    "    train_use = None\n",
    "    test_use = None\n",
    "\n",
    "    tmp = re_train_use.groupby([UID], sort=False)[f\"{col}_age_mean\", f\"{col}_gender_mean\"].agg([\"sum\", \"max\", \"mean\", \"min\", \"std\"])\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    tmp.to_pickle(f\"{UFEDIR}/train_stats_catemlb_{col}.pkl\")\n",
    "    tmp = None\n",
    "\n",
    "    tmp = re_test_use.groupby([UID], sort=False)[f\"{col}_age_mean\", f\"{col}_gender_mean\"].agg([\"sum\", \"max\", \"mean\", \"min\", \"std\"])\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    tmp.to_pickle(f\"{UFEDIR}/test_stats_catemlb_{col}.pkl\")\n",
    "    tmp = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\",]:  # \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    gen_catemlb(tol_train, tol_test, train_user, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One key(O1)-Time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "tmp = tol_train.groupby([UID], sort=False).agg(\n",
    "    {\n",
    "        UID: [\"count\"], \n",
    "        \"click_times\": OP_SET1,\n",
    "        \"time\": OP_SET1,\n",
    "        \"creative_id\": [\"nunique\"],\n",
    "        \"ad_id\": [\"nunique\"],\n",
    "        \"product_id\": [\"nunique\"],\n",
    "        \"product_category\": [\"nunique\"],\n",
    "        \"advertiser_id\": [\"nunique\"],\n",
    "        \"industry\": [\"nunique\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "tmp.columns = bch_rencol(tmp.columns)\n",
    "\n",
    "tmp.to_pickle(f\"{UFEDIR}/train_stats_o1.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "tmp = tol_test.groupby([UID], sort=False).agg(\n",
    "    {\n",
    "        UID: [\"count\"], \n",
    "        \"click_times\": OP_SET1,\n",
    "        \"time\": OP_SET1,\n",
    "        \"creative_id\": [\"nunique\"],\n",
    "        \"ad_id\": [\"nunique\"],\n",
    "        \"product_id\": [\"nunique\"],\n",
    "        \"product_category\": [\"nunique\"],\n",
    "        \"advertiser_id\": [\"nunique\"],\n",
    "        \"industry\": [\"nunique\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "tmp.columns = bch_rencol(tmp.columns)\n",
    "\n",
    "tmp.to_pickle(f\"{UFEDIR}/test_stats_o1.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comb Key(O2)-Time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for i in tqdm(range(1, 7)):\n",
    "    for cl in combinations(id_list, i):\n",
    "        key_set = [UID] + list(cl)\n",
    "        tmp = tol_train.groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "        tmp.columns = bch_rencol(tmp.columns, prefix=f\"{'C'.join(key_set[1:-1])}_\")\n",
    "        tmp.to_pickle(f\"{UFEDIR}/train_stats_o2_{'C'.join(key_set)}.pkl\")\n",
    "        tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for i in tqdm(range(1, 7)):\n",
    "    for cl in combinations(id_list, i):\n",
    "        key_set = [UID] + list(cl)\n",
    "        tmp = tol_test.groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "        tmp.columns = bch_rencol(tmp.columns, prefix=f\"{'C'.join(key_set[1:-1])}_\")\n",
    "        tmp.to_pickle(f\"{UFEDIR}/test_stats_o2_{'C'.join(key_set)}.pkl\")\n",
    "        tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Windows-Time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_set = [3, 7, 10, 30]\n",
    "\n",
    "for bins in bins_set:\n",
    "    tol_bins = pd.cut(pd.concat([tol_train[\"time\"], tol_test[\"time\"]]), bins, labels=range(bins))\n",
    "    tol_train[f\"bins{bins}\"] = tol_bins[:len(tol_train)]\n",
    "    tol_test[f\"bins{bins}\"] = tol_bins[len(tol_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Init Bin Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Key(O1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train\n",
    "for bins in bins_set:\n",
    "    for cb in tqdm(range(bins)):\n",
    "        tmp = tol_train[tol_train[f\"bins{bins}\"] == cb].groupby([UID], sort=False).agg(\n",
    "            {\n",
    "                UID: [\"count\"], \n",
    "                \"click_times\": OP_SET1,\n",
    "                \"time\": OP_SET1,\n",
    "                \"creative_id\": [\"nunique\"],\n",
    "                \"ad_id\": [\"nunique\"],\n",
    "                \"product_id\": [\"nunique\"],\n",
    "                \"product_category\": [\"nunique\"],\n",
    "                \"advertiser_id\": [\"nunique\"],\n",
    "                \"industry\": [\"nunique\"],\n",
    "            }\n",
    "        )\n",
    "        tmp.columns = bch_rencol(tmp.columns, prefix=f\"bins{bins}_{cb}_\")\n",
    "        tmp.to_pickle(f\"{UFEDIR}/train_stats_o1_bins{bins}_{cb}.pkl\")\n",
    "        tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o1 for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "for bins in bins_set:\n",
    "    for cb in tqdm(range(bins)):\n",
    "        tmp = tol_test[tol_test[f\"bins{bins}\"] == cb].groupby([UID], sort=False).agg(\n",
    "            {\n",
    "                UID: [\"count\"], \n",
    "                \"click_times\": OP_SET1,\n",
    "                \"time\": OP_SET1,\n",
    "                \"creative_id\": [\"nunique\"],\n",
    "                \"ad_id\": [\"nunique\"],\n",
    "                \"product_id\": [\"nunique\"],\n",
    "                \"product_category\": [\"nunique\"],\n",
    "                \"advertiser_id\": [\"nunique\"],\n",
    "                \"industry\": [\"nunique\"],\n",
    "            }\n",
    "        )\n",
    "        tmp.columns = bch_rencol(tmp.columns, prefix=f\"bins{bins}_{cb}_\")\n",
    "        tmp.to_pickle(f\"{UFEDIR}/test_stats_o1_bins{bins}_{cb}.pkl\")\n",
    "        tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o1 for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comb Key(O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for bins in bins_set:\n",
    "    for cb in tqdm(range(bins)):\n",
    "        for i in tqdm(range(1, 7)):\n",
    "            for cl in combinations(id_list, i):\n",
    "                key_set = [UID] + list(cl)\n",
    "                tmp = tol_train[tol_train[f\"bins{bins}\"] == cb].groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "                tmp.columns = bch_rencol(tmp.columns, prefix=f\"bins{bins}_{cb}_{'C'.join(key_set[1:-1])}_\")\n",
    "                tmp.to_pickle(f\"{UFEDIR}/train_stats_o2_bins{bins}_{cb}_{'C'.join(key_set)}.pkl\")\n",
    "                tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o2 bin for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for bins in bins_set:\n",
    "    for cb in tqdm(range(bins)):\n",
    "        for i in tqdm(range(1, 7)):\n",
    "            for cl in combinations(id_list, i):\n",
    "                key_set = [UID] + list(cl)\n",
    "                tmp = tol_test[tol_test[f\"bins{bins}\"] == cb].groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "                tmp.columns = bch_rencol(tmp.columns, prefix=f\"bins{bins}_{cb}_{'C'.join(key_set[1:-1])}_\")\n",
    "                tmp.to_pickle(f\"{UFEDIR}/test_stats_o2_bins{bins}_{cb}_{'C'.join(key_set)}.pkl\")\n",
    "                tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o2 bin for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_set = [1, 2, 3, 7, 14, 21, 30, 60, 90]\n",
    "\n",
    "tol_train[\"max_time\"] = tol_train.groupby([UID], sort=False)[\"time\"].transform(\"max\")\n",
    "tol_test[\"max_time\"] = tol_test.groupby([UID], sort=False)[\"time\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Get Max time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Key(O1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train\n",
    "for win in slide_set:\n",
    "    tmp = tol_train[tol_train[\"time\"] >= tol_train[\"max_time\"] - win].groupby([UID], sort=False).agg(\n",
    "        {\n",
    "            UID: [\"count\"], \n",
    "            \"click_times\": OP_SET1,\n",
    "            \"time\": OP_SET1,\n",
    "            \"creative_id\": [\"nunique\"],\n",
    "            \"ad_id\": [\"nunique\"],\n",
    "            \"product_id\": [\"nunique\"],\n",
    "            \"product_category\": [\"nunique\"],\n",
    "            \"advertiser_id\": [\"nunique\"],\n",
    "            \"industry\": [\"nunique\"],\n",
    "        }\n",
    "    )\n",
    "    tmp.columns = bch_rencol(tmp.columns, prefix=f\"slide{win}_\")\n",
    "    tmp.to_pickle(f\"{UFEDIR}/train_stats_o1_slide{win}.pkl\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o1 slide for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "for win in slide_set:\n",
    "    tmp = tol_test[tol_test[\"time\"] >= tol_test[\"max_time\"] - win].groupby([UID], sort=False).agg(\n",
    "        {\n",
    "            UID: [\"count\"], \n",
    "            \"click_times\": OP_SET1,\n",
    "            \"time\": OP_SET1,\n",
    "            \"creative_id\": [\"nunique\"],\n",
    "            \"ad_id\": [\"nunique\"],\n",
    "            \"product_id\": [\"nunique\"],\n",
    "            \"product_category\": [\"nunique\"],\n",
    "            \"advertiser_id\": [\"nunique\"],\n",
    "            \"industry\": [\"nunique\"],\n",
    "        }\n",
    "    )\n",
    "    tmp.columns = bch_rencol(tmp.columns, prefix=f\"slide{win}_\")\n",
    "    tmp.to_pickle(f\"{UFEDIR}/test_stats_o1_slide{win}.pkl\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o1 slide for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comb Key(O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for win in slide_set:\n",
    "    for i in tqdm(range(1, 7)):\n",
    "        for cl in combinations(id_list, i):\n",
    "            key_set = [UID] + list(cl)\n",
    "            tmp = tol_train[tol_train[\"time\"] >= tol_train[\"max_time\"] - win].groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "            tmp.columns = bch_rencol(tmp.columns, prefix=f\"slide{win}_{'C'.join(key_set[1:-1])}_\")\n",
    "            tmp.to_pickle(f\"{UFEDIR}/train_stats_o2_slide{win}_{'C'.join(key_set)}.pkl\")\n",
    "            tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o2 slide for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for win in slide_set:\n",
    "    for i in tqdm(range(1, 7)):\n",
    "        for cl in combinations(id_list, i):\n",
    "            key_set = [UID] + list(cl)\n",
    "            tmp = tol_test[tol_test[\"time\"] >= tol_test[\"max_time\"] - win].groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "            tmp.columns = bch_rencol(tmp.columns, prefix=f\"slide{win}_{'C'.join(key_set[1:-1])}_\")\n",
    "            tmp.to_pickle(f\"{UFEDIR}/test_stats_o2_slide{win}_{'C'.join(key_set)}.pkl\")\n",
    "            tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o2 slide for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TOP Category) One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.concat([tol_train, tol_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_dic = {\n",
    "    \"creative_id\": 300,\n",
    "    \"ad_id\": 300,\n",
    "    \"product_id\": 100,\n",
    "    \"advertiser_id\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid in dum_dic:\n",
    "    use_id = tol_data[cid].value_counts().head(dum_dic[cid]).index.values\n",
    "    tol_data[f\"{cid}_dummy\"] = tol_data[cid]\n",
    "    tol_data[f\"{cid}_dummy\"][~tol_data[f\"{cid}_dummy\"].isin(use_id)] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid in  [\"time\", \"click_times\", \"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    ocid = cid\n",
    "    if cid in dum_dic:\n",
    "        cid = f\"{cid}_dummy\"\n",
    "    tmp = tol_data[[UID, cid]].groupby([UID, cid], sort=False)[[cid]].agg([\"count\"]).unstack().fillna(0)\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    reduce_mem_usage(tmp).to_pickle(f\"{UFEDIR}/onehot_catecnt_{ocid}.pkl\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Train \n",
    "\n",
    "__weak learner prediction as features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================Once=================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_test.index = -tol_test.index -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.concat([tol_train, tol_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tol_train, tol_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data[f\"{UID}_count\"] = tol_data[[UID]].groupby([UID])[UID].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    tmp = tol_data[[UID, col]]\n",
    "    tol_data[f\"{UID}_{col}_std\"] = tmp.groupby([UID])[col].transform(\"std\")\n",
    "    tol_data[f\"{col}_{UID}_count\"] = tmp.groupby([col])[UID].transform(\"count\")\n",
    "    tol_data[f\"{col}_{UID}_std\"] = tmp.groupby([col])[UID].transform(\"std\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"time\", \"click_times\"]:\n",
    "    tmp = tol_data[[UID, col]]\n",
    "    tol_data[f\"{UID}_{col}_std\"] = tmp.groupby([UID])[col].transform(\"std\")\n",
    "    tol_data[f\"{UID}_{col}_mean\"] = tmp.groupby([UID])[col].transform(\"mean\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = reduce_mem_usage(tol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data.to_pickle(f\"{UDDIR}/imd/tol_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============End===============#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if have tol_data.pkl\n",
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")\n",
    "\n",
    "tol_data = pd.read_pickle(f\"{UDDIR}/imd/tol_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred))** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.merge(tol_data, train_user[[UID, \"age\"]], how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df_hist_train df_new_train df_hist_new_train to train 3 models\n",
    "train_df = tol_data[tol_data[\"age\"].notnull()]\n",
    "test_df = tol_data[tol_data[\"age\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [UID, \"age\"]\n",
    "cat_features = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in tol_data.columns if f not in drop_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits= 3\n",
    "folds = GroupKFold(n_splits=n_splits)\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "print ('feats:' + str(len(feats)))\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[\"age\"],groups=train_df[UID])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df[\"age\"].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[\"age\"].iloc[valid_idx] \n",
    "    \n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "               \"objective\" : \"regression\", \n",
    "               \"boosting\" : \"gbdt\", \n",
    "               \"metric\" : \"rmse\",  \n",
    "               \"max_depth\": 7, \n",
    "               \"num_leaves\" : 31, \n",
    "               \"max_bin\" : 255, \n",
    "               \"learning_rate\" : 0.1, \n",
    "               \"subsample\" : 0.8,\n",
    "               \"colsample_bytree\" : 0.8, \n",
    "               \"verbosity\": -1,\n",
    "               \"num_threads\" : -1,\n",
    "    }\n",
    "    \n",
    "\n",
    "    if n_fold >= 0:\n",
    "        evals_result = {}\n",
    "        dtrain = lgb.Dataset(\n",
    "            train_x, label=train_y,categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(\n",
    "            valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features)\n",
    "        bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=30000,\n",
    "            valid_sets=[dval], early_stopping_rounds=100, verbose_eval=20,)#feval = evalerror\n",
    "        \n",
    "        new_list = sorted(zip(feats, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:]\n",
    "        for item in new_list:\n",
    "            print (item) \n",
    "\n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "\n",
    "        sub_preds += bst.predict(test_df[feats], num_iteration=bst.best_iteration) / folds.n_splits # test_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = rmse(train_df[\"age\"],  oof_preds)\n",
    "print('Full OOF RMSE %.6f' % cv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df[[UID]]\n",
    "b = test_df[[UID]]\n",
    "\n",
    "a[\"age_pred\"] = oof_preds\n",
    "b[\"age_pred\"] = sub_preds\n",
    "\n",
    "a1 = a.groupby([UID])[\"age_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])\n",
    "b1 = b.groupby([UID])[\"age_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.append(b1).add_prefix(\"age_gkf_agg_pred_\").reset_index().sort_values(by=[UID]).reset_index(drop=True).to_pickle(f\"{UFEDIR}/meta_age_group_regeress.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.merge(tol_data, train_user[[UID, \"gender\"]], how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df_hist_train df_new_train df_hist_new_train to train 3 models\n",
    "train_df = tol_data[tol_data[\"gender\"].notnull()]\n",
    "test_df = tol_data[tol_data[\"gender\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [UID, \"gender\"]\n",
    "cat_features = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in tol_data.columns if f not in drop_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits= 3\n",
    "folds = GroupKFold(n_splits=n_splits)\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "print ('feats:' + str(len(feats)))\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[\"gender\"],groups=train_df[UID])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df[\"gender\"].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[\"gender\"].iloc[valid_idx] \n",
    "    \n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "               \"objective\" : \"regression\", \n",
    "               \"boosting\" : \"gbdt\", \n",
    "               \"metric\" : \"rmse\",  \n",
    "               \"max_depth\": 7, \n",
    "               \"num_leaves\" : 31, \n",
    "               \"max_bin\" : 255, \n",
    "               \"learning_rate\" : 0.1, \n",
    "               \"subsample\" : 0.8,\n",
    "               \"colsample_bytree\" : 0.8, \n",
    "               \"verbosity\": -1,\n",
    "               \"num_threads\" : -1,\n",
    "    }\n",
    "    \n",
    "\n",
    "    if n_fold >= 0:\n",
    "        evals_result = {}\n",
    "        dtrain = lgb.Dataset(\n",
    "            train_x, label=train_y,categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(\n",
    "            valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features)\n",
    "        bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=30000,\n",
    "            valid_sets=[dval], early_stopping_rounds=100, verbose_eval=20,)#feval = evalerror\n",
    "        \n",
    "        new_list = sorted(zip(feats, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:]\n",
    "        for item in new_list:\n",
    "            print (item) \n",
    "\n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "\n",
    "        sub_preds += bst.predict(test_df[feats], num_iteration=bst.best_iteration) / folds.n_splits # test_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = rmse(train_df[\"gender\"],  oof_preds)\n",
    "print('Full OOF RMSE %.6f' % cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df[[UID]]\n",
    "b = test_df[[UID]]\n",
    "\n",
    "a[\"gender_pred\"] = oof_preds\n",
    "b[\"gender_pred\"] = sub_preds\n",
    "\n",
    "a1 = a.groupby([UID])[\"gender_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])\n",
    "b1 = b.groupby([UID])[\"gender_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.append(b1).add_prefix(\"gender_gkf_agg_pred_\").reset_index().sort_values(by=[UID]).reset_index(drop=True).to_pickle(f\"{UFEDIR}/meta_gender_group_regeress.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fname = sorted(os.listdir(UFEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reduce memory (only once)\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"w2v_\") or fname.startswith(\"tfidf_svd_\") or fname.startswith(\"meta_age_\") or fname.startswith(\"meta_gender_\") or (fname.find(\"catemlb\") != -1) or (fname.find(\"stats\") != -1):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         reduce_mem_usage(pd.read_pickle(f\"{UFEDIR}/{fname}\")).to_pickle(f\"{UFEDIR}/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()\n",
    "train_feat[UID] = train_user[UID]\n",
    "test_feat[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"w2v_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_w2v = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_w2v, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_w2v, how=\"left\", on=UID)\n",
    "        cur_w2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats o1\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_feat = pd.merge(train_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_feat = pd.merge(test_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats getidxmax\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_o2_getidxmax\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_feat = pd.merge(train_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_o2_getidxmax\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_feat = pd.merge(test_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats getidxmax\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"train_stats_catemlb\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         train_feat = pd.merge(train_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "#     elif fname.startswith(\"test_stats_catemlb\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         test_feat = pd.merge(test_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not use\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"tfidf_svd_\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         cur_tfidf_svd = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "#         train_feat = pd.merge(train_feat, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "#         test_feat = pd.merge(test_feat, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "#         cur_tfidf_svd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"meta_age_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        age_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, age_agg_pred, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, age_agg_pred, how=\"left\", on=UID)\n",
    "        age_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"meta_gender_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        gender_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, gender_agg_pred, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, gender_agg_pred, how=\"left\", on=UID)\n",
    "        gender_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"d2v_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_d2v = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_d2v, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_d2v, how=\"left\", on=UID)\n",
    "        cur_d2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"glove_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_glove = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_glove, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_glove, how=\"left\", on=UID)\n",
    "        cur_glove = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not use has same effect with w2v and doc2vec\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"onehot_catecnt_\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         cur_ohcc = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "#         train_feat = pd.merge(train_feat, cur_ohcc, how=\"left\", on=UID)\n",
    "#         test_feat = pd.merge(test_feat, cur_ohcc, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"tfidf_count_emb_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_tce = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_tce, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_tce, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure feat and user(target) have same order\n",
    "# if true --> sum == 0\n",
    "np.sum(train_feat[UID] != train_user[UID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure feat and user(target) have same order\n",
    "# if true --> sum == 0\n",
    "np.sum(test_feat[UID] != test_user[UID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure train and test features have same order\n",
    "# if true --> sum == 0\n",
    "np.sum(train_feat.columns != test_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.memory_usage().sum() / 1024**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat.memory_usage().sum() / 1024**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.to_pickle(f\"{UDDIR}/feat_ing/train_feat_tol_v05.pkl\")\n",
    "test_feat.to_pickle(f\"{UDDIR}/feat_ing/test_feat_tol_v05.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")\n",
    "train_feat = pd.read_pickle(f\"{UDDIR}/feat_ing/train_feat_tol_v05.pkl\")\n",
    "test_feat = pd.read_pickle(f\"{UDDIR}/feat_ing/test_feat_tol_v05.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat.drop([col for col in train_feat.columns if col.find(\"creative_id_gender_\") != -1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feat.drop([col for col in test_feat.columns if col.find(\"creative_id_gender_\") != -1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training&Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.set_index(UID, inplace=True)\n",
    "test_feat.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_suffix = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "\n",
    "    \"\"\"RAdam optimizer.\n",
    "\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay for each param.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n",
    "        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n",
    "        min_lr: float >= 0. Minimum learning rate after warmup.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n",
    "                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n",
    "        super(RAdam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.total_steps = K.variable(total_steps, name='total_steps')\n",
    "            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n",
    "            self.min_lr = K.variable(min_lr, name='min_lr')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.initial_weight_decay = weight_decay\n",
    "        self.initial_total_steps = total_steps\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.learning_rate\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        if self.initial_total_steps > 0:\n",
    "            warmup_steps = self.total_steps * self.warmup_proportion\n",
    "            decay_steps = self.total_steps - warmup_steps\n",
    "            lr = K.switch(\n",
    "                t <= warmup_steps,\n",
    "                lr * (t / warmup_steps),\n",
    "                lr * (1.0 - K.minimum(t, decay_steps) / decay_steps),\n",
    "            )\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        else:\n",
    "            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        beta_1_t = K.pow(self.beta_1, t)\n",
    "        beta_2_t = K.pow(self.beta_2, t)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            m_corr_t = m_t / (1.0 - beta_1_t)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t) + self.epsilon)\n",
    "\n",
    "            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                         (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                         sma_inf / sma_t)\n",
    "\n",
    "            p_t = K.switch(sma_t > 5, r_t * m_corr_t / v_corr_t, m_corr_t)\n",
    "\n",
    "            if self.initial_weight_decay > 0:\n",
    "                p_t += self.weight_decay * p\n",
    "\n",
    "            p_t = p - lr * p_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'lr': float(K.get_value(self.learning_rate)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'decay': float(K.get_value(self.decay)),\n",
    "            'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': float(K.get_value(self.total_steps)),\n",
    "            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n",
    "            'min_lr': float(K.get_value(self.min_lr)),\n",
    "        }\n",
    "        base_config = super(RAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(object):\n",
    "    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5, alpha=0.5):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.count = 0\n",
    "\n",
    "    def inject(self, model):\n",
    "        \"\"\"Inject the Lookahead algorithm for the given model.\n",
    "        The following code is modified from keras's _make_train_function method.\n",
    "        See: https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497\n",
    "        \"\"\"\n",
    "        if not hasattr(model, 'train_function'):\n",
    "            raise RuntimeError('You must compile your model before using it.')\n",
    "\n",
    "        model._check_trainable_weights_consistency()\n",
    "\n",
    "        if model.train_function is None:\n",
    "            inputs = (model._feed_inputs +\n",
    "                      model._feed_targets +\n",
    "                      model._feed_sample_weights)\n",
    "            if model._uses_dynamic_learning_phase():\n",
    "                inputs += [K.learning_phase()]\n",
    "            fast_params = model._collected_trainable_weights\n",
    "\n",
    "            with K.name_scope('training'):\n",
    "                with K.name_scope(model.optimizer.__class__.__name__):\n",
    "                    training_updates = model.optimizer.get_updates(\n",
    "                        params=fast_params,\n",
    "                        loss=model.total_loss)\n",
    "                    slow_params = [K.variable(p) for p in fast_params]\n",
    "                fast_updates = (model.updates +\n",
    "                                training_updates +\n",
    "                                model.metrics_updates)\n",
    "\n",
    "                slow_updates, copy_updates = [], []\n",
    "                for p, q in zip(fast_params, slow_params):\n",
    "                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n",
    "                    copy_updates.append(K.update(p, q))\n",
    "\n",
    "                # Gets loss and metrics. Updates weights at each call.\n",
    "                fast_train_function = K.function(\n",
    "                    inputs,\n",
    "                    [model.total_loss] + model.metrics_tensors,\n",
    "                    updates=fast_updates,\n",
    "                    name='fast_train_function',\n",
    "                    **model._function_kwargs)\n",
    "\n",
    "                def F(inputs):\n",
    "                    self.count += 1\n",
    "                    R = fast_train_function(inputs)\n",
    "                    if self.count % self.k == 0:\n",
    "                        K.batch_get_value(slow_updates)\n",
    "                        K.batch_get_value(copy_updates)\n",
    "                    return R\n",
    "                \n",
    "                model.train_function = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            # decoupled weight decay (2/4)\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay')\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "#     @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd  # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            # decoupled weight decay (4/4)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.learning_rate)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Position_Embedding(Layer): \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):        \n",
    "        self.size = size         \n",
    "        self.mode = mode       \n",
    "        super(Position_Embedding, self).__init__(**kwargs) \n",
    "\n",
    "    def call(self, x): \n",
    "        if (self.size == None) or (self.mode == 'sum'):            \n",
    "            self.size = int(x.shape[-1])        \n",
    "            batch_size, seq_len = K.shape(x)[0], K.shape(x)[1]        \n",
    "            position_j = 1. / K.pow(10000., \\\n",
    "                2 * K.arange(self.size / 2, dtype='float32') / self.size)        \n",
    "            position_j = K.expand_dims(position_j, 0)        \n",
    "            position_i = K.cumsum(K.ones_like(x[:, :, 0]), 1)-1     \n",
    "            position_i = K.expand_dims(position_i, 2)        \n",
    "            position_ij = K.dot(position_i, position_j)        \n",
    "            position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2) \n",
    "            if self.mode == 'sum': \n",
    "                return position_ij + x \n",
    "            elif self.mode == 'concat': \n",
    "                return K.concatenate([position_ij, x], 2) \n",
    "\n",
    "    def compute_output_shape(self, input_shape): \n",
    "        if self.mode == 'sum': \n",
    "            return input_shape \n",
    "        elif self.mode == 'concat': \n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(Layer): \n",
    "    def __init__(self, output_dim=30, activation=\"relu\",**kwargs): \n",
    "        self.output_dim = output_dim \n",
    "        self.activate = activations.get(activation)\n",
    "        super(FM, self).__init__(**kwargs) \n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.weight = self.add_weight(name='weight',shape=(input_shape[1], self.output_dim),initializer='glorot_uniform',trainable=True) \n",
    "        self.bias = self.add_weight(name='bias',shape=(self.output_dim,),initializer='zeros',trainable=True) \n",
    "        self.kernel = self.add_weight(name='kernel',shape=(input_shape[1], self.output_dim),initializer='glorot_uniform',trainable=True) \n",
    "        super(FM, self).build(input_shape) \n",
    "        \n",
    "    def call(self, x):\n",
    "        feature = K.dot(x,self.weight) + self.bias\n",
    "        a = K.pow(K.dot(x,self.kernel), 2)\n",
    "        b = K.dot(x, K.pow(self.kernel, 2))\n",
    "        cross = K.mean(a-b, 1, keepdims=True)*0.5\n",
    "        cross = K.repeat_elements(K.reshape(cross, (-1, 1)), self.output_dim, axis=-1) \n",
    "        return self.activate(feature + cross) \n",
    "    \n",
    "    def compute_output_shape(self, input_shape): \n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(inshape, outshape):\n",
    "    # create two models\n",
    "    input1 = Input(shape=inshape)\n",
    "    embed_1 = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(input1)\n",
    "#     lstm_1 = LSTM(256)\n",
    "    dense_1 = Dense(128, kernel_initializer='normal', activation='relu')(input1)\n",
    "#     dense_1 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(64, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "#     dense_2 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(32, kernel_initializer='normal', activation='relu')(dense_2)\n",
    "#     dense_3 = Dropout(0.5)(dense_3)\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(dense_3)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer=\"adadelta\", metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet(inshape, outshape):\n",
    "    # create two models\n",
    "    input1  = Input(shape=inshape)\n",
    "\n",
    "    dense_1 = Dense(256, kernel_initializer='normal', activation='relu')(input1)\n",
    "    #dense_1 = Dropout(0.6)(dense_1)\n",
    "    dense_2 = Dense(128, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "    #dense_2 = Dropout(0.6)(dense_2)\n",
    "    dense_2_x  = concatenate([dense_1,dense_2])\n",
    "    dense_3 = Dense(64, kernel_initializer='normal', activation='relu')(dense_2_x)\n",
    "    #dense_3 = Dropout(0.6)(dense_3)\n",
    "    dense_3_x  = concatenate([dense_1,dense_2,dense_3])\n",
    "    dense_4 = Dense(7, kernel_initializer='normal', activation='relu')(dense_3_x)\n",
    "    #dense_4 = Dropout(0.6)(dense_4)\n",
    "    dense_4_x  = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(dense_4_x)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FMNet(inshape, outshape):\n",
    "    # create two models\n",
    "    input1  = Input(shape=inshape)\n",
    "    #DNN_model_I\n",
    "    dense_1 = Dense(100, kernel_initializer='normal', activation='tanh')(input1)\n",
    "    dense_2 = Dense(150, kernel_initializer='normal', activation='tanh')(dense_1)\n",
    "    dense_3 = Dense(150, kernel_initializer='normal', activation='tanh')(dense_2)\n",
    "    dense_4 = Dense(100, kernel_initializer='normal', activation='tanh')(dense_3)\n",
    "    dense_5 = Dense(64, kernel_initializer='normal', activation='tanh')(dense_4)\n",
    "    #FM_model_II\n",
    "    FM_1    = FM(200)(input1)\n",
    "    FM_2    = FM(64)(FM_1)\n",
    "\n",
    "    x       = concatenate([dense_5,FM_2])\n",
    "    x_tmp   = Dense(32,kernel_initializer='normal', activation='softmax')(x)\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(x_tmp)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadnprec(col, max_feature=None, maxlen=None):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    \n",
    "    train_X = train_seq[col].values\n",
    "    test_X = test_seq[col].values\n",
    "    \n",
    "    # Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    # Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    return train_X, test_X, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(col, word_index, max_features):\n",
    "    \n",
    "    EMBEDDING_FILE = (f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "    k = Glove.load(EMBEDDING_FILE)\n",
    "    \n",
    "    embeddings_index = []\n",
    "    for i in tqdm(k.dictionary.keys()):\n",
    "        embeddings_index.append((i,k.word_vectors[k.dictionary[i]]))\n",
    "\n",
    "    embeddings_index = dict(pd.DataFrame(embeddings_index).values)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v(col, word_index, max_features):    \n",
    "    EMBEDDING_FILE = (f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fname = sorted(os.listdir(UFEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()\n",
    "train_feat[UID] = train_user[UID]\n",
    "test_feat[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.set_index(UID, inplace=True)\n",
    "test_feat.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  w2v_avg_creative_id.pkl\n",
      "current filename:  w2v_avg_ad_id.pkl\n",
      "current filename:  w2v_avg_product_id.pkl\n",
      "current filename:  w2v_avg_product_category.pkl\n",
      "current filename:  w2v_avg_advertiser_id.pkl\n",
      "current filename:  w2v_avg_industry.pkl\n"
     ]
    }
   ],
   "source": [
    "w2v_dic = dict()\n",
    "for cur_id in ID_SET:\n",
    "    for fname in feat_fname:\n",
    "        if fname.startswith(f\"w2v_avg_{cur_id}\"):\n",
    "            print(\"current filename: \", fname)\n",
    "            w2v_dic[cur_id] = pd.read_pickle(f\"{UFEDIR}/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ctid = pd.merge(train_feat, w2v_dic[\"creative_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_adid = pd.merge(train_feat, w2v_dic[\"ad_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_atid = pd.merge(train_feat, w2v_dic[\"advertiser_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "\n",
    "train_pdid = pd.merge(train_feat, w2v_dic[\"product_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_pdct = pd.merge(train_feat, w2v_dic[\"product_category\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_inds = pd.merge(train_feat, w2v_dic[\"industry\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ctid = pd.merge(test_feat, w2v_dic[\"creative_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_adid = pd.merge(test_feat, w2v_dic[\"ad_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_atid = pd.merge(test_feat, w2v_dic[\"advertiser_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "\n",
    "test_pdid = pd.merge(test_feat, w2v_dic[\"product_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_pdct = pd.merge(test_feat, w2v_dic[\"product_category\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_inds = pd.merge(test_feat, w2v_dic[\"industry\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  d2v_avg_creative_id.pkl\n",
      "current filename:  d2v_avg_ad_id.pkl\n",
      "current filename:  d2v_avg_product_id.pkl\n",
      "current filename:  d2v_avg_product_category.pkl\n",
      "current filename:  d2v_avg_advertiser_id.pkl\n",
      "current filename:  d2v_avg_industry.pkl\n"
     ]
    }
   ],
   "source": [
    "d2v_dic = dict()\n",
    "for cur_id in ID_SET:\n",
    "    for fname in feat_fname:\n",
    "        if fname.startswith(f\"d2v_avg_{cur_id}\"):\n",
    "            print(\"current filename: \", fname)\n",
    "            d2v_dic[cur_id] = pd.read_pickle(f\"{UFEDIR}/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dctid = pd.merge(train_feat, d2v_dic[\"creative_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_dadid = pd.merge(train_feat, d2v_dic[\"ad_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_datid = pd.merge(train_feat, d2v_dic[\"advertiser_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "\n",
    "train_dpdid = pd.merge(train_feat, d2v_dic[\"product_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_dpdct = pd.merge(train_feat, d2v_dic[\"product_category\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_dinds = pd.merge(train_feat, d2v_dic[\"industry\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dctid = pd.merge(test_feat, d2v_dic[\"creative_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_dadid = pd.merge(test_feat, d2v_dic[\"ad_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_datid = pd.merge(test_feat, d2v_dic[\"advertiser_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "\n",
    "test_dpdid = pd.merge(test_feat, d2v_dic[\"product_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_dpdct = pd.merge(test_feat, d2v_dic[\"product_category\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_dinds = pd.merge(test_feat, d2v_dic[\"industry\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pemb = pd.DataFrame()\n",
    "test_pemb = pd.DataFrame()\n",
    "train_pemb[UID] = train_user[UID]\n",
    "test_pemb[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  meta_age_group_regeress.pkl\n",
      "current filename:  meta_gender_group_regeress.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"meta_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        gender_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_pemb = pd.merge(train_pemb, gender_agg_pred, how=\"left\", on=UID)\n",
    "        test_pemb = pd.merge(test_pemb, gender_agg_pred, how=\"left\", on=UID)\n",
    "        gender_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  tfidf_count_emb_age_all.pkl\n",
      "current filename:  tfidf_count_emb_gender_all.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"tfidf_count_emb_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_tce = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_pemb = pd.merge(train_pemb, cur_tce, how=\"left\", on=UID)\n",
    "        test_pemb = pd.merge(test_pemb, cur_tce, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  test_stats_catemlb_ad_id.pkl\n",
      "current filename:  test_stats_catemlb_advertiser_id.pkl\n",
      "current filename:  test_stats_catemlb_creative_id.pkl\n",
      "current filename:  test_stats_catemlb_industry.pkl\n",
      "current filename:  test_stats_catemlb_product_category.pkl\n",
      "current filename:  test_stats_catemlb_product_id.pkl\n",
      "current filename:  train_stats_catemlb_ad_id.pkl\n",
      "current filename:  train_stats_catemlb_advertiser_id.pkl\n",
      "current filename:  train_stats_catemlb_creative_id.pkl\n",
      "current filename:  train_stats_catemlb_industry.pkl\n",
      "current filename:  train_stats_catemlb_product_category.pkl\n",
      "current filename:  train_stats_catemlb_product_id.pkl\n"
     ]
    }
   ],
   "source": [
    "# stats cate target encode\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_catemlb\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_pemb = pd.merge(train_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_catemlb\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_pemb = pd.merge(test_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  test_stats_o1.pkl\n",
      "current filename:  train_stats_o1.pkl\n"
     ]
    }
   ],
   "source": [
    "# stats o1\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_pemb = pd.merge(train_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_pemb = pd.merge(test_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pemb.set_index(UID, inplace=True)\n",
    "test_pemb.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_pemb.columns == train_pemb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stat = pd.DataFrame()\n",
    "# test_stat = pd.DataFrame()\n",
    "# train_stat[UID] = train_user[UID]\n",
    "# test_stat[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats o1\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"train_stats_o1\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         train_stat = pd.merge(train_stat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "#     elif fname.startswith(\"test_stats_o1\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         test_stat = pd.merge(test_stat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats getidxmax\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"train_stats_o2_getidxmax\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         train_stat = pd.merge(train_stat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "#     elif fname.startswith(\"test_stats_o2_getidxmax\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         test_stat = pd.merge(test_stat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to make sure feat and user(target) have same order\n",
    "# # if true --> sum == 0\n",
    "# np.sum(train_stat.index != train_user.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to make sure feat and user(target) have same order\n",
    "# # if true --> sum == 0\n",
    "# np.sum(test_stat.index != test_user.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to make sure train and test features have same order\n",
    "# # if true --> sum == 0\n",
    "# np.sum(train_stat.columns != test_stat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stat.set_index(UID, inplace=True)\n",
    "# test_stat.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna and inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnaninf(val):\n",
    "    return np.mean(val[(~np.isnan(val))&(~np.isinf(val))].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillmean(df):\n",
    "    for col in tqdm(df.columns):\n",
    "        if df[col].count() < len(df):\n",
    "            df[col] = df[col].replace([np.nan, np.inf], nnaninf(df[col]))\n",
    "    # check\n",
    "    for col in tqdm(df.columns):\n",
    "        if df[col].count() < len(df):\n",
    "            log(col)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 219/219 [00:00<00:00, 458.53it/s]\n",
      "100%|| 219/219 [00:00<00:00, 572.51it/s]\n",
      "100%|| 219/219 [00:00<00:00, 231.24it/s]\n",
      "100%|| 219/219 [00:00<00:00, 528.89it/s]\n"
     ]
    }
   ],
   "source": [
    "train_pemb = fillmean(train_pemb)\n",
    "test_pemb = fillmean(test_pemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620639.0 -7.8203125\n",
      "4789123.0 -7.1171875\n"
     ]
    }
   ],
   "source": [
    "print(train_pemb.values.max(), train_pemb.values.min())\n",
    "print(test_pemb.values.max(), test_pemb.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stat = fillmean(train_stat)\n",
    "# test_stat = fillmean(test_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_stat.values.max(), train_stat.values.min())\n",
    "# print(test_stat.values.max(), test_stat.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transmms(tr_df, te_df):\n",
    "    tol_df = pd.concat([tr_df, te_df])\n",
    "    mms = MinMaxScaler()\n",
    "    mms_tol_df = mms.fit_transform(tol_df)\n",
    "    mms_tr_df = mms_tol_df[:len(tr_df)]\n",
    "    mms_te_df = mms_tol_df[len(tr_df):]\n",
    "    \n",
    "    return mms_tr_df, mms_te_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms_tr_df, mms_te_df = transmms(train_pemb, test_pemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mms_tr_stat_df, mms_te_stat_df = transmms(train_stat, test_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002 0.0\n",
      "1.0000000000000002 0.0\n"
     ]
    }
   ],
   "source": [
    "print(mms_tr_df.max(), mms_tr_df.min())\n",
    "print(mms_te_df.max(), mms_te_df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 219)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mms_tr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 219)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mms_te_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 1, 300)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(900000,1,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nn k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_lstm_atten(maxlen, num_classes):\n",
    "    inp_ctid = Input(shape=(1,300))\n",
    "    inp_adid = Input(shape=(1,300))\n",
    "    inp_atid = Input(shape=(1,300))\n",
    "\n",
    "    inp_pdid = Input(shape=(1,300))\n",
    "    inp_pdct = Input(shape=(1,300))\n",
    "    inp_inds = Input(shape=(1,300))\n",
    "\n",
    "    # Stage ctid\n",
    "    x = inp_ctid\n",
    "    # x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp_ctid)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    # z = Conv1D(64, kernel_size = 1, kernel_initializer=initializers.he_uniform(seed=2020), activation = \"tanh\")(y)\n",
    "\n",
    "    atten_1 = Attention(maxlen)(x) # skip connect\n",
    "    atten_2 = Attention(maxlen)(y)\n",
    "    avg_pool_1 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_1 = GlobalMaxPooling1D()(y)\n",
    "    # max_pool1 = GlobalMaxPooling1D()(z)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 3, 5, 8]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage adid\n",
    "    x = inp_adid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_adid)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_3 = Attention(maxlen)(x)\n",
    "    atten_4 = Attention(maxlen)(y)\n",
    "    avg_pool_2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_2 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_1 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_atid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_5 = Attention(maxlen)(x)\n",
    "    atten_6 = Attention(maxlen)(y)\n",
    "    avg_pool_3 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_3 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_pdid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_7 = Attention(maxlen)(x)\n",
    "    atten_8 = Attention(maxlen)(y)\n",
    "    avg_pool_4 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_4 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_pdct\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_9 = Attention(maxlen)(x)\n",
    "    atten_10 = Attention(maxlen)(y)\n",
    "    avg_pool_5 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_5 = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_inds\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_11 = Attention(maxlen)(x)\n",
    "    atten_12 = Attention(maxlen)(y)\n",
    "    avg_pool_6 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_6 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    inp_dctid = Input(shape=(1,300))\n",
    "    inp_dadid = Input(shape=(1,300))\n",
    "    inp_datid = Input(shape=(1,300))\n",
    "\n",
    "    inp_dpdid = Input(shape=(1,300))\n",
    "    inp_dpdct = Input(shape=(1,300))\n",
    "    inp_dinds = Input(shape=(1,300))    \n",
    "\n",
    "\n",
    "    # Stage ctid\n",
    "    x = inp_dctid\n",
    "    # x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp_dctid)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    # z = Conv1D(64, kernel_size = 1, kernel_initializer=initializers.he_uniform(seed=2020), activation = \"tanh\")(y)\n",
    "\n",
    "    atten_13 = Attention(maxlen)(x) # skip connect\n",
    "    atten_14 = Attention(maxlen)(y)\n",
    "    avg_pool_7 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_7 = GlobalMaxPooling1D()(y)\n",
    "    # max_pool1 = GlobalMaxPooling1D()(z)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 3, 5, 8]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage adid\n",
    "    x = inp_dadid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_dadid)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_15 = Attention(maxlen)(x)\n",
    "    atten_16 = Attention(maxlen)(y)\n",
    "    avg_pool_8 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_8 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_1 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_datid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_dusage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_17 = Attention(maxlen)(x)\n",
    "    atten_18 = Attention(maxlen)(y)\n",
    "    avg_pool_9 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_9 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_dpdid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_dusage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_19 = Attention(maxlen)(x)\n",
    "    atten_20 = Attention(maxlen)(y)\n",
    "    avg_pool_10 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_10 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_dpdct\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_dusage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_21 = Attention(maxlen)(x)\n",
    "    atten_22 = Attention(maxlen)(y)\n",
    "    avg_pool_11 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_11 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_dinds\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_dusage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_23 = Attention(maxlen)(x)\n",
    "    atten_24 = Attention(maxlen)(y)\n",
    "    avg_pool_12 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_12 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "    \n",
    "    \n",
    "    # Stage prediction embedding\n",
    "    inp_pemb = Input(shape=(219,))\n",
    "    \n",
    "    aux_1 = Dense(32)(inp_pemb)\n",
    "#     aux_1 = BatchNormalization()(x)\n",
    "\n",
    "    # conc = concatenate([atten_1, atten_2, text_cnn, atten_3, atten_4, text_cnn_1, atten_5, atten_6, text_cnn_2,])\n",
    "    conc = concatenate([atten_1, atten_2, avg_pool_1, max_pool_1,\n",
    "                        atten_3, atten_4, avg_pool_2, max_pool_2,\n",
    "                        atten_5, atten_6, avg_pool_3, max_pool_3,\n",
    "                        atten_7, atten_8, avg_pool_4, max_pool_4,\n",
    "                        atten_9, atten_10, avg_pool_5, max_pool_5,\n",
    "                        atten_11, atten_12, avg_pool_6, max_pool_6,\n",
    "                        atten_13, atten_14, avg_pool_7, max_pool_7,\n",
    "                        atten_15, atten_16, avg_pool_8, max_pool_8,\n",
    "                        atten_17, atten_18, avg_pool_9, max_pool_9,\n",
    "                        atten_19, atten_20, avg_pool_10, max_pool_10,\n",
    "                        atten_21, atten_22, avg_pool_11, max_pool_11,\n",
    "                        atten_23, atten_24, avg_pool_12, max_pool_12,\n",
    "                        aux_1])\n",
    "    \n",
    "    conc = Dense(256)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "\n",
    "    conc = Dense(128)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    # conc = PReLU()(conc)\n",
    "\n",
    "    outp = Dense(num_classes, activation=\"softmax\")(conc)    \n",
    "\n",
    "    model = Model(inputs=[inp_ctid, inp_adid, inp_atid, inp_pdid, inp_pdct, inp_inds, \n",
    "                         inp_dctid, inp_dadid, inp_datid, inp_dpdid, inp_dpdct, inp_dinds,\n",
    "                         inp_pemb, ], outputs=outp)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1, 256)       439296      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1, 256)       329472      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1, 256)       329472      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1, 256)       329472      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1, 256)       329472      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 1, 256)       329472      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1, 128)       123264      bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1, 128)       123264      bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1, 128)       123264      bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1, 128)       123264      bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 1, 128)       123264      bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 1, 128)       123264      bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 219)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 256)          257         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 128)          129         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 256)          257         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 128)          129         bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 256)          257         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 128)          129         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 256)          257         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 128)          129         bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 256)          257         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 128)          129         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 256)          257         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 128)          129         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           7040        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7712)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_13[0][0]               \n",
      "                                                                 attention_14[0][0]               \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_15[0][0]               \n",
      "                                                                 attention_16[0][0]               \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_17[0][0]               \n",
      "                                                                 attention_18[0][0]               \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 attention_19[0][0]               \n",
      "                                                                 attention_20[0][0]               \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 attention_21[0][0]               \n",
      "                                                                 attention_22[0][0]               \n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 attention_23[0][0]               \n",
      "                                                                 attention_24[0][0]               \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          1974528     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,674,658\n",
      "Trainable params: 7,673,890\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "720000/720000 [==============================] - 911s 1ms/step - loss: 1.4115 - acc: 0.4274 - val_loss: 1.3288 - val_acc: 0.4507\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 881s 1ms/step - loss: 1.3325 - acc: 0.4495 - val_loss: 1.3159 - val_acc: 0.4552\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 875s 1ms/step - loss: 1.3142 - acc: 0.4566 - val_loss: 1.3113 - val_acc: 0.4536\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 876s 1ms/step - loss: 1.3013 - acc: 0.4612 - val_loss: 1.3032 - val_acc: 0.4599\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 864s 1ms/step - loss: 1.2901 - acc: 0.4655 - val_loss: 1.3030 - val_acc: 0.4601\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 873s 1ms/step - loss: 1.2791 - acc: 0.4692 - val_loss: 1.3014 - val_acc: 0.4593\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 880s 1ms/step - loss: 1.2683 - acc: 0.4739 - val_loss: 1.3040 - val_acc: 0.4605\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 873s 1ms/step - loss: 1.2565 - acc: 0.4778 - val_loss: 1.3069 - val_acc: 0.4582\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 866s 1ms/step - loss: 1.2442 - acc: 0.4827 - val_loss: 1.3150 - val_acc: 0.4562\n",
      "180000/180000 [==============================] - 52s 289us/step\n",
      "1000000/1000000 [==============================] - 270s 270us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1, 256)       439296      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1, 256)       329472      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1, 256)       329472      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1, 256)       329472      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1, 256)       329472      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 1, 256)       329472      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1, 128)       123264      bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1, 128)       123264      bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1, 128)       123264      bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1, 128)       123264      bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 1, 128)       123264      bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 1, 128)       123264      bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 219)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 256)          257         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 128)          129         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 256)          257         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 128)          129         bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 256)          257         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 128)          129         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 256)          257         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 128)          129         bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 256)          257         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 128)          129         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 256)          257         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 128)          129         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           7040        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7712)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_13[0][0]               \n",
      "                                                                 attention_14[0][0]               \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_15[0][0]               \n",
      "                                                                 attention_16[0][0]               \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_17[0][0]               \n",
      "                                                                 attention_18[0][0]               \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 attention_19[0][0]               \n",
      "                                                                 attention_20[0][0]               \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 attention_21[0][0]               \n",
      "                                                                 attention_22[0][0]               \n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 attention_23[0][0]               \n",
      "                                                                 attention_24[0][0]               \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          1974528     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,674,658\n",
      "Trainable params: 7,673,890\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   400/720000 [..............................] - ETA: 11:57:37 - loss: 3.1257 - acc: 0.1075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.267313). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 908s 1ms/step - loss: 1.4204 - acc: 0.4256 - val_loss: 1.3424 - val_acc: 0.4453\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 890s 1ms/step - loss: 1.3353 - acc: 0.4496 - val_loss: 1.3148 - val_acc: 0.4576\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 891s 1ms/step - loss: 1.3172 - acc: 0.4557 - val_loss: 1.3162 - val_acc: 0.4546\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 889s 1ms/step - loss: 1.3043 - acc: 0.4600 - val_loss: 1.3061 - val_acc: 0.4569\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 891s 1ms/step - loss: 1.2932 - acc: 0.4641 - val_loss: 1.3062 - val_acc: 0.4599\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 894s 1ms/step - loss: 1.2830 - acc: 0.4679 - val_loss: 1.3072 - val_acc: 0.4572\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 898s 1ms/step - loss: 1.2727 - acc: 0.4721 - val_loss: 1.3029 - val_acc: 0.4616\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 897s 1ms/step - loss: 1.2613 - acc: 0.4757 - val_loss: 1.3068 - val_acc: 0.4577\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 915s 1ms/step - loss: 1.2499 - acc: 0.4802 - val_loss: 1.3105 - val_acc: 0.4596\n",
      "Epoch 10/50\n",
      "720000/720000 [==============================] - 934s 1ms/step - loss: 1.2368 - acc: 0.4853 - val_loss: 1.3196 - val_acc: 0.4564\n",
      "180000/180000 [==============================] - 61s 337us/step\n",
      "1000000/1000000 [==============================] - 317s 317us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1, 256)       439296      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1, 256)       329472      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1, 256)       329472      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1, 256)       329472      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1, 256)       329472      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 1, 256)       329472      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1, 128)       123264      bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1, 128)       123264      bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1, 128)       123264      bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1, 128)       123264      bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 1, 128)       123264      bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 1, 128)       123264      bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 219)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 256)          257         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 128)          129         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 256)          257         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 128)          129         bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 256)          257         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 128)          129         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 256)          257         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 128)          129         bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 256)          257         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 128)          129         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 256)          257         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 128)          129         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           7040        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7712)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_13[0][0]               \n",
      "                                                                 attention_14[0][0]               \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_15[0][0]               \n",
      "                                                                 attention_16[0][0]               \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_17[0][0]               \n",
      "                                                                 attention_18[0][0]               \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 attention_19[0][0]               \n",
      "                                                                 attention_20[0][0]               \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 attention_21[0][0]               \n",
      "                                                                 attention_22[0][0]               \n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 attention_23[0][0]               \n",
      "                                                                 attention_24[0][0]               \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          1974528     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,674,658\n",
      "Trainable params: 7,673,890\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   400/720000 [..............................] - ETA: 12:14:42 - loss: 3.0505 - acc: 0.0975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.280202). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 959s 1ms/step - loss: 1.4214 - acc: 0.4262 - val_loss: 1.3604 - val_acc: 0.4368\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 934s 1ms/step - loss: 1.3339 - acc: 0.4495 - val_loss: 1.3215 - val_acc: 0.4510\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 937s 1ms/step - loss: 1.3159 - acc: 0.4563 - val_loss: 1.3136 - val_acc: 0.4556\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 935s 1ms/step - loss: 1.3036 - acc: 0.4607 - val_loss: 1.3107 - val_acc: 0.4568\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 944s 1ms/step - loss: 1.2929 - acc: 0.4646 - val_loss: 1.3078 - val_acc: 0.4566\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 949s 1ms/step - loss: 1.2829 - acc: 0.4690 - val_loss: 1.3037 - val_acc: 0.4596\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 948s 1ms/step - loss: 1.2732 - acc: 0.4717 - val_loss: 1.3051 - val_acc: 0.4589\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 947s 1ms/step - loss: 1.2621 - acc: 0.4758 - val_loss: 1.3113 - val_acc: 0.4563\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 938s 1ms/step - loss: 1.2507 - acc: 0.4802 - val_loss: 1.3157 - val_acc: 0.4560\n",
      "180000/180000 [==============================] - 61s 341us/step\n",
      "1000000/1000000 [==============================] - 321s 321us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1, 256)       439296      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1, 256)       329472      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1, 256)       329472      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1, 256)       329472      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1, 256)       329472      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 1, 256)       329472      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1, 128)       123264      bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1, 128)       123264      bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1, 128)       123264      bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1, 128)       123264      bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 1, 128)       123264      bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 1, 128)       123264      bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 219)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 256)          257         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 128)          129         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 256)          257         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 128)          129         bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 256)          257         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 128)          129         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 256)          257         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 128)          129         bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 256)          257         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 128)          129         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 256)          257         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 128)          129         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           7040        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7712)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_13[0][0]               \n",
      "                                                                 attention_14[0][0]               \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_15[0][0]               \n",
      "                                                                 attention_16[0][0]               \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_17[0][0]               \n",
      "                                                                 attention_18[0][0]               \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 attention_19[0][0]               \n",
      "                                                                 attention_20[0][0]               \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 attention_21[0][0]               \n",
      "                                                                 attention_22[0][0]               \n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 attention_23[0][0]               \n",
      "                                                                 attention_24[0][0]               \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          1974528     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,674,658\n",
      "Trainable params: 7,673,890\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2638\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2639\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2640\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'bidirectional_18/while_1/add_1' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2642\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'bidirectional_18/while_1/add_1' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m     \u001b[0m__getattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF_Output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m_swig_getattr\u001b[0;34m(self, class_type, name)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swig_getattr_nondynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m_swig_getattr_nondynamic\u001b[0;34m(self, class_type, name, static)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'object' has no attribute '__getattr__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-bc573cabc895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mpred_val_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mpred_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0mfit_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0mfit_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    315\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-ac6073736afb>\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   3022\u001b[0m     \"\"\"\n\u001b[1;32m   3023\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_tf_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 731\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    732\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    401\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 731\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    732\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_AddGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1002\u001b[0m   \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m   \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_input_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mbroadcast_gradient_args\u001b[0;34m(s0, s1, name)\u001b[0m\n\u001b[1;32m    827\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 829\u001b[0;31m         \"BroadcastGradientArgs\", s0=s0, s1=s1, name=name)\n\u001b[0m\u001b[1;32m    830\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3614\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3616\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3617\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3618\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_control_flow_post_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_control_flow_post_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_control_flow_post_processing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2052\u001b[0m       \u001b[0mcontrol_flow_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckInputFromValidContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_control_flow_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2054\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_control_flow_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2056\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reconstruct_sequence_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAddOp\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m   2481\u001b[0m             \u001b[0mop_input_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AddOpInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2482\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2483\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AddOpInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_AddOpInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_AddOpInternal\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m   2502\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m         \u001b[0mreal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreal_x\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_x\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAddValue\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m   2429\u001b[0m               \u001b[0mforward_ctxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mforward_ctxt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgrad_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2431\u001b[0;31m             \u001b[0mreal_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetRealValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_external_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2433\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreal_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mGetRealValue\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1024\u001b[0m           \u001b[0;31m# Record the history of this value in forward_ctxt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m           \u001b[0mhistory_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_grad_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddForwardAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAddForwardAccumulator\u001b[0;34m(self, value, dead_branch)\u001b[0m\n\u001b[1;32m    891\u001b[0m                 value, self.forward_context)\n\u001b[1;32m    892\u001b[0m           acc = gen_data_flow_ops.stack_v2(\n\u001b[0;32m--> 893\u001b[0;31m               max_size=max_size, elem_type=value.dtype.base_dtype, name=\"f_acc\")\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurr_ctxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0mcurr_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\u001b[0m in \u001b[0;36mstack_v2\u001b[0;34m(max_size, elem_type, stack_name, name)\u001b[0m\n\u001b[1;32m   5607\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5608\u001b[0m         \u001b[0;34m\"StackV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5609\u001b[0;31m                    stack_name=stack_name, name=name)\n\u001b[0m\u001b[1;32m   5610\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5611\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3614\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3616\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3617\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3618\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2025\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   2026\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 2027\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_AddInputList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m       \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_AddInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m   \u001b[0;31m# Add control inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_tf_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;31m# cache of executor(s) stored for every session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/c_api_util.py\u001b[0m in \u001b[0;36mtf_output\u001b[0;34m(c_op, index)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mWrapped\u001b[0m \u001b[0mTF_Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \"\"\"\n\u001b[0;32m--> 186\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m   \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_TF_Output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]\n",
    "maxlen=200\n",
    "# for age\n",
    "num_classes = 10\n",
    "train_y = np_utils.to_categorical(train_user[\"age\"]-1)\n",
    "# # for gender\n",
    "# num_classes = 2\n",
    "# train_y = np_utils.to_categorical(train_user[\"gender\"]-1)\n",
    "\n",
    "test_set = [test_ctid, test_adid, test_atid, test_pdid, test_pdct, test_inds, \n",
    "            test_dctid, test_dadid, test_datid, test_dpdid, test_dpdct, test_dinds, \n",
    "           mms_te_df, ]\n",
    "predicted_train_age = np.zeros(train_y.shape)\n",
    "predicted_age = np.zeros((len(test_user),num_classes))\n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_ctid, train_y))\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    K.clear_session()\n",
    "\n",
    "    X_train_ctid = train_ctid[train_idx]\n",
    "    X_train_adid = train_adid[train_idx]\n",
    "    X_train_atid = train_atid[train_idx]\n",
    "    X_train_pdid = train_pdid[train_idx]\n",
    "    X_train_pdct = train_pdct[train_idx]\n",
    "    X_train_inds = train_inds[train_idx]\n",
    "    \n",
    "    X_train_pemb = mms_tr_df[train_idx]\n",
    "    \n",
    "    X_train_dctid = train_dctid[train_idx]\n",
    "    X_train_dadid = train_dadid[train_idx]\n",
    "    X_train_datid = train_datid[train_idx]\n",
    "    X_train_dpdid = train_dpdid[train_idx]\n",
    "    X_train_dpdct = train_dpdct[train_idx]\n",
    "    X_train_dinds = train_dinds[train_idx]\n",
    "\n",
    "    X_val_train_ctid = train_ctid[valid_idx]\n",
    "    X_val_train_adid = train_adid[valid_idx]\n",
    "    X_val_train_atid = train_atid[valid_idx]\n",
    "    X_val_train_pdid = train_pdid[valid_idx]\n",
    "    X_val_train_pdct = train_pdct[valid_idx]\n",
    "    X_val_train_inds = train_inds[valid_idx]\n",
    "    \n",
    "    X_val_train_pemb = mms_tr_df[valid_idx]\n",
    "\n",
    "    X_val_train_dctid = train_dctid[valid_idx]\n",
    "    X_val_train_dadid = train_dadid[valid_idx]\n",
    "    X_val_train_datid = train_datid[valid_idx]\n",
    "    X_val_train_dpdid = train_dpdid[valid_idx]\n",
    "    X_val_train_dpdct = train_dpdct[valid_idx]\n",
    "    X_val_train_dinds = train_dinds[valid_idx]\n",
    "\n",
    "\n",
    "    y_train = train_y[train_idx]\n",
    "    y_val = train_y[valid_idx]\n",
    "\n",
    "    X_train_set = [X_train_ctid, X_train_adid, X_train_atid, X_train_pdid, X_train_pdct, X_train_inds, \n",
    "                  X_train_dctid, X_train_dadid, X_train_datid, X_train_dpdid, X_train_dpdct, X_train_dinds,\n",
    "                  X_train_pemb, ]\n",
    "    X_val_set = [X_val_train_ctid, X_val_train_adid, X_val_train_atid, X_val_train_pdid, X_val_train_pdct, X_val_train_inds, \n",
    "                X_val_train_dctid, X_val_train_dadid, X_val_train_datid, X_val_train_dpdid, X_val_train_dpdct, X_val_train_dinds, \n",
    "                X_val_train_pemb, ]\n",
    "\n",
    "    model = sim_lstm_atten(maxlen, num_classes)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])\n",
    "    print(\"MODEL COMPLIE...\")\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train_set, y_train, batch_size=200, epochs=50, validation_data=(X_val_set, y_val), callbacks=cb, verbose=1)\n",
    "    pred_val_y = model.predict(X_val_set, batch_size=200, verbose=1)\n",
    "    pred_test_y = model.predict(test_set, batch_size=200, verbose=1)\n",
    "    predicted_train_age[valid_idx] = pred_val_y\n",
    "    predicted_age += pred_test_y / len(splits)\n",
    "    \n",
    "accuracy_score(np.argmax(train_y,axis=1),np.argmax(predicted_train_age,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     253015\n",
       "2     176461\n",
       "4     172392\n",
       "5     131855\n",
       "6     125634\n",
       "7      53348\n",
       "1      26667\n",
       "8      25059\n",
       "9      24135\n",
       "10     11434\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.argmax(predicted_age, axis=1)+1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4562333333333333\n",
      "0.45637222222222223\n",
      "0.4559666666666667\n",
      "0.03961666666666667\n",
      "0.039127777777777775\n"
     ]
    }
   ],
   "source": [
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print(accuracy_score(np.argmax(train_y[valid_idx],axis=1),np.argmax(predicted_train_age[valid_idx],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4561907407407408"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.4562333333333333+0.45637222222222223+0.4559666666666667)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1, 256)       439296      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1, 256)       329472      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1, 256)       329472      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1, 256)       329472      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1, 256)       329472      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 1, 256)       329472      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1, 128)       123264      bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1, 128)       123264      bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1, 128)       123264      bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1, 128)       123264      bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 1, 128)       123264      bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 1, 128)       123264      bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 219)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 256)          257         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 128)          129         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 256)          257         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 128)          129         bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 256)          257         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 128)          129         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 256)          257         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 128)          129         bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 256)          257         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 128)          129         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 256)          257         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 128)          129         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           7040        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7712)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_13[0][0]               \n",
      "                                                                 attention_14[0][0]               \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_15[0][0]               \n",
      "                                                                 attention_16[0][0]               \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_17[0][0]               \n",
      "                                                                 attention_18[0][0]               \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 attention_19[0][0]               \n",
      "                                                                 attention_20[0][0]               \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 attention_21[0][0]               \n",
      "                                                                 attention_22[0][0]               \n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 attention_23[0][0]               \n",
      "                                                                 attention_24[0][0]               \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          1974528     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,673,626\n",
      "Trainable params: 7,672,858\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   400/720000 [..............................] - ETA: 12:25:53 - loss: 0.8669 - acc: 0.5075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.278560). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 983s 1ms/step - loss: 0.1898 - acc: 0.9317 - val_loss: 0.1706 - val_acc: 0.9387\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 958s 1ms/step - loss: 0.1743 - acc: 0.9377 - val_loss: 0.1689 - val_acc: 0.9398\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 959s 1ms/step - loss: 0.1705 - acc: 0.9390 - val_loss: 0.1676 - val_acc: 0.9405\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 967s 1ms/step - loss: 0.1675 - acc: 0.9401 - val_loss: 0.1683 - val_acc: 0.9397\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 958s 1ms/step - loss: 0.1647 - acc: 0.9412 - val_loss: 0.1650 - val_acc: 0.9410\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 959s 1ms/step - loss: 0.1617 - acc: 0.9422 - val_loss: 0.1649 - val_acc: 0.9414\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 964s 1ms/step - loss: 0.1590 - acc: 0.9435 - val_loss: 0.1664 - val_acc: 0.9406\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 966s 1ms/step - loss: 0.1558 - acc: 0.9445 - val_loss: 0.1671 - val_acc: 0.9405\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 963s 1ms/step - loss: 0.1523 - acc: 0.9459 - val_loss: 0.1688 - val_acc: 0.9398\n",
      "180000/180000 [==============================] - 63s 347us/step\n",
      "1000000/1000000 [==============================] - 326s 326us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1, 256)       439296      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1, 256)       329472      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1, 256)       329472      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1, 256)       329472      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1, 256)       329472      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 1, 256)       329472      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1, 128)       123264      bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1, 128)       123264      bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1, 128)       123264      bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1, 128)       123264      bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 1, 128)       123264      bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 1, 128)       123264      bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 219)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 256)          257         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 128)          129         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 256)          257         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 128)          129         bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 256)          257         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 128)          129         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 256)          257         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 128)          129         bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 256)          257         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 128)          129         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 256)          257         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 128)          129         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           7040        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7712)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_13[0][0]               \n",
      "                                                                 attention_14[0][0]               \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_15[0][0]               \n",
      "                                                                 attention_16[0][0]               \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_17[0][0]               \n",
      "                                                                 attention_18[0][0]               \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 attention_19[0][0]               \n",
      "                                                                 attention_20[0][0]               \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 attention_21[0][0]               \n",
      "                                                                 attention_22[0][0]               \n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 attention_23[0][0]               \n",
      "                                                                 attention_24[0][0]               \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          1974528     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,673,626\n",
      "Trainable params: 7,672,858\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   400/720000 [..............................] - ETA: 12:05:57 - loss: 0.9175 - acc: 0.5500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.271523). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 961s 1ms/step - loss: 0.1911 - acc: 0.9314 - val_loss: 0.1730 - val_acc: 0.9382\n",
      "Epoch 2/50\n",
      " 74200/720000 [==>...........................] - ETA: 13:11 - loss: 0.1719 - acc: 0.9385"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-726fe23c4675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mpred_val_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mpred_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]\n",
    "\n",
    "maxlen=200\n",
    "# for age\n",
    "num_classes = 2\n",
    "train_y = np_utils.to_categorical(train_user[\"gender\"]-1)\n",
    "# # for gender\n",
    "# num_classes = 2\n",
    "# train_y = np_utils.to_categorical(train_user[\"gender\"]-1)\n",
    "\n",
    "test_set = [test_ctid, test_adid, test_atid, test_pdid, test_pdct, test_inds, \n",
    "            test_dctid, test_dadid, test_datid, test_dpdid, test_dpdct, test_dinds, \n",
    "           mms_te_df, ]\n",
    "predicted_train_gender = np.zeros(train_y.shape)\n",
    "predicted_gender = np.zeros((len(test_user),num_classes))\n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_ctid, train_y))\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    K.clear_session()\n",
    "\n",
    "    X_train_ctid = train_ctid[train_idx]\n",
    "    X_train_adid = train_adid[train_idx]\n",
    "    X_train_atid = train_atid[train_idx]\n",
    "    X_train_pdid = train_pdid[train_idx]\n",
    "    X_train_pdct = train_pdct[train_idx]\n",
    "    X_train_inds = train_inds[train_idx]\n",
    "    \n",
    "    X_train_pemb = mms_tr_df[train_idx]\n",
    "    \n",
    "    X_train_dctid = train_dctid[train_idx]\n",
    "    X_train_dadid = train_dadid[train_idx]\n",
    "    X_train_datid = train_datid[train_idx]\n",
    "    X_train_dpdid = train_dpdid[train_idx]\n",
    "    X_train_dpdct = train_dpdct[train_idx]\n",
    "    X_train_dinds = train_dinds[train_idx]\n",
    "\n",
    "    X_val_train_ctid = train_ctid[valid_idx]\n",
    "    X_val_train_adid = train_adid[valid_idx]\n",
    "    X_val_train_atid = train_atid[valid_idx]\n",
    "    X_val_train_pdid = train_pdid[valid_idx]\n",
    "    X_val_train_pdct = train_pdct[valid_idx]\n",
    "    X_val_train_inds = train_inds[valid_idx]\n",
    "    \n",
    "    X_val_train_pemb = mms_tr_df[valid_idx]\n",
    "\n",
    "    X_val_train_dctid = train_dctid[valid_idx]\n",
    "    X_val_train_dadid = train_dadid[valid_idx]\n",
    "    X_val_train_datid = train_datid[valid_idx]\n",
    "    X_val_train_dpdid = train_dpdid[valid_idx]\n",
    "    X_val_train_dpdct = train_dpdct[valid_idx]\n",
    "    X_val_train_dinds = train_dinds[valid_idx]\n",
    "\n",
    "\n",
    "    y_train = train_y[train_idx]\n",
    "    y_val = train_y[valid_idx]\n",
    "\n",
    "    X_train_set = [X_train_ctid, X_train_adid, X_train_atid, X_train_pdid, X_train_pdct, X_train_inds, \n",
    "                  X_train_dctid, X_train_dadid, X_train_datid, X_train_dpdid, X_train_dpdct, X_train_dinds,\n",
    "                  X_train_pemb, ]\n",
    "    X_val_set = [X_val_train_ctid, X_val_train_adid, X_val_train_atid, X_val_train_pdid, X_val_train_pdct, X_val_train_inds, \n",
    "                X_val_train_dctid, X_val_train_dadid, X_val_train_datid, X_val_train_dpdid, X_val_train_dpdct, X_val_train_dinds, \n",
    "                X_val_train_pemb, ]\n",
    "\n",
    "    model = sim_lstm_atten(maxlen, num_classes)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])\n",
    "    print(\"MODEL COMPLIE...\")\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train_set, y_train, batch_size=200, epochs=50, validation_data=(X_val_set, y_val), callbacks=cb, verbose=1)\n",
    "    pred_val_y = model.predict(X_val_set, batch_size=200, verbose=1)\n",
    "    pred_test_y = model.predict(test_set, batch_size=200, verbose=1)\n",
    "    predicted_train_gender[valid_idx] = pred_val_y\n",
    "    predicted_gender += pred_test_y / len(splits)\n",
    "\n",
    "accuracy_score(np.argmax(train_y,axis=1),np.argmax(predicted_train_gender,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9398\n",
      "0.6710166666666667\n",
      "0.6699944444444444\n",
      "0.6692166666666667\n",
      "0.6689833333333334\n"
     ]
    }
   ],
   "source": [
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print(accuracy_score(np.argmax(train_y[valid_idx],axis=1),np.argmax(predicted_train_gender[valid_idx],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    679272\n",
       "2    320728\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.argmax(predicted_gender, axis=1)+1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4109907407407407"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9398 + 0.4561907407407408 + 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: \n",
    "# 1. add stats feature and prediction embedding\n",
    "# 2. optimize embedding\n",
    "# 3. adjust network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4299088888888889 + 0.9316455555555555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "1. add stats feature\n",
    "0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. optimize embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{UDDIR}/imd/seqidx_dic.pkl\", \"rb\") as f:\n",
    "    seqidx_dic = pickle.loads(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_ids = list(combinations(ID_SET, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_eq = True\n",
    "for cp in comb_ids:\n",
    "    if (seqidx_dic[cp[0]] != seqidx_dic[cp[1]]).sum() != 0:\n",
    "        print(cp)\n",
    "        is_eq = False\n",
    "        break\n",
    "assert is_eq, \"Must True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder index\n",
    "re_train_user = pd.DataFrame(index=seqidx_dic[\"creative_id\"][seqidx_dic[\"creative_id\"].isin(train_user[UID])])\n",
    "re_train_user = pd.merge(re_train_user, train_user, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder index\n",
    "re_test_user = pd.DataFrame(index=seqidx_dic[\"creative_id\"][~seqidx_dic[\"creative_id\"].isin(train_user[UID])])\n",
    "re_test_user = pd.merge(re_test_user, test_user, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np_utils.to_categorical(re_train_user[\"age\"]-1)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"creative_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_val = train_seq[col].values\n",
    "test_seq_val = test_seq[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_seq_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = tokenizer.texts_to_sequences(train_seq_val)\n",
    "test_X = tokenizer.texts_to_sequences(test_seq_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=35)\n",
    "test_X = pad_sequences(test_X, maxlen=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(train_X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(35,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_1 = Embedding(35, 300)(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #     lstm_1 = LSTM(256)\n",
    "# dense_1 = Dense(128, kernel_initializer='normal', activation='relu')(embed_1)\n",
    "# #     dense_1 = Dropout(0.5)(dense_1)\n",
    "# dense_2 = Dense(64, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "# #     dense_2 = Dropout(0.5)(dense_2)\n",
    "# dense_3 = Dense(32, kernel_initializer='normal', activation='relu')(dense_2)\n",
    "# #     dense_3 = Dropout(0.5)(dense_3)\n",
    "# out     = Dense(10,kernel_initializer='normal', activation='softmax')(dense_3)\n",
    "# Compile model\n",
    "# model = Model(inputs=input1, outputs = input1)\n",
    "# model.compile(loss ='categorical_crossentropy', optimizer=\"adadelta\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     train_X, \n",
    "#     train_y, \n",
    "#     batch_size=2048, \n",
    "#     epochs=10, \n",
    "# #     validation_data=([train_X], train_y), \n",
    "#     callbacks = cb, \n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1024\n",
    "model = Sequential()\n",
    "model.add(Embedding(2481135, output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(2481116, 64))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X[:1000], train_y[:1000], batch_size=2048, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = model.predict(train_X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "assert output_array.shape == (32, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_seq[col].values[:10000], train_seq[col].values[:10000], batch_size=2048, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_softmax= model.predict(test_X[:10000], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.argmax(res_softmax, axis=1)+1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "mms_tol_feat = mms.fit_transform(tol_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mms_tr_feat = mms_tol_feat[:len(train_feat)]\n",
    "mms_te_feat = mms_tol_feat[len(train_feat):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every category\n",
    "# pd.Series(np.argmax(train_y, axis=1)+1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "st_X_train, st_X_val, st_y_train, st_y_val = train_test_split(mms_tr_feat, train_y, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model(inshape=(st_X_train.shape[1],), outshape=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    st_X_train, \n",
    "    st_y_train, \n",
    "    batch_size=2048, \n",
    "    epochs=10, \n",
    "    validation_data=([st_X_val], st_y_val), \n",
    "    callbacks = cb, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(st_X_val, st_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_softmax= model.predict(test_feat, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_age = np.argmax(res_softmax, axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = model.predict(st_X_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 9000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 300 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"creative_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.DataFrame()\n",
    "test_X = pd.DataFrame()\n",
    "embedding_matrix = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\"]:\n",
    "    cur_train_X, cur_test_X, word_index = loadnprec(col, maxlen=maxlen)\n",
    "    cur_embedding_matrix = load_glove(col, word_index, max_features)\n",
    "    # concat\n",
    "    if len(train_X):\n",
    "        train_X = np.concatenate((train_X, cur_train_X), axis=1)\n",
    "    else:\n",
    "        train_X = cur_train_X\n",
    "    # concat\n",
    "    if len(test_X):\n",
    "        test_X = np.concatenate((test_X, cur_test_X), axis=1)\n",
    "    else:\n",
    "        test_X = cur_test_X\n",
    "    # concat\n",
    "    if len(embedding_matrix):\n",
    "        embedding_matrix = np.concatenate((embedding_matrix, cur_embedding_matrix), axis=1)\n",
    "    else:\n",
    "        embedding_matrix = cur_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = copy.deepcopy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = copy.deepcopy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_atten(max_features, embedding_matrix):\n",
    "    \n",
    "    inp_active = Input(shape=(maxlen,))\n",
    "    \n",
    "    # Stage ctid\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp_active)\n",
    "    x = Position_Embedding()(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    z = Conv1D(64, kernel_size = 1, kernel_initializer=initializers.he_uniform(seed=2020), activation = \"tanh\")(y)\n",
    "    \n",
    "    atten_1 = Attention(maxlen)(x) # skip connect\n",
    "    atten_2 = Attention(maxlen)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    max_pool1 = GlobalMaxPooling1D()(z)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2, 3, 5, 8]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    text_cnn = concatenate(convs, axis=1)\n",
    "\n",
    "    \n",
    "    # Stage adid\n",
    "    x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    x = Position_Embedding()(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    atten_3 = Attention(maxlen)(x)\n",
    "    atten_4 = Attention(maxlen)(y)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2, 4, 6, 10]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    text_cnn_1 = concatenate(convs, axis=1)\n",
    "    \n",
    "    # Stage atid\n",
    "    x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    x = Position_Embedding()(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    atten_3 = Attention(maxlen)(x)\n",
    "    atten_4 = Attention(maxlen)(y)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2, 4, 6, 10]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    text_cnn_1 = concatenate(convs, axis=1)    \n",
    "    \n",
    "\n",
    "      \n",
    "    \n",
    "#     x = Dense(32)(aux)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool, max_pool1, text_cnn,])\n",
    "    conc = Dense(256)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    \n",
    "    conc = Dense(128)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "#     conc = PReLU()(conc)\n",
    "\n",
    "    outp = Dense(num_classes, activation=\"softmax\")(conc)    \n",
    "\n",
    "    model = Model(inputs=[inp_active], outputs=outp)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inshape = (train_X.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]\n",
    "\n",
    "train_meta = np.zeros(train_y.shape)\n",
    "test_meta = np.zeros((test_X.shape[0],num_classes))\n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_X, train_y))\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    K.clear_session()\n",
    "    X_train = train_X[train_idx]\n",
    "    X_val = train_X[valid_idx]\n",
    "    y_train = train_y[train_idx]\n",
    "    y_val = train_y[valid_idx]\n",
    "    \n",
    "#     Aux_train = aux_train[train_idx]|\n",
    "#     Aux_val = aux_train[valid_idx]\n",
    "    \n",
    "    model = model_lstm_atten(max_features, embedding_matrix)\n",
    "#     model = multi_gpu_model(model, gpus=8)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])\n",
    "    print(\"MODEL COMPLIE...\")\n",
    "#     lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "#     lookahead.inject(model) # add into model\n",
    "#     print(\"LOOKAHEAD INJECT...\")\n",
    "    model.summary()\n",
    "    model.fit([X_train], y_train, batch_size=1024, epochs=1, validation_data=([X_val], y_val), callbacks = cb, verbose=1)\n",
    "    pred_val_y = model.predict([X_val], batch_size=2048, verbose=1)\n",
    "    pred_test_y = model.predict([test_X], batch_size=2048, verbose=1)\n",
    "    train_meta[valid_idx] = pred_val_y\n",
    "    test_meta += pred_test_y / len(splits)\n",
    "\n",
    "print(accuracy_score(np.argmax(train_y,axis=1),np.argmax(train_meta,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(K.name_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(K.variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb_multi_age = lgb.Booster(model_file=f\"{UMDIR}/lgb_multi_age_20200511045531.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb_multi_gender = lgb.Booster(model_file=f\"{UMDIR}/lgb_multi_gender_20200511034408.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_user[[UID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res[\"predicted_age\"] = np.argmax(predicted_age, axis=1)+1\n",
    "# res[\"predicted_age\"] = predicted_age\n",
    "# res[\"predicted_age\"] = [list(x).index(max(x))+1 for x in model_lgb_multi_age.predict(test_feat, num_iteration=model_lgb_multi_age.best_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"] = np.argmax(predicted_gender, axis=1)+1\n",
    "# res[\"predicted_gender\"] = [list(x).index(max(x))+1 for x in model_lgb_multi_gender.predict(test_feat, num_iteration=model_lgb_multi_gender.best_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_suffix = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))\n",
    "res.to_csv(f\"{RESDIR}/res-{res_suffix}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     253015\n",
       "2     176461\n",
       "4     172392\n",
       "5     131855\n",
       "6     125634\n",
       "7      53348\n",
       "1      26667\n",
       "8      25059\n",
       "9      24135\n",
       "10     11434\n",
       "Name: predicted_age, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    679272\n",
       "2    320728\n",
       "Name: predicted_gender, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(f\"{RESDIR}/res-20200528143657.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"] = tmp[\"predicted_gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"predicted_age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cent result to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ti import session\n",
    "ti_session = session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ti_session.upload_data(path=f\"{RESDIR}/res-20200515004850.csv\", bucket=\"etveritas-1252104022\", key_prefix=RESDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
