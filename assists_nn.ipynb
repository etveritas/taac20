{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if used pip install package\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install wget\n",
    "# !pip install gensim\n",
    "# !pip install catboost\n",
    "# !pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import wget\n",
    "import time\n",
    "import tarfile\n",
    "import zipfile\n",
    "import functools\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from itertools import product, combinations\n",
    "from scipy.special import comb, perm\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cbt\n",
    "from glove import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression,BayesianRidge,SGDClassifier,PassiveAggressiveClassifier,RidgeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC,NuSVC,SVC\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, LSTM, GRU  #, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import gensim\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# ray.init(object_store_memory=int(100e6))\n",
    "# import modin.pandas as pd\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, filename='default.log', stream=sys.stdout):\n",
    "        self.terminal = stream\n",
    "        self.log = open(filename, 'a')\n",
    "        \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        \n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.stdout = Logger(\"logs/default.log\", sys.stdout)\n",
    "# sys.stderr = Logger(\"logs/default_err.log\", sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDIR = \"data\"\n",
    "UDDIR = \"user_data\"\n",
    "UFEDIR = \"user_data/feat_data_v05\"\n",
    "UMDIR = \"user_data/model_data\"\n",
    "RESDIR = \"prediction_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UID = \"user_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (Only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/train_preliminary.zip\", out=DDIR)\n",
    "# test_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/test.zip\", out=DDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myunzip(filename):\n",
    "#     zFile = zipfile.ZipFile(filename, \"r\")\n",
    "#     for fileM in zFile.namelist(): \n",
    "#         zFile.extract(fileM, DDIR)\n",
    "#         print(fileM)\n",
    "#     zFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myunzip(train_fname)\n",
    "# myunzip(test_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bch_rencol(values, prefix=\"\", suffix=\"\"):\n",
    "    return list(map(lambda x: f\"{prefix}\"+\"_\".join(list(map(lambda y: str(y), x)))+f\"{suffix}\" \n",
    "                    if hasattr(x, \"__iter__\") and not isinstance(x, str) \n",
    "                    else f\"{prefix}\"+str(x)+f\"{suffix}\", values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynunique(values):\n",
    "    return values.nunique(dropna=False)\n",
    "def getidxmax(x):\n",
    "    return x.idxmax()[1]\n",
    "# for time series\n",
    "def at_len(x):\n",
    "    return len(x)\n",
    "\n",
    "def at_sum(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def at_max(x):\n",
    "    return np.max(x)\n",
    "\n",
    "def at_min(x):\n",
    "    return np.min(x)\n",
    "\n",
    "def at_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def at_range(x):\n",
    "    return at_max(x) - at_min(x)\n",
    "\n",
    "def at_nunq(x):\n",
    "    return len(set(x))\n",
    "\n",
    "def at_lenDrange(x):\n",
    "    return at_len(x)/(at_range(x)+1)\n",
    "\n",
    "def at_lenDnunq(x):\n",
    "    return at_len(x)/at_nunq(x)\n",
    "\n",
    "def at_percentile(n):\n",
    "    def at_percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    at_percentile_.__name__ = f\"at_percentile_{n}\"\n",
    "    return at_percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_SET = [\"sum\", \"max\", \"min\", \"mean\", \"std\"]\n",
    "OP_SET1 = [\"nunique\", \"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"skew\", at_percentile(0.25), at_percentile(0.75)]\n",
    "OP_SET2 = [\"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"skew\", at_percentile(0.25), at_percentile(0.75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_SET = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\", \"time\", \"click_times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting_level = 0\n",
    "is_start = None\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        self.history = [self.start]\n",
    "\n",
    "    def check(self, info):\n",
    "        current = time.time()\n",
    "        print(f\"[{info}] spend {current - self.history[-1]:0.2f} sec\")\n",
    "        self.history.append(current)\n",
    "\n",
    "def log(entry):\n",
    "    global nesting_level\n",
    "    space = \"-\" * (4 * nesting_level)\n",
    "    print(f\"{space}{entry}\")\n",
    "\n",
    "def timeit(method, start_log=None):\n",
    "    @functools.wraps(method)\n",
    "    def timed(*args, **kw):\n",
    "        global is_start\n",
    "        global nesting_level\n",
    "\n",
    "        if not is_start:\n",
    "            print()\n",
    "\n",
    "        is_start = True\n",
    "        log(f\"Start [{method.__name__}]:\" + (start_log if start_log else \"\"))\n",
    "        log(f'Start time: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        nesting_level += 1\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        end_time = time.time()\n",
    "\n",
    "        nesting_level -= 1\n",
    "        log(f\"End   [{method.__name__}]. Time elapsed: {end_time - start_time:0.2f} sec.\")\n",
    "        is_start = False\n",
    "\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration (todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train data\n",
    "train_click_log = pd.read_csv(f\"{DDIR}/train_preliminary/click_log.csv\")\n",
    "train_ad = pd.read_csv(f\"{DDIR}/train_preliminary/ad.csv\")\n",
    "# tag\n",
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "test_click_log = pd.read_csv(f\"{DDIR}/test/click_log.csv\")\n",
    "test_ad = pd.read_csv(f\"{DDIR}/test/ad.csv\")\n",
    "# pd.DataFrame(np.sort(test_click_log[UID].unique()), columns=[UID]).to_csv(f\"{DDIR}/test/user.csv\", index=False)\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_click_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_click_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ad[\"product_id\"] = train_ad[\"product_id\"].replace(\"\\\\N\", -1).astype(int)\n",
    "train_ad[\"industry\"] = train_ad[\"industry\"].replace(\"\\\\N\", -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ad[\"product_id\"] = test_ad[\"product_id\"].replace(\"\\\\N\", -1).astype(int)\n",
    "test_ad[\"industry\"] = test_ad[\"industry\"].replace(\"\\\\N\", -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creative id in train (creative id is unique in train_ad)\n",
    "len(train_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creative id in test (creative id is unique in test_ad)\n",
    "len(test_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check whether the same creative_id in train and test have same ad info\n",
    "insect1d = np.intersect1d(train_click_log.creative_id.unique(), test_click_log.creative_id.unique())\n",
    "print(\"Same creative id: \", insect1d.shape)\n",
    "print(\"Diff number: \", np.sum(train_ad[train_ad.creative_id.isin(insect1d)].values != test_ad[test_ad.creative_id.isin(insect1d)].values))\n",
    "# checked: they all have same ad info (result is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether click and ad have diff creative_id\n",
    "print(\"Diff list: \", np.setdiff1d(train_click_log.creative_id.unique(), train_ad.creative_id))\n",
    "print(\"Diff list: \", np.setdiff1d(train_ad.creative_id, train_click_log.creative_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether click and ad have diff creative_id\n",
    "print(\"Diff list: \", np.setdiff1d(test_click_log.creative_id.unique(), test_ad.creative_id))\n",
    "print(\"Diff list: \", np.setdiff1d(test_ad.creative_id, test_click_log.creative_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click time\n",
    "sns.lineplot(x=train_click_log.time.value_counts().index, y=train_click_log.time.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=test_click_log.time.value_counts().index, y=test_click_log.time.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_grouped = data.groupby(data.index)\n",
    "# results = Parallel(n_jobs=8)(delayed(key_func)(group) for name, group in data_grouped)\n",
    "# data = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ID sequence (Only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort by time, for time series\n",
    "# train_click_log.sort_values(by=\"time\", inplace=True)\n",
    "# test_click_log.sort_values(by=\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tol_train = pd.merge(train_click_log, train_ad, how=\"left\", on=\"creative_id\")\n",
    "# tol_test = pd.merge(test_click_log, test_ad, how=\"left\", on=\"creative_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_click_log, test_click_log\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeit\n",
    "# def gen_id_series(data, dtyp=\"train\"):\n",
    "#     for col in ID_SET:\n",
    "#         tmp = data.groupby([UID], sort=False)[[col]].agg(lambda x: [f\"word_{y}\" for y in x])\n",
    "#         tmp.columns = bch_rencol(tmp.columns)\n",
    "#         tmp.to_pickle(f\"{UDDIR}/imd/{dtyp}_{col}_seq.pkl\")\n",
    "#         tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_id_series(tol_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_id_series(tol_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqidx_dic = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ID_SET:\n",
    "#     train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "#     test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "#     tol_seq = pd.concat([train_seq, test_seq])\n",
    "#     seqidx_dic[col] = tol_seq.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{UDDIR}/imd/seqidx_dic.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(seqidx_dic), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF&Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_tfidf(col, nr_max=1):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    tol_seq[col] = tol_seq[col].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    tfidf_enc = TfidfVectorizer(ngram_range=(1, nr_max), min_df=30)\n",
    "    tfidf_vec = tfidf_enc.fit_transform(tol_seq[col].values)\n",
    "    log(f\"TF-IDF shape: {tfidf_vec.shape}\")\n",
    "\n",
    "    # save sparse matrix\n",
    "    scipy.sparse.save_npz(f\"{UDDIR}/imd/sparse_tfidf_{col}.npz\", tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_count(col, nr_max=1):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    tol_seq[col] = tol_seq[col].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    count_enc = CountVectorizer(ngram_range=(1, nr_max), min_df=30)\n",
    "    count_vec = count_enc.fit_transform(tol_seq[col].values)\n",
    "    log(f\"Count shape: {count_vec.shape}\")\n",
    "\n",
    "    # save sparse matrix\n",
    "    scipy.sparse.save_npz(f\"{UDDIR}/imd/sparse_count_{col}.npz\", count_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET[-2:]:\n",
    "    gen_tfidf(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET[-2:]:\n",
    "    gen_count(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_svd(col, index, prefix=\"tfidf\", n_cpt=64):\n",
    "    tfidf_vec = scipy.sparse.load_npz(f\"{UDDIR}/imd/sparse_{prefix}_{col}.npz\")\n",
    "    if tfidf_vec.shape[1] > n_cpt:\n",
    "        svd_enc = TruncatedSVD(n_components=n_cpt, n_iter=20, random_state=2020)\n",
    "        mode_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    else:\n",
    "        n_cpt = tfidf_vec.shape[1]\n",
    "        mode_svd = tfidf_vec.todense()\n",
    "    mode_svd = pd.DataFrame(mode_svd)\n",
    "    mode_svd.columns = [f\"{prefix}_svd_{col}_{i}\" for i in range(n_cpt)]\n",
    "    mode_svd.index = index\n",
    "    # save svd pkl\n",
    "    mode_svd.to_pickle(f\"{UFEDIR}/{prefix}_svd_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{UDDIR}/imd/seqidx_dic.pkl\", \"rb\") as f:\n",
    "    seqidx_dic = pickle.loads(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET:\n",
    "    gen_svd(col, seqidx_dic[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = None\n",
    "for col in tqdm(ID_SET):\n",
    "    print(\"current filename: \", f\"{UDDIR}/imd/sparse_tfidf_{col}.npz\", f\"{UDDIR}/imd/sparse_count_{col}.npz\")\n",
    "    tfidf_vec = scipy.sparse.load_npz(f\"{UDDIR}/imd/sparse_tfidf_{col}.npz\")\n",
    "    count_vec = scipy.sparse.load_npz(f\"{UDDIR}/imd/sparse_count_{col}.npz\")\n",
    "    sparse_matrix = scipy.sparse.csr_matrix(scipy.sparse.hstack([sparse_matrix, tfidf_vec]))\n",
    "    sparse_matrix = scipy.sparse.csr_matrix(scipy.sparse.hstack([sparse_matrix, count_vec]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{UDDIR}/imd/seqidx_dic.pkl\", \"rb\") as f:\n",
    "    seqidx_dic = pickle.loads(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_ids = list(combinations(ID_SET, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_eq = True\n",
    "for cp in comb_ids:\n",
    "    if (seqidx_dic[cp[0]] != seqidx_dic[cp[1]]).sum() != 0:\n",
    "        print(cp)\n",
    "        is_eq = False\n",
    "        break\n",
    "assert is_eq, \"Must True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sparse_matrix[seqidx_dic[\"creative_id\"].isin(train_user[UID])]\n",
    "X_test = sparse_matrix[~seqidx_dic[\"creative_id\"].isin(train_user[UID])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder index\n",
    "re_train_user = pd.DataFrame(index=seqidx_dic[\"creative_id\"][seqidx_dic[\"creative_id\"].isin(train_user[UID])])\n",
    "re_train_user = pd.merge(re_train_user, train_user, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder index\n",
    "re_test_user = pd.DataFrame(index=seqidx_dic[\"creative_id\"][~seqidx_dic[\"creative_id\"].isin(train_user[UID])])\n",
    "re_test_user = pd.merge(re_test_user, test_user, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(X_train.shape)\n",
    "log(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_clf = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_seed = 2020\n",
    "num_classes = 10\n",
    "n_splits = 5\n",
    "y = re_train_user[\"age\"]\n",
    "model_zoo = [SGDClassifier(n_jobs=-1,verbose=1), SGDClassifier(loss='log',n_jobs=-1,verbose=1),\n",
    "             SGDClassifier(loss='modified_huber',n_jobs=-1,verbose=1),\n",
    "             PassiveAggressiveClassifier(n_jobs=-1,verbose=1),LogisticRegression(C=10),\n",
    "             RidgeClassifier(solver='lsqr',fit_intercept=False),LinearSVC(verbose=1,max_iter=500),\n",
    "             BernoulliNB(),MultinomialNB()]\n",
    "oof = []\n",
    "count = 0\n",
    "columns = ['SGD_HINGE', 'SGD_LOG','SGD_HUBER','PAC','LR','RIDGE','LSVC','BNB','MNB']\n",
    "\n",
    "for model in model_zoo:\n",
    "    cv_pred_stack = np.zeros((X_train.shape[0],num_classes))\n",
    "    test_pred_stack = np.zeros((X_test.shape[0],num_classes))\n",
    "    skf = KFold(n_splits=n_splits,random_state=random_seed)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    if os.path.exists(f\"{UDDIR}/imd/tfidf_count_emb_age_{columns[count]}.pkl\"):\n",
    "        tmp = pd.read_pickle(f\"{UDDIR}/imd/tfidf_count_emb_age_{columns[count]}.pkl\")\n",
    "    else:\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "            print(index,model)\n",
    "            \n",
    "            train_x = test_x = train_y = test_y = None\n",
    "            gc.collect()\n",
    "            \n",
    "            train_x, test_x, train_y, test_y = X_train[train_index], X_train[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "            model.fit(train_x,train_y)\n",
    "            try:\n",
    "                y_val = model._predict_proba_lr(test_x)\n",
    "            except:\n",
    "                y_val = model.predict_proba(test_x)\n",
    "            cv_pred_stack[test_index] = y_val\n",
    "            print(y_val.shape)\n",
    "            try:\n",
    "                test_pred_stack += model._predict_proba_lr(X_test) / n_splits\n",
    "            except:\n",
    "                test_pred_stack += model.predict_proba(X_test) / n_splits\n",
    "            \n",
    "                \n",
    "        print(model,'score:',accuracy_score(y,np.argmax(cv_pred_stack,axis=1)))\n",
    "\n",
    "        a = pd.DataFrame(cv_pred_stack).add_prefix(columns[count]+\"_\")\n",
    "        a[UID] = re_train_user[UID].values\n",
    "        b = pd.DataFrame(test_pred_stack).add_prefix(columns[count]+\"_\")\n",
    "        b[UID] = re_test_user[UID].values\n",
    "        tmp = a.append(b)\n",
    "        tmp.to_pickle(f\"{UDDIR}/imd/tfidf_count_emb_age_{columns[count]}.pkl\")\n",
    "        \n",
    "    count += 1\n",
    "    oof.append(tmp)\n",
    "    \n",
    "\n",
    "df_agg = pd.DataFrame()\n",
    "for i in tqdm(oof):\n",
    "    df_agg[i.columns] = i\n",
    "df_agg.to_pickle(f\"{UFEDIR}/tfidf_count_emb_age_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_clf.check(\"clf embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_seed = 2020\n",
    "num_classes = 2\n",
    "n_splits = 5\n",
    "y = re_train_user[\"gender\"]\n",
    "model_zoo = [SGDClassifier(n_jobs=-1,verbose=1), SGDClassifier(loss='log',n_jobs=-1,verbose=1),\n",
    "             SGDClassifier(loss='modified_huber',n_jobs=-1,verbose=1),\n",
    "             PassiveAggressiveClassifier(n_jobs=-1,verbose=1),LogisticRegression(C=10),\n",
    "             RidgeClassifier(solver='lsqr',fit_intercept=False),LinearSVC(verbose=1,max_iter=500),\n",
    "             BernoulliNB(),MultinomialNB()]\n",
    "oof = []\n",
    "count = 0\n",
    "columns = ['SGD_HINGE', 'SGD_LOG','SGD_HUBER','PAC','LR','RIDGE','LSVC','BNB','MNB']\n",
    "\n",
    "for model in model_zoo:\n",
    "    cv_pred_stack = np.zeros((X_train.shape[0],num_classes))\n",
    "    test_pred_stack = np.zeros((X_test.shape[0],num_classes))\n",
    "    skf = KFold(n_splits=n_splits,random_state=random_seed)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    if os.path.exists(f\"{UDDIR}/imd/tfidf_count_emb_gender_{columns[count]}.pkl\"):\n",
    "        tmp = pd.read_pickle(f\"{UDDIR}/imd/tfidf_count_emb_gender_{columns[count]}.pkl\")\n",
    "    else:\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "            print(index,model)\n",
    "\n",
    "            train_x = test_x = train_y = test_y = None\n",
    "            gc.collect()\n",
    "\n",
    "            train_x, test_x, train_y, test_y = X_train[train_index], X_train[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "            model.fit(train_x,train_y)\n",
    "            try:\n",
    "                y_val = model._predict_proba_lr(test_x)\n",
    "            except:\n",
    "                y_val = model.predict_proba(test_x)\n",
    "            cv_pred_stack[test_index] = y_val\n",
    "            print(y_val.shape)\n",
    "            try:\n",
    "                test_pred_stack += model._predict_proba_lr(X_test) / n_splits\n",
    "            except:\n",
    "                test_pred_stack += model.predict_proba(X_test) / n_splits\n",
    "        print(model,'score:',accuracy_score(y,np.argmax(cv_pred_stack,axis=1)))\n",
    "\n",
    "        a = pd.DataFrame(cv_pred_stack).add_prefix(columns[count]+\"_\")\n",
    "        a[UID] = re_train_user[UID].values\n",
    "        b = pd.DataFrame(test_pred_stack).add_prefix(columns[count]+\"_\")\n",
    "        b[UID] = re_test_user[UID].values\n",
    "        tmp = a.append(b)\n",
    "        tmp.to_pickle(f\"{UDDIR}/imd/tfidf_count_emb_gender_{columns[count]}.pkl\")\n",
    "        \n",
    "    count += 1\n",
    "    oof.append(tmp)\n",
    "    \n",
    "\n",
    "df_agg = pd.DataFrame()\n",
    "for i in tqdm(oof):\n",
    "    df_agg[i.columns] = i\n",
    "df_agg.to_pickle(f\"{UFEDIR}/tfidf_count_emb_gender_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_clf.check(\"clf embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_w2v(col, vesize=300, win=20):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    if os.path.exists(f\"{UMDIR}/vectors/w2v_{col}.model\"):\n",
    "        model = gensim.models.Word2Vec.load(f\"{UMDIR}/vectors/w2v_{col}.model\")\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec(sentences=tol_seq[col], size=vesize, window=win, workers=32, sg=0, iter=10)\n",
    "        model.save(f\"{UMDIR}/vectors/w2v_{col}.model\")\n",
    "    \n",
    "    w2v_list = list()\n",
    "    \n",
    "    for it in tqdm(tol_seq[col]):\n",
    "        tmp = np.zeros(vesize)\n",
    "        cnt = 0\n",
    "        for wd in it:\n",
    "            cnt += 1\n",
    "            if wd in model:\n",
    "                tmp += model[wd]\n",
    "        w2v_list.append(list(tmp/cnt))\n",
    "    \n",
    "    w2v_avg = pd.DataFrame(w2v_list)\n",
    "    w2v_avg.index = tol_seq.index\n",
    "    w2v_avg.columns = [f\"w2v_avg_{col}_{i}\" for i in range(vesize)]\n",
    "    w2v_avg = reduce_mem_usage(w2v_avg)\n",
    "    w2v_avg.to_pickle(f\"{UFEDIR}/w2v_avg_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET:\n",
    "    gen_w2v(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_d2v(col, vesize=300, win=20):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    \n",
    "    if os.path.exists(f\"{UMDIR}/vectors/d2v_{col}.model\"):\n",
    "        model = gensim.models.Doc2Vec.load(f\"{UMDIR}/vectors/d2v_{col}.model\")\n",
    "    else:\n",
    "        docs = [gensim.models.doc2vec.TaggedDocument(words=i[1],tags=[str(i[0])]) for i in tol_seq[col].reset_index().values]\n",
    "        model = gensim.models.Doc2Vec(documents=docs, size=vesize, window=win, workers=32, iter=10)\n",
    "        model.save(f\"{UMDIR}/vectors/d2v_{col}.model\")\n",
    "    \n",
    "    d2v_list = list()\n",
    "    for it, cps in tqdm(tol_seq[col].reset_index().values):\n",
    "#         if it in model.docvecs:\n",
    "        d2v_list.append(model.docvecs[str(it)])\n",
    "#         else:\n",
    "#             d2v_list.append(model.infer_vector(cps))\n",
    "\n",
    "    d2v_avg = pd.DataFrame(d2v_list)\n",
    "    d2v_avg.index = tol_seq.index\n",
    "    d2v_avg.columns = [f\"d2v_avg_{col}_{i}\" for i in range(vesize)]\n",
    "    d2v_avg = reduce_mem_usage(d2v_avg)\n",
    "    d2v_avg.to_pickle(f\"{UFEDIR}/d2v_avg_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in ID_SET:\n",
    "    gen_d2v(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_glove(col, vesize=300, win=20):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    \n",
    "    if os.path.exists(f\"{UMDIR}/vectors/glove_{col}.model\"):\n",
    "        glove = Glove.load(f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "    else:\n",
    "        matrix =  Corpus()\n",
    "        matrix.fit(tol_seq[col].values)\n",
    "        glove = Glove(no_components=vesize, learning_rate=0.05)\n",
    "        glove.fit(matrix.matrix,epochs=10,no_threads=30,verbose=1)\n",
    "        glove.add_dictionary(matrix.dictionary)\n",
    "        glove.save(f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "        \n",
    "    ans = []\n",
    "    for i in tqdm(tol_seq[col].values):\n",
    "        line = []\n",
    "        for j in i:\n",
    "            line.append(glove.word_vectors[glove.dictionary[j]])\n",
    "        ans.append(np.mean(line,axis=0))\n",
    "    \n",
    "    glove_avg = pd.DataFrame(ans)\n",
    "    glove_avg.index = tol_seq.index\n",
    "    glove_avg.columns = [f\"glove_avg_{col}_{i}\" for i in range(vesize)]\n",
    "    glove_avg = reduce_mem_usage(glove_avg)\n",
    "    glove_avg.to_pickle(f\"{UFEDIR}/glove_avg_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ID_SET:\n",
    "    gen_glove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_train = pd.merge(train_click_log, train_ad, how=\"left\", on=\"creative_id\")\n",
    "tol_test = pd.merge(test_click_log, test_ad, how=\"left\", on=\"creative_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_click_log, train_ad\n",
    "del test_click_log, test_ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category map to Label (use KFold TODO)\n",
    "\n",
    "__a little overfitting age__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_catemlb(train_data, test_data, tag, col):\n",
    "    train_use = pd.merge(train_data[[UID, col]], tag, how=\"left\", on=UID)\n",
    "    test_use = test_data[[UID, col]]\n",
    "    \n",
    "    nfolds = 5\n",
    "    kfold = KFold(n_splits=nfolds)\n",
    "    \n",
    "    kf_map_df = pd.DataFrame()\n",
    "\n",
    "    re_train_use = pd.DataFrame()\n",
    "    re_test_use = pd.DataFrame()\n",
    "\n",
    "    for tr_idx, val_idx in kfold.split(train_use):\n",
    "        tr_ucid, val_ucid = train_use.iloc[tr_idx], train_use.iloc[val_idx]\n",
    "        kf_map = tr_ucid.groupby([col])[[\"age\", \"gender\"]].agg([\"mean\"])\n",
    "        kf_map.columns = bch_rencol(kf_map.columns, prefix=f\"{col}_\")\n",
    "        # only use intersect between train and test\n",
    "        kf_map.drop(np.setdiff1d(kf_map.index.unique(), test_use[col].unique()), inplace=True)\n",
    "        # get kf_cum\n",
    "        if kf_map_df.empty:\n",
    "            kf_map_df = kf_map\n",
    "        else:\n",
    "            kf_map_df = (kf_map_df + kf_map).fillna(0)\n",
    "        \n",
    "        val_ucid = pd.merge(val_ucid, kf_map, how=\"left\", on=col)\n",
    "        re_train_use = pd.concat([re_train_use, val_ucid])\n",
    "    \n",
    "    kf_map_df = kf_map_df/nfolds\n",
    "    re_test_use = pd.merge(test_use, kf_map_df, how=\"left\", on=col)\n",
    "\n",
    "    assert len(train_use) == len(re_train_use)\n",
    "    assert len(test_use) == len(re_test_use)\n",
    "\n",
    "    train_use = None\n",
    "    test_use = None\n",
    "\n",
    "    tmp = re_train_use.groupby([UID], sort=False)[f\"{col}_age_mean\", f\"{col}_gender_mean\"].agg([\"sum\", \"max\", \"mean\", \"min\", \"std\"])\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    tmp.to_pickle(f\"{UFEDIR}/train_stats_catemlb_{col}.pkl\")\n",
    "    tmp = None\n",
    "\n",
    "    tmp = re_test_use.groupby([UID], sort=False)[f\"{col}_age_mean\", f\"{col}_gender_mean\"].agg([\"sum\", \"max\", \"mean\", \"min\", \"std\"])\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    tmp.to_pickle(f\"{UFEDIR}/test_stats_catemlb_{col}.pkl\")\n",
    "    tmp = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\",]:  # \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    gen_catemlb(tol_train, tol_test, train_user, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One key(O1)-Time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "tmp = tol_train.groupby([UID], sort=False).agg(\n",
    "    {\n",
    "        UID: [\"count\"], \n",
    "        \"click_times\": OP_SET1,\n",
    "        \"time\": OP_SET1,\n",
    "        \"creative_id\": [\"nunique\"],\n",
    "        \"ad_id\": [\"nunique\"],\n",
    "        \"product_id\": [\"nunique\"],\n",
    "        \"product_category\": [\"nunique\"],\n",
    "        \"advertiser_id\": [\"nunique\"],\n",
    "        \"industry\": [\"nunique\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "tmp.columns = bch_rencol(tmp.columns)\n",
    "\n",
    "tmp.to_pickle(f\"{UFEDIR}/train_stats_o1.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "tmp = tol_test.groupby([UID], sort=False).agg(\n",
    "    {\n",
    "        UID: [\"count\"], \n",
    "        \"click_times\": OP_SET1,\n",
    "        \"time\": OP_SET1,\n",
    "        \"creative_id\": [\"nunique\"],\n",
    "        \"ad_id\": [\"nunique\"],\n",
    "        \"product_id\": [\"nunique\"],\n",
    "        \"product_category\": [\"nunique\"],\n",
    "        \"advertiser_id\": [\"nunique\"],\n",
    "        \"industry\": [\"nunique\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "tmp.columns = bch_rencol(tmp.columns)\n",
    "\n",
    "tmp.to_pickle(f\"{UFEDIR}/test_stats_o1.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comb Key(O2)-Time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for i in tqdm(range(1, 7)):\n",
    "    for cl in combinations(id_list, i):\n",
    "        key_set = [UID] + list(cl)\n",
    "        tmp = tol_train.groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "        tmp.columns = bch_rencol(tmp.columns, prefix=f\"{'C'.join(key_set[1:-1])}_\")\n",
    "        tmp.to_pickle(f\"{UFEDIR}/train_stats_o2_{'C'.join(key_set)}.pkl\")\n",
    "        tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for i in tqdm(range(1, 7)):\n",
    "    for cl in combinations(id_list, i):\n",
    "        key_set = [UID] + list(cl)\n",
    "        tmp = tol_test.groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "        tmp.columns = bch_rencol(tmp.columns, prefix=f\"{'C'.join(key_set[1:-1])}_\")\n",
    "        tmp.to_pickle(f\"{UFEDIR}/test_stats_o2_{'C'.join(key_set)}.pkl\")\n",
    "        tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Windows-Time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_set = [3, 7, 10, 30]\n",
    "\n",
    "for bins in bins_set:\n",
    "    tol_bins = pd.cut(pd.concat([tol_train[\"time\"], tol_test[\"time\"]]), bins, labels=range(bins))\n",
    "    tol_train[f\"bins{bins}\"] = tol_bins[:len(tol_train)]\n",
    "    tol_test[f\"bins{bins}\"] = tol_bins[len(tol_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Init Bin Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Key(O1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train\n",
    "for bins in bins_set:\n",
    "    for cb in tqdm(range(bins)):\n",
    "        tmp = tol_train[tol_train[f\"bins{bins}\"] == cb].groupby([UID], sort=False).agg(\n",
    "            {\n",
    "                UID: [\"count\"], \n",
    "                \"click_times\": OP_SET1,\n",
    "                \"time\": OP_SET1,\n",
    "                \"creative_id\": [\"nunique\"],\n",
    "                \"ad_id\": [\"nunique\"],\n",
    "                \"product_id\": [\"nunique\"],\n",
    "                \"product_category\": [\"nunique\"],\n",
    "                \"advertiser_id\": [\"nunique\"],\n",
    "                \"industry\": [\"nunique\"],\n",
    "            }\n",
    "        )\n",
    "        tmp.columns = bch_rencol(tmp.columns, prefix=f\"bins{bins}_{cb}_\")\n",
    "        tmp.to_pickle(f\"{UFEDIR}/train_stats_o1_bins{bins}_{cb}.pkl\")\n",
    "        tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o1 for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "for bins in bins_set:\n",
    "    for cb in tqdm(range(bins)):\n",
    "        tmp = tol_test[tol_test[f\"bins{bins}\"] == cb].groupby([UID], sort=False).agg(\n",
    "            {\n",
    "                UID: [\"count\"], \n",
    "                \"click_times\": OP_SET1,\n",
    "                \"time\": OP_SET1,\n",
    "                \"creative_id\": [\"nunique\"],\n",
    "                \"ad_id\": [\"nunique\"],\n",
    "                \"product_id\": [\"nunique\"],\n",
    "                \"product_category\": [\"nunique\"],\n",
    "                \"advertiser_id\": [\"nunique\"],\n",
    "                \"industry\": [\"nunique\"],\n",
    "            }\n",
    "        )\n",
    "        tmp.columns = bch_rencol(tmp.columns, prefix=f\"bins{bins}_{cb}_\")\n",
    "        tmp.to_pickle(f\"{UFEDIR}/test_stats_o1_bins{bins}_{cb}.pkl\")\n",
    "        tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o1 for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comb Key(O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for bins in bins_set:\n",
    "    for cb in tqdm(range(bins)):\n",
    "        for i in tqdm(range(1, 7)):\n",
    "            for cl in combinations(id_list, i):\n",
    "                key_set = [UID] + list(cl)\n",
    "                tmp = tol_train[tol_train[f\"bins{bins}\"] == cb].groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "                tmp.columns = bch_rencol(tmp.columns, prefix=f\"bins{bins}_{cb}_{'C'.join(key_set[1:-1])}_\")\n",
    "                tmp.to_pickle(f\"{UFEDIR}/train_stats_o2_bins{bins}_{cb}_{'C'.join(key_set)}.pkl\")\n",
    "                tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o2 bin for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for bins in bins_set:\n",
    "    for cb in tqdm(range(bins)):\n",
    "        for i in tqdm(range(1, 7)):\n",
    "            for cl in combinations(id_list, i):\n",
    "                key_set = [UID] + list(cl)\n",
    "                tmp = tol_test[tol_test[f\"bins{bins}\"] == cb].groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "                tmp.columns = bch_rencol(tmp.columns, prefix=f\"bins{bins}_{cb}_{'C'.join(key_set[1:-1])}_\")\n",
    "                tmp.to_pickle(f\"{UFEDIR}/test_stats_o2_bins{bins}_{cb}_{'C'.join(key_set)}.pkl\")\n",
    "                tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o2 bin for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_set = [1, 2, 3, 7, 14, 21, 30, 60, 90]\n",
    "\n",
    "tol_train[\"max_time\"] = tol_train.groupby([UID], sort=False)[\"time\"].transform(\"max\")\n",
    "tol_test[\"max_time\"] = tol_test.groupby([UID], sort=False)[\"time\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Get Max time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Key(O1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train\n",
    "for win in slide_set:\n",
    "    tmp = tol_train[tol_train[\"time\"] >= tol_train[\"max_time\"] - win].groupby([UID], sort=False).agg(\n",
    "        {\n",
    "            UID: [\"count\"], \n",
    "            \"click_times\": OP_SET1,\n",
    "            \"time\": OP_SET1,\n",
    "            \"creative_id\": [\"nunique\"],\n",
    "            \"ad_id\": [\"nunique\"],\n",
    "            \"product_id\": [\"nunique\"],\n",
    "            \"product_category\": [\"nunique\"],\n",
    "            \"advertiser_id\": [\"nunique\"],\n",
    "            \"industry\": [\"nunique\"],\n",
    "        }\n",
    "    )\n",
    "    tmp.columns = bch_rencol(tmp.columns, prefix=f\"slide{win}_\")\n",
    "    tmp.to_pickle(f\"{UFEDIR}/train_stats_o1_slide{win}.pkl\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o1 slide for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "for win in slide_set:\n",
    "    tmp = tol_test[tol_test[\"time\"] >= tol_test[\"max_time\"] - win].groupby([UID], sort=False).agg(\n",
    "        {\n",
    "            UID: [\"count\"], \n",
    "            \"click_times\": OP_SET1,\n",
    "            \"time\": OP_SET1,\n",
    "            \"creative_id\": [\"nunique\"],\n",
    "            \"ad_id\": [\"nunique\"],\n",
    "            \"product_id\": [\"nunique\"],\n",
    "            \"product_category\": [\"nunique\"],\n",
    "            \"advertiser_id\": [\"nunique\"],\n",
    "            \"industry\": [\"nunique\"],\n",
    "        }\n",
    "    )\n",
    "    tmp.columns = bch_rencol(tmp.columns, prefix=f\"slide{win}_\")\n",
    "    tmp.to_pickle(f\"{UFEDIR}/test_stats_o1_slide{win}.pkl\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o1 slide for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comb Key(O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for win in slide_set:\n",
    "    for i in tqdm(range(1, 7)):\n",
    "        for cl in combinations(id_list, i):\n",
    "            key_set = [UID] + list(cl)\n",
    "            tmp = tol_train[tol_train[\"time\"] >= tol_train[\"max_time\"] - win].groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "            tmp.columns = bch_rencol(tmp.columns, prefix=f\"slide{win}_{'C'.join(key_set[1:-1])}_\")\n",
    "            tmp.to_pickle(f\"{UFEDIR}/train_stats_o2_slide{win}_{'C'.join(key_set)}.pkl\")\n",
    "            tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o2 slide for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for win in slide_set:\n",
    "    for i in tqdm(range(1, 7)):\n",
    "        for cl in combinations(id_list, i):\n",
    "            key_set = [UID] + list(cl)\n",
    "            tmp = tol_test[tol_test[\"time\"] >= tol_test[\"max_time\"] - win].groupby(key_set, sort=False)[key_set[-1:]].agg([\"count\"]).groupby([UID]).agg(OP_SET2)\n",
    "            tmp.columns = bch_rencol(tmp.columns, prefix=f\"slide{win}_{'C'.join(key_set[1:-1])}_\")\n",
    "            tmp.to_pickle(f\"{UFEDIR}/test_stats_o2_slide{win}_{'C'.join(key_set)}.pkl\")\n",
    "            tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_tw.check(\"Generate o2 slide for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TOP Category) One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.concat([tol_train, tol_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_dic = {\n",
    "    \"creative_id\": 300,\n",
    "    \"ad_id\": 300,\n",
    "    \"product_id\": 100,\n",
    "    \"advertiser_id\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid in dum_dic:\n",
    "    use_id = tol_data[cid].value_counts().head(dum_dic[cid]).index.values\n",
    "    tol_data[f\"{cid}_dummy\"] = tol_data[cid]\n",
    "    tol_data[f\"{cid}_dummy\"][~tol_data[f\"{cid}_dummy\"].isin(use_id)] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid in  [\"time\", \"click_times\", \"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    ocid = cid\n",
    "    if cid in dum_dic:\n",
    "        cid = f\"{cid}_dummy\"\n",
    "    tmp = tol_data[[UID, cid]].groupby([UID, cid], sort=False)[[cid]].agg([\"count\"]).unstack().fillna(0)\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    reduce_mem_usage(tmp).to_pickle(f\"{UFEDIR}/onehot_catecnt_{ocid}.pkl\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Train \n",
    "\n",
    "__weak learner prediction as features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================Once=================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_test.index = -tol_test.index -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.concat([tol_train, tol_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tol_train, tol_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data[f\"{UID}_count\"] = tol_data[[UID]].groupby([UID])[UID].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    tmp = tol_data[[UID, col]]\n",
    "    tol_data[f\"{UID}_{col}_std\"] = tmp.groupby([UID])[col].transform(\"std\")\n",
    "    tol_data[f\"{col}_{UID}_count\"] = tmp.groupby([col])[UID].transform(\"count\")\n",
    "    tol_data[f\"{col}_{UID}_std\"] = tmp.groupby([col])[UID].transform(\"std\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"time\", \"click_times\"]:\n",
    "    tmp = tol_data[[UID, col]]\n",
    "    tol_data[f\"{UID}_{col}_std\"] = tmp.groupby([UID])[col].transform(\"std\")\n",
    "    tol_data[f\"{UID}_{col}_mean\"] = tmp.groupby([UID])[col].transform(\"mean\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = reduce_mem_usage(tol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data.to_pickle(f\"{UDDIR}/imd/tol_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============End===============#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if have tol_data.pkl\n",
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")\n",
    "\n",
    "tol_data = pd.read_pickle(f\"{UDDIR}/imd/tol_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred))** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.merge(tol_data, train_user[[UID, \"age\"]], how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df_hist_train df_new_train df_hist_new_train to train 3 models\n",
    "train_df = tol_data[tol_data[\"age\"].notnull()]\n",
    "test_df = tol_data[tol_data[\"age\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [UID, \"age\"]\n",
    "cat_features = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in tol_data.columns if f not in drop_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits= 3\n",
    "folds = GroupKFold(n_splits=n_splits)\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "print ('feats:' + str(len(feats)))\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[\"age\"],groups=train_df[UID])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df[\"age\"].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[\"age\"].iloc[valid_idx] \n",
    "    \n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "               \"objective\" : \"regression\", \n",
    "               \"boosting\" : \"gbdt\", \n",
    "               \"metric\" : \"rmse\",  \n",
    "               \"max_depth\": 7, \n",
    "               \"num_leaves\" : 31, \n",
    "               \"max_bin\" : 255, \n",
    "               \"learning_rate\" : 0.1, \n",
    "               \"subsample\" : 0.8,\n",
    "               \"colsample_bytree\" : 0.8, \n",
    "               \"verbosity\": -1,\n",
    "               \"num_threads\" : -1,\n",
    "    }\n",
    "    \n",
    "\n",
    "    if n_fold >= 0:\n",
    "        evals_result = {}\n",
    "        dtrain = lgb.Dataset(\n",
    "            train_x, label=train_y,categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(\n",
    "            valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features)\n",
    "        bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=30000,\n",
    "            valid_sets=[dval], early_stopping_rounds=100, verbose_eval=20,)#feval = evalerror\n",
    "        \n",
    "        new_list = sorted(zip(feats, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:]\n",
    "        for item in new_list:\n",
    "            print (item) \n",
    "\n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "\n",
    "        sub_preds += bst.predict(test_df[feats], num_iteration=bst.best_iteration) / folds.n_splits # test_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = rmse(train_df[\"age\"],  oof_preds)\n",
    "print('Full OOF RMSE %.6f' % cv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df[[UID]]\n",
    "b = test_df[[UID]]\n",
    "\n",
    "a[\"age_pred\"] = oof_preds\n",
    "b[\"age_pred\"] = sub_preds\n",
    "\n",
    "a1 = a.groupby([UID])[\"age_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])\n",
    "b1 = b.groupby([UID])[\"age_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.append(b1).add_prefix(\"age_gkf_agg_pred_\").reset_index().sort_values(by=[UID]).reset_index(drop=True).to_pickle(f\"{UFEDIR}/meta_age_group_regeress.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.merge(tol_data, train_user[[UID, \"gender\"]], how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df_hist_train df_new_train df_hist_new_train to train 3 models\n",
    "train_df = tol_data[tol_data[\"gender\"].notnull()]\n",
    "test_df = tol_data[tol_data[\"gender\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [UID, \"gender\"]\n",
    "cat_features = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in tol_data.columns if f not in drop_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits= 3\n",
    "folds = GroupKFold(n_splits=n_splits)\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "print ('feats:' + str(len(feats)))\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[\"gender\"],groups=train_df[UID])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df[\"gender\"].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[\"gender\"].iloc[valid_idx] \n",
    "    \n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "               \"objective\" : \"regression\", \n",
    "               \"boosting\" : \"gbdt\", \n",
    "               \"metric\" : \"rmse\",  \n",
    "               \"max_depth\": 7, \n",
    "               \"num_leaves\" : 31, \n",
    "               \"max_bin\" : 255, \n",
    "               \"learning_rate\" : 0.1, \n",
    "               \"subsample\" : 0.8,\n",
    "               \"colsample_bytree\" : 0.8, \n",
    "               \"verbosity\": -1,\n",
    "               \"num_threads\" : -1,\n",
    "    }\n",
    "    \n",
    "\n",
    "    if n_fold >= 0:\n",
    "        evals_result = {}\n",
    "        dtrain = lgb.Dataset(\n",
    "            train_x, label=train_y,categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(\n",
    "            valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features)\n",
    "        bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=30000,\n",
    "            valid_sets=[dval], early_stopping_rounds=100, verbose_eval=20,)#feval = evalerror\n",
    "        \n",
    "        new_list = sorted(zip(feats, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:]\n",
    "        for item in new_list:\n",
    "            print (item) \n",
    "\n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "\n",
    "        sub_preds += bst.predict(test_df[feats], num_iteration=bst.best_iteration) / folds.n_splits # test_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = rmse(train_df[\"gender\"],  oof_preds)\n",
    "print('Full OOF RMSE %.6f' % cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df[[UID]]\n",
    "b = test_df[[UID]]\n",
    "\n",
    "a[\"gender_pred\"] = oof_preds\n",
    "b[\"gender_pred\"] = sub_preds\n",
    "\n",
    "a1 = a.groupby([UID])[\"gender_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])\n",
    "b1 = b.groupby([UID])[\"gender_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.append(b1).add_prefix(\"gender_gkf_agg_pred_\").reset_index().sort_values(by=[UID]).reset_index(drop=True).to_pickle(f\"{UFEDIR}/meta_gender_group_regeress.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fname = sorted(os.listdir(UFEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reduce memory (only once)\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"w2v_\") or fname.startswith(\"tfidf_svd_\") or fname.startswith(\"meta_age_\") or fname.startswith(\"meta_gender_\") or (fname.find(\"catemlb\") != -1) or (fname.find(\"stats\") != -1):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         reduce_mem_usage(pd.read_pickle(f\"{UFEDIR}/{fname}\")).to_pickle(f\"{UFEDIR}/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()\n",
    "train_feat[UID] = train_user[UID]\n",
    "test_feat[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"w2v_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_w2v = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_w2v, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_w2v, how=\"left\", on=UID)\n",
    "        cur_w2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats o1\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_feat = pd.merge(train_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_feat = pd.merge(test_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats getidxmax\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_o2_getidxmax\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_feat = pd.merge(train_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_o2_getidxmax\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_feat = pd.merge(test_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats getidxmax\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"train_stats_catemlb\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         train_feat = pd.merge(train_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "#     elif fname.startswith(\"test_stats_catemlb\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         test_feat = pd.merge(test_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not use\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"tfidf_svd_\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         cur_tfidf_svd = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "#         train_feat = pd.merge(train_feat, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "#         test_feat = pd.merge(test_feat, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "#         cur_tfidf_svd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"meta_age_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        age_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, age_agg_pred, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, age_agg_pred, how=\"left\", on=UID)\n",
    "        age_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"meta_gender_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        gender_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, gender_agg_pred, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, gender_agg_pred, how=\"left\", on=UID)\n",
    "        gender_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"d2v_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_d2v = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_d2v, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_d2v, how=\"left\", on=UID)\n",
    "        cur_d2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"glove_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_glove = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_glove, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_glove, how=\"left\", on=UID)\n",
    "        cur_glove = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not use has same effect with w2v and doc2vec\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"onehot_catecnt_\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         cur_ohcc = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "#         train_feat = pd.merge(train_feat, cur_ohcc, how=\"left\", on=UID)\n",
    "#         test_feat = pd.merge(test_feat, cur_ohcc, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"tfidf_count_emb_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_tce = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_tce, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_tce, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure feat and user(target) have same order\n",
    "# if true --> sum == 0\n",
    "np.sum(train_feat[UID] != train_user[UID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure feat and user(target) have same order\n",
    "# if true --> sum == 0\n",
    "np.sum(test_feat[UID] != test_user[UID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure train and test features have same order\n",
    "# if true --> sum == 0\n",
    "np.sum(train_feat.columns != test_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.memory_usage().sum() / 1024**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat.memory_usage().sum() / 1024**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.to_pickle(f\"{UDDIR}/feat_ing/train_feat_tol_v05.pkl\")\n",
    "test_feat.to_pickle(f\"{UDDIR}/feat_ing/test_feat_tol_v05.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")\n",
    "train_feat = pd.read_pickle(f\"{UDDIR}/feat_ing/train_feat_tol_v05.pkl\")\n",
    "test_feat = pd.read_pickle(f\"{UDDIR}/feat_ing/test_feat_tol_v05.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat.drop([col for col in train_feat.columns if col.find(\"creative_id_gender_\") != -1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feat.drop([col for col in test_feat.columns if col.find(\"creative_id_gender_\") != -1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training&Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.set_index(UID, inplace=True)\n",
    "test_feat.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_suffix = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "\n",
    "    \"\"\"RAdam optimizer.\n",
    "\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay for each param.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n",
    "        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n",
    "        min_lr: float >= 0. Minimum learning rate after warmup.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n",
    "                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n",
    "        super(RAdam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.total_steps = K.variable(total_steps, name='total_steps')\n",
    "            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n",
    "            self.min_lr = K.variable(min_lr, name='min_lr')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.initial_weight_decay = weight_decay\n",
    "        self.initial_total_steps = total_steps\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.learning_rate\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        if self.initial_total_steps > 0:\n",
    "            warmup_steps = self.total_steps * self.warmup_proportion\n",
    "            decay_steps = self.total_steps - warmup_steps\n",
    "            lr = K.switch(\n",
    "                t <= warmup_steps,\n",
    "                lr * (t / warmup_steps),\n",
    "                lr * (1.0 - K.minimum(t, decay_steps) / decay_steps),\n",
    "            )\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        else:\n",
    "            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        beta_1_t = K.pow(self.beta_1, t)\n",
    "        beta_2_t = K.pow(self.beta_2, t)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            m_corr_t = m_t / (1.0 - beta_1_t)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t) + self.epsilon)\n",
    "\n",
    "            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                         (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                         sma_inf / sma_t)\n",
    "\n",
    "            p_t = K.switch(sma_t > 5, r_t * m_corr_t / v_corr_t, m_corr_t)\n",
    "\n",
    "            if self.initial_weight_decay > 0:\n",
    "                p_t += self.weight_decay * p\n",
    "\n",
    "            p_t = p - lr * p_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'lr': float(K.get_value(self.learning_rate)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'decay': float(K.get_value(self.decay)),\n",
    "            'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': float(K.get_value(self.total_steps)),\n",
    "            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n",
    "            'min_lr': float(K.get_value(self.min_lr)),\n",
    "        }\n",
    "        base_config = super(RAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(object):\n",
    "    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5, alpha=0.5):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.count = 0\n",
    "\n",
    "    def inject(self, model):\n",
    "        \"\"\"Inject the Lookahead algorithm for the given model.\n",
    "        The following code is modified from keras's _make_train_function method.\n",
    "        See: https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497\n",
    "        \"\"\"\n",
    "        if not hasattr(model, 'train_function'):\n",
    "            raise RuntimeError('You must compile your model before using it.')\n",
    "\n",
    "        model._check_trainable_weights_consistency()\n",
    "\n",
    "        if model.train_function is None:\n",
    "            inputs = (model._feed_inputs +\n",
    "                      model._feed_targets +\n",
    "                      model._feed_sample_weights)\n",
    "            if model._uses_dynamic_learning_phase():\n",
    "                inputs += [K.learning_phase()]\n",
    "            fast_params = model._collected_trainable_weights\n",
    "\n",
    "            with K.name_scope('training'):\n",
    "                with K.name_scope(model.optimizer.__class__.__name__):\n",
    "                    training_updates = model.optimizer.get_updates(\n",
    "                        params=fast_params,\n",
    "                        loss=model.total_loss)\n",
    "                    slow_params = [K.variable(p) for p in fast_params]\n",
    "                fast_updates = (model.updates +\n",
    "                                training_updates +\n",
    "                                model.metrics_updates)\n",
    "\n",
    "                slow_updates, copy_updates = [], []\n",
    "                for p, q in zip(fast_params, slow_params):\n",
    "                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n",
    "                    copy_updates.append(K.update(p, q))\n",
    "\n",
    "                # Gets loss and metrics. Updates weights at each call.\n",
    "                fast_train_function = K.function(\n",
    "                    inputs,\n",
    "                    [model.total_loss] + model.metrics_tensors,\n",
    "                    updates=fast_updates,\n",
    "                    name='fast_train_function',\n",
    "                    **model._function_kwargs)\n",
    "\n",
    "                def F(inputs):\n",
    "                    self.count += 1\n",
    "                    R = fast_train_function(inputs)\n",
    "                    if self.count % self.k == 0:\n",
    "                        K.batch_get_value(slow_updates)\n",
    "                        K.batch_get_value(copy_updates)\n",
    "                    return R\n",
    "                \n",
    "                model.train_function = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            # decoupled weight decay (2/4)\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay')\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "#     @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd  # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            # decoupled weight decay (4/4)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.learning_rate)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Position_Embedding(Layer): \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):        \n",
    "        self.size = size         \n",
    "        self.mode = mode       \n",
    "        super(Position_Embedding, self).__init__(**kwargs) \n",
    "\n",
    "    def call(self, x): \n",
    "        if (self.size == None) or (self.mode == 'sum'):            \n",
    "            self.size = int(x.shape[-1])        \n",
    "            batch_size, seq_len = K.shape(x)[0], K.shape(x)[1]        \n",
    "            position_j = 1. / K.pow(10000., \\\n",
    "                2 * K.arange(self.size / 2, dtype='float32') / self.size)        \n",
    "            position_j = K.expand_dims(position_j, 0)        \n",
    "            position_i = K.cumsum(K.ones_like(x[:, :, 0]), 1)-1     \n",
    "            position_i = K.expand_dims(position_i, 2)        \n",
    "            position_ij = K.dot(position_i, position_j)        \n",
    "            position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2) \n",
    "            if self.mode == 'sum': \n",
    "                return position_ij + x \n",
    "            elif self.mode == 'concat': \n",
    "                return K.concatenate([position_ij, x], 2) \n",
    "\n",
    "    def compute_output_shape(self, input_shape): \n",
    "        if self.mode == 'sum': \n",
    "            return input_shape \n",
    "        elif self.mode == 'concat': \n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(Layer): \n",
    "    def __init__(self, output_dim=30, activation=\"relu\",**kwargs): \n",
    "        self.output_dim = output_dim \n",
    "        self.activate = activations.get(activation)\n",
    "        super(FM, self).__init__(**kwargs) \n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.weight = self.add_weight(name='weight',shape=(input_shape[1], self.output_dim),initializer='glorot_uniform',trainable=True) \n",
    "        self.bias = self.add_weight(name='bias',shape=(self.output_dim,),initializer='zeros',trainable=True) \n",
    "        self.kernel = self.add_weight(name='kernel',shape=(input_shape[1], self.output_dim),initializer='glorot_uniform',trainable=True) \n",
    "        super(FM, self).build(input_shape) \n",
    "        \n",
    "    def call(self, x):\n",
    "        feature = K.dot(x,self.weight) + self.bias\n",
    "        a = K.pow(K.dot(x,self.kernel), 2)\n",
    "        b = K.dot(x, K.pow(self.kernel, 2))\n",
    "        cross = K.mean(a-b, 1, keepdims=True)*0.5\n",
    "        cross = K.repeat_elements(K.reshape(cross, (-1, 1)), self.output_dim, axis=-1) \n",
    "        return self.activate(feature + cross) \n",
    "    \n",
    "    def compute_output_shape(self, input_shape): \n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(inshape, outshape):\n",
    "    # create two models\n",
    "    input1 = Input(shape=inshape)\n",
    "    embed_1 = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(input1)\n",
    "#     lstm_1 = LSTM(256)\n",
    "    dense_1 = Dense(128, kernel_initializer='normal', activation='relu')(input1)\n",
    "#     dense_1 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(64, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "#     dense_2 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(32, kernel_initializer='normal', activation='relu')(dense_2)\n",
    "#     dense_3 = Dropout(0.5)(dense_3)\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(dense_3)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer=\"adadelta\", metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet(inshape, outshape):\n",
    "    # create two models\n",
    "    input1  = Input(shape=inshape)\n",
    "\n",
    "    dense_1 = Dense(256, kernel_initializer='normal', activation='relu')(input1)\n",
    "    #dense_1 = Dropout(0.6)(dense_1)\n",
    "    dense_2 = Dense(128, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "    #dense_2 = Dropout(0.6)(dense_2)\n",
    "    dense_2_x  = concatenate([dense_1,dense_2])\n",
    "    dense_3 = Dense(64, kernel_initializer='normal', activation='relu')(dense_2_x)\n",
    "    #dense_3 = Dropout(0.6)(dense_3)\n",
    "    dense_3_x  = concatenate([dense_1,dense_2,dense_3])\n",
    "    dense_4 = Dense(7, kernel_initializer='normal', activation='relu')(dense_3_x)\n",
    "    #dense_4 = Dropout(0.6)(dense_4)\n",
    "    dense_4_x  = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(dense_4_x)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FMNet(inshape, outshape):\n",
    "    # create two models\n",
    "    input1  = Input(shape=inshape)\n",
    "    #DNN_model_I\n",
    "    dense_1 = Dense(100, kernel_initializer='normal', activation='tanh')(input1)\n",
    "    dense_2 = Dense(150, kernel_initializer='normal', activation='tanh')(dense_1)\n",
    "    dense_3 = Dense(150, kernel_initializer='normal', activation='tanh')(dense_2)\n",
    "    dense_4 = Dense(100, kernel_initializer='normal', activation='tanh')(dense_3)\n",
    "    dense_5 = Dense(64, kernel_initializer='normal', activation='tanh')(dense_4)\n",
    "    #FM_model_II\n",
    "    FM_1    = FM(200)(input1)\n",
    "    FM_2    = FM(64)(FM_1)\n",
    "\n",
    "    x       = concatenate([dense_5,FM_2])\n",
    "    x_tmp   = Dense(32,kernel_initializer='normal', activation='softmax')(x)\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(x_tmp)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadnprec(col, max_feature=None, maxlen=None):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    \n",
    "    train_X = train_seq[col].values\n",
    "    test_X = test_seq[col].values\n",
    "    \n",
    "    # Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    # Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    return train_X, test_X, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(col, word_index, max_features):\n",
    "    \n",
    "    EMBEDDING_FILE = (f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "    k = Glove.load(EMBEDDING_FILE)\n",
    "    \n",
    "    embeddings_index = []\n",
    "    for i in tqdm(k.dictionary.keys()):\n",
    "        embeddings_index.append((i,k.word_vectors[k.dictionary[i]]))\n",
    "\n",
    "    embeddings_index = dict(pd.DataFrame(embeddings_index).values)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v(col, word_index, max_features):    \n",
    "    EMBEDDING_FILE = (f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fname = sorted(os.listdir(UFEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()\n",
    "train_feat[UID] = train_user[UID]\n",
    "test_feat[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.set_index(UID, inplace=True)\n",
    "test_feat.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_id = [\"creative_id\", \"ad_id\", \"advertiser_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  w2v_avg_creative_id.pkl\n",
      "current filename:  w2v_avg_ad_id.pkl\n",
      "current filename:  w2v_avg_product_id.pkl\n",
      "current filename:  w2v_avg_product_category.pkl\n",
      "current filename:  w2v_avg_advertiser_id.pkl\n",
      "current filename:  w2v_avg_industry.pkl\n"
     ]
    }
   ],
   "source": [
    "w2v_dic = dict()\n",
    "for cur_id in ID_SET:\n",
    "    for fname in feat_fname:\n",
    "        if fname.startswith(f\"w2v_avg_{cur_id}\"):\n",
    "            print(\"current filename: \", fname)\n",
    "            w2v_dic[cur_id] = pd.read_pickle(f\"{UFEDIR}/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ctid = pd.merge(train_feat, w2v_dic[\"creative_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_adid = pd.merge(train_feat, w2v_dic[\"ad_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_atid = pd.merge(train_feat, w2v_dic[\"advertiser_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "\n",
    "train_pdid = pd.merge(train_feat, w2v_dic[\"product_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_pdct = pd.merge(train_feat, w2v_dic[\"product_category\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "train_inds = pd.merge(train_feat, w2v_dic[\"industry\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ctid = pd.merge(test_feat, w2v_dic[\"creative_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_adid = pd.merge(test_feat, w2v_dic[\"ad_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_atid = pd.merge(test_feat, w2v_dic[\"advertiser_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "\n",
    "test_pdid = pd.merge(test_feat, w2v_dic[\"product_id\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_pdct = pd.merge(test_feat, w2v_dic[\"product_category\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "test_inds = pd.merge(test_feat, w2v_dic[\"industry\"], how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predemb = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ot = pd.DataFrame()\n",
    "# test_ot = pd.DataFrame()\n",
    "# train_ot[UID] = train_user[UID]\n",
    "# test_ot[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats o1\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"train_stats_o1\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         train_ot = pd.merge(train_ot, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "#     elif fname.startswith(\"test_stats_o1\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         test_ot = pd.merge(test_ot, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats getidxmax\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"train_stats_o2_getidxmax\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         train_ot = pd.merge(train_ot, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "#     elif fname.startswith(\"test_stats_o2_getidxmax\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         test_ot = pd.merge(test_ot, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"meta_age_\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         age_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "#         train_ot = pd.merge(train_ot, age_agg_pred, how=\"left\", on=UID)\n",
    "#         test_ot = pd.merge(test_ot, age_agg_pred, how=\"left\", on=UID)\n",
    "#         age_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"meta_gender_\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         gender_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "#         train_ot = pd.merge(train_ot, gender_agg_pred, how=\"left\", on=UID)\n",
    "#         test_ot = pd.merge(test_ot, gender_agg_pred, how=\"left\", on=UID)\n",
    "#         gender_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"tfidf_count_emb_\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         cur_tce = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "#         train_ot = pd.merge(train_ot, cur_tce, how=\"left\", on=UID)\n",
    "#         test_ot = pd.merge(test_ot, cur_tce, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ot.set_index(UID, inplace=True)\n",
    "# test_ot.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to make sure feat and user(target) have same order\n",
    "# # if true --> sum == 0\n",
    "# np.sum(train_ot.index != train_user.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to make sure feat and user(target) have same order\n",
    "# # if true --> sum == 0\n",
    "# np.sum(test_ot.index != test_ot.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to make sure train and test features have same order\n",
    "# # if true --> sum == 0\n",
    "# np.sum(train_ot.columns != test_ot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill mean\n",
    "# for col in tqdm(train_ot.columns):\n",
    "#     if train_ot[col].count() < len(train_feat):\n",
    "#         train_ot[col] = train_ot[col].fillna(np.mean((train_ot[col][~pd.isna(train_ot[col])]).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in tqdm(train_ot.columns):\n",
    "#     if train_ot[col].count() < len(train_ot):\n",
    "#         log(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in tqdm(test_feat.columns):\n",
    "#     if test_ot[col].count() < len(test_feat):\n",
    "#         test_ot[col] = test_ot[col].fillna(np.mean((test_ot[col][~pd.isna(test_ot[col])]).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for col in tqdm(test_ot.columns):\n",
    "#     if test_ot[col].count() < len(test_ot):\n",
    "#         log(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tol_ot = pd.concat([train_ot, test_ot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mms = MinMaxScaler()\n",
    "# mms_tol_ot = mms.fit_transform(tol_ot)\n",
    "# mms_tr_ot = mms_tol_ot[:len(train_ot)]\n",
    "# mms_te_ot = mms_tol_ot[len(train_ot):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 1, 300)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(900000,1,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for age\n",
    "# num_classes = 10\n",
    "# train_y = np_utils.to_categorical(train_user[\"age\"]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gender\n",
    "num_classes = 2\n",
    "train_y = np_utils.to_categorical(train_user[\"gender\"]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [train_ctid, train_adid, train_atid, train_pdid, train_pdct, train_inds]\n",
    "test_set = [test_ctid, test_adid, test_atid, test_pdid, test_pdct, test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ctid = Input(shape=(1,300))\n",
    "inp_adid = Input(shape=(1,300))\n",
    "inp_atid = Input(shape=(1,300))\n",
    "\n",
    "inp_pdid = Input(shape=(1,300))\n",
    "inp_pdct = Input(shape=(1,300))\n",
    "inp_inds = Input(shape=(1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_ot = Input(shape=(mms_tr_ot.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage ctid\n",
    "x = inp_ctid\n",
    "# x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp_ctid)\n",
    "# x = Position_Embedding()(x)\n",
    "# x = SpatialDropout1D(0.1)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "# z = Conv1D(64, kernel_size = 1, kernel_initializer=initializers.he_uniform(seed=2020), activation = \"tanh\")(y)\n",
    "\n",
    "atten_1 = Attention(maxlen)(x) # skip connect\n",
    "atten_2 = Attention(maxlen)(y)\n",
    "# avg_pool = GlobalAveragePooling1D()(y)\n",
    "# max_pool = GlobalMaxPooling1D()(y)\n",
    "# max_pool1 = GlobalMaxPooling1D()(z)\n",
    "\n",
    "# convs = []\n",
    "# filter_sizes = [2, 3, 5, 8]\n",
    "# for fsz in filter_sizes:\n",
    "#     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "#     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "#     l_pool = Flatten()(l_pool)\n",
    "#     convs.append(l_pool)\n",
    "# text_cnn = concatenate(convs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage adid\n",
    "x = inp_adid\n",
    "# x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_adid)\n",
    "# x = Position_Embedding()(x)\n",
    "# x = SpatialDropout1D(0.1)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "atten_3 = Attention(maxlen)(x)\n",
    "atten_4 = Attention(maxlen)(y)\n",
    "\n",
    "# convs = []\n",
    "# filter_sizes = [2, 4, 6, 10]\n",
    "# for fsz in filter_sizes:\n",
    "#     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "#     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "#     l_pool = Flatten()(l_pool)\n",
    "#     convs.append(l_pool)\n",
    "# text_cnn_1 = concatenate(convs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage atid\n",
    "x = inp_atid\n",
    "# x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "# x = Position_Embedding()(x)\n",
    "# x = SpatialDropout1D(0.1)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "atten_5 = Attention(maxlen)(x)\n",
    "atten_6 = Attention(maxlen)(y)\n",
    "\n",
    "# convs = []\n",
    "# filter_sizes = [2, 4, 6, 10]\n",
    "# for fsz in filter_sizes:\n",
    "#     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "#     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "#     l_pool = Flatten()(l_pool)\n",
    "#     convs.append(l_pool)\n",
    "# text_cnn_2 = concatenate(convs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage atid\n",
    "x = inp_pdid\n",
    "# x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "# x = Position_Embedding()(x)\n",
    "# x = SpatialDropout1D(0.1)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "atten_7 = Attention(maxlen)(x)\n",
    "atten_8 = Attention(maxlen)(y)\n",
    "\n",
    "# convs = []\n",
    "# filter_sizes = [2, 4, 6, 10]\n",
    "# for fsz in filter_sizes:\n",
    "#     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "#     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "#     l_pool = Flatten()(l_pool)\n",
    "#     convs.append(l_pool)\n",
    "# text_cnn_2 = concatenate(convs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage atid\n",
    "x = inp_pdct\n",
    "# x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "# x = Position_Embedding()(x)\n",
    "# x = SpatialDropout1D(0.1)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "atten_9 = Attention(maxlen)(x)\n",
    "atten_10 = Attention(maxlen)(y)\n",
    "\n",
    "# convs = []\n",
    "# filter_sizes = [2, 4, 6, 10]\n",
    "# for fsz in filter_sizes:\n",
    "#     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "#     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "#     l_pool = Flatten()(l_pool)\n",
    "#     convs.append(l_pool)\n",
    "# text_cnn_2 = concatenate(convs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage atid\n",
    "x = inp_inds\n",
    "# x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "# x = Position_Embedding()(x)\n",
    "# x = SpatialDropout1D(0.1)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "atten_11 = Attention(maxlen)(x)\n",
    "atten_12 = Attention(maxlen)(y)\n",
    "\n",
    "# convs = []\n",
    "# filter_sizes = [2, 4, 6, 10]\n",
    "# for fsz in filter_sizes:\n",
    "#     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "#     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "#     l_pool = Flatten()(l_pool)\n",
    "#     convs.append(l_pool)\n",
    "# text_cnn_2 = concatenate(convs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stage other\n",
    "# dense_1 = Dense(128, kernel_initializer='normal', activation='relu')(inp_ot)\n",
    "# # dense_1 = Dropout(0.6)(dense_2)\n",
    "# dense_2 = Dense(64, kernel_initializer='normal', activation='relu')(dense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Dense(32)(aux)\n",
    "# x = BatchNormalization()(x)\n",
    "\n",
    "# conc = concatenate([atten_1, atten_2, text_cnn, atten_3, atten_4, text_cnn_1, atten_5, atten_6, text_cnn_2,])\n",
    "conc = concatenate([atten_1, atten_2, atten_3, atten_4, atten_5, atten_6, atten_7, atten_8, atten_9, atten_10, atten_11, atten_12, ])\n",
    "conc = Dense(256)(conc)\n",
    "conc = BatchNormalization()(conc)\n",
    "conc = PReLU()(conc)\n",
    "conc = Dropout(0.2)(conc)\n",
    "\n",
    "conc = Dense(128)(conc)\n",
    "conc = BatchNormalization()(conc)\n",
    "# conc = PReLU()(conc)\n",
    "\n",
    "outp = Dense(num_classes, activation=\"softmax\")(conc)    \n",
    "\n",
    "model = Model(inputs=[inp_ctid, inp_adid, inp_atid, inp_pdid, inp_pdct, inp_inds], outputs=outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([train_ctid, train_adid, train_atid, train_pdid, train_pdct, train_inds], train_y, batch_size=200, epochs=5, callbacks=cb, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_age = model.predict(test_set, \n",
    "                              batch_size=200, \n",
    "                              verbose=1\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"predicted_age.npy\", predicted_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_self_age = model.predict(train_set, \n",
    "                                   batch_size=200, \n",
    "                                   verbose=1\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.argmax(train_y,axis=1),np.argmax(predicted_self_age,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_gender = model.predict(test_set, \n",
    "                              batch_size=200, \n",
    "                              verbose=1\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"predicted_age.npy\", predicted_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_self_gender = model.predict(train_set, \n",
    "                                   batch_size=200, \n",
    "                                   verbose=1\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.argmax(train_y,axis=1),np.argmax(predicted_self_gender,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nn k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_lstm_atten(maxlen, num_classes):\n",
    "    inp_ctid = Input(shape=(1,300))\n",
    "    inp_adid = Input(shape=(1,300))\n",
    "    inp_atid = Input(shape=(1,300))\n",
    "\n",
    "    inp_pdid = Input(shape=(1,300))\n",
    "    inp_pdct = Input(shape=(1,300))\n",
    "    inp_inds = Input(shape=(1,300))\n",
    "\n",
    "    # Stage ctid\n",
    "    x = inp_ctid\n",
    "    # x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp_ctid)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    # z = Conv1D(64, kernel_size = 1, kernel_initializer=initializers.he_uniform(seed=2020), activation = \"tanh\")(y)\n",
    "\n",
    "    atten_1 = Attention(maxlen)(x) # skip connect\n",
    "    atten_2 = Attention(maxlen)(y)\n",
    "    # avg_pool = GlobalAveragePooling1D()(y)\n",
    "    # max_pool = GlobalMaxPooling1D()(y)\n",
    "    # max_pool1 = GlobalMaxPooling1D()(z)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 3, 5, 8]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage adid\n",
    "    x = inp_adid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_adid)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_3 = Attention(maxlen)(x)\n",
    "    atten_4 = Attention(maxlen)(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_1 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_atid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_5 = Attention(maxlen)(x)\n",
    "    atten_6 = Attention(maxlen)(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_pdid\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_7 = Attention(maxlen)(x)\n",
    "    atten_8 = Attention(maxlen)(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_pdct\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_9 = Attention(maxlen)(x)\n",
    "    atten_10 = Attention(maxlen)(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # Stage atid\n",
    "    x = inp_inds\n",
    "    # x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    # x = Position_Embedding()(x)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "\n",
    "    atten_11 = Attention(maxlen)(x)\n",
    "    atten_12 = Attention(maxlen)(y)\n",
    "\n",
    "    # convs = []\n",
    "    # filter_sizes = [2, 4, 6, 10]\n",
    "    # for fsz in filter_sizes:\n",
    "    #     l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "    #     l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "    #     l_pool = Flatten()(l_pool)\n",
    "    #     convs.append(l_pool)\n",
    "    # text_cnn_2 = concatenate(convs, axis=1)\n",
    "\n",
    "    # x = Dense(32)(aux)\n",
    "    # x = BatchNormalization()(x)\n",
    "\n",
    "    # conc = concatenate([atten_1, atten_2, text_cnn, atten_3, atten_4, text_cnn_1, atten_5, atten_6, text_cnn_2,])\n",
    "    conc = concatenate([atten_1, atten_2, atten_3, atten_4, atten_5, atten_6, atten_7, atten_8, atten_9, atten_10, atten_11, atten_12, ])\n",
    "    conc = Dense(256)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "\n",
    "    conc = Dense(128)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    # conc = PReLU()(conc)\n",
    "\n",
    "    outp = Dense(num_classes, activation=\"softmax\")(conc)    \n",
    "\n",
    "    model = Model(inputs=[inp_ctid, inp_adid, inp_atid, inp_pdid, inp_pdct, inp_inds], outputs=outp)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,454,614\n",
      "Trainable params: 3,453,846\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "720000/720000 [==============================] - 455s 632us/step - loss: 1.5066 - acc: 0.3879 - val_loss: 1.4111 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 434s 602us/step - loss: 1.4104 - acc: 0.4159 - val_loss: 1.3909 - val_acc: 0.4235\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 430s 597us/step - loss: 1.3833 - acc: 0.4266 - val_loss: 1.3798 - val_acc: 0.4259\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 432s 599us/step - loss: 1.3632 - acc: 0.4348 - val_loss: 1.3754 - val_acc: 0.4292\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 436s 606us/step - loss: 1.3470 - acc: 0.4402 - val_loss: 1.3737 - val_acc: 0.4302\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 435s 604us/step - loss: 1.3320 - acc: 0.4465 - val_loss: 1.3719 - val_acc: 0.4305\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 439s 610us/step - loss: 1.3167 - acc: 0.4524 - val_loss: 1.3769 - val_acc: 0.4301\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 439s 609us/step - loss: 1.3013 - acc: 0.4579 - val_loss: 1.3784 - val_acc: 0.4297\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 438s 609us/step - loss: 1.2846 - acc: 0.4649 - val_loss: 1.3859 - val_acc: 0.4289\n",
      "180000/180000 [==============================] - 28s 155us/step\n",
      "1000000/1000000 [==============================] - 147s 147us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,454,614\n",
      "Trainable params: 3,453,846\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:40:36 - loss: 3.0093 - acc: 0.1167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.132757). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 455s 631us/step - loss: 1.5131 - acc: 0.3867 - val_loss: 1.4352 - val_acc: 0.4051\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 445s 618us/step - loss: 1.4124 - acc: 0.4148 - val_loss: 1.3949 - val_acc: 0.4223\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 447s 621us/step - loss: 1.3863 - acc: 0.4255 - val_loss: 1.3851 - val_acc: 0.4251\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 450s 625us/step - loss: 1.3676 - acc: 0.4327 - val_loss: 1.3772 - val_acc: 0.4303\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 449s 623us/step - loss: 1.3525 - acc: 0.4389 - val_loss: 1.3785 - val_acc: 0.4294\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 455s 632us/step - loss: 1.3382 - acc: 0.4438 - val_loss: 1.3755 - val_acc: 0.4300\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 451s 627us/step - loss: 1.3241 - acc: 0.4498 - val_loss: 1.3781 - val_acc: 0.4303\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 453s 629us/step - loss: 1.3096 - acc: 0.4550 - val_loss: 1.3808 - val_acc: 0.4306\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 455s 632us/step - loss: 1.2948 - acc: 0.4608 - val_loss: 1.3825 - val_acc: 0.4297\n",
      "180000/180000 [==============================] - 28s 155us/step\n",
      "1000000/1000000 [==============================] - 150s 150us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,454,614\n",
      "Trainable params: 3,453,846\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:39:20 - loss: 3.0260 - acc: 0.0917"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.136982). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 460s 639us/step - loss: 1.5166 - acc: 0.3862 - val_loss: 1.4206 - val_acc: 0.4112\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 450s 625us/step - loss: 1.4138 - acc: 0.4158 - val_loss: 1.4009 - val_acc: 0.4186\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 455s 632us/step - loss: 1.3869 - acc: 0.4254 - val_loss: 1.3867 - val_acc: 0.4236\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 460s 638us/step - loss: 1.3688 - acc: 0.4324 - val_loss: 1.3776 - val_acc: 0.4274\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 457s 635us/step - loss: 1.3535 - acc: 0.4387 - val_loss: 1.3712 - val_acc: 0.4311\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 457s 635us/step - loss: 1.3389 - acc: 0.4447 - val_loss: 1.3727 - val_acc: 0.4292\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 457s 635us/step - loss: 1.3249 - acc: 0.4495 - val_loss: 1.3723 - val_acc: 0.4331\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 462s 641us/step - loss: 1.3108 - acc: 0.4547 - val_loss: 1.3783 - val_acc: 0.4305\n",
      "180000/180000 [==============================] - 29s 162us/step\n",
      "1000000/1000000 [==============================] - 155s 155us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,454,614\n",
      "Trainable params: 3,453,846\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:42:31 - loss: 3.0370 - acc: 0.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.137250). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 462s 642us/step - loss: 1.5171 - acc: 0.3851 - val_loss: 1.4205 - val_acc: 0.4127\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 451s 626us/step - loss: 1.4135 - acc: 0.4149 - val_loss: 1.4028 - val_acc: 0.4194\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 449s 624us/step - loss: 1.3865 - acc: 0.4255 - val_loss: 1.3846 - val_acc: 0.4258\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 451s 626us/step - loss: 1.3680 - acc: 0.4331 - val_loss: 1.3829 - val_acc: 0.4270\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 450s 624us/step - loss: 1.3527 - acc: 0.4385 - val_loss: 1.3755 - val_acc: 0.4309\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 451s 626us/step - loss: 1.3391 - acc: 0.4438 - val_loss: 1.3733 - val_acc: 0.4302\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 453s 629us/step - loss: 1.3249 - acc: 0.4487 - val_loss: 1.3784 - val_acc: 0.4289\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 452s 628us/step - loss: 1.3106 - acc: 0.4548 - val_loss: 1.3836 - val_acc: 0.4294\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 452s 628us/step - loss: 1.2962 - acc: 0.4605 - val_loss: 1.3864 - val_acc: 0.4295\n",
      "180000/180000 [==============================] - 28s 156us/step\n",
      "1000000/1000000 [==============================] - 146s 146us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,454,614\n",
      "Trainable params: 3,453,846\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:43:41 - loss: 3.0717 - acc: 0.1167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.139848). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 474s 659us/step - loss: 1.5147 - acc: 0.3864 - val_loss: 1.4322 - val_acc: 0.4078\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 458s 636us/step - loss: 1.4133 - acc: 0.4147 - val_loss: 1.4000 - val_acc: 0.4214\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 456s 634us/step - loss: 1.3865 - acc: 0.4250 - val_loss: 1.3839 - val_acc: 0.4253\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 455s 632us/step - loss: 1.3680 - acc: 0.4322 - val_loss: 1.3790 - val_acc: 0.4303\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 455s 632us/step - loss: 1.3531 - acc: 0.4376 - val_loss: 1.3776 - val_acc: 0.4308\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 457s 635us/step - loss: 1.3391 - acc: 0.4429 - val_loss: 1.3732 - val_acc: 0.4329\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 463s 642us/step - loss: 1.3255 - acc: 0.4485 - val_loss: 1.3754 - val_acc: 0.4330\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 462s 642us/step - loss: 1.3111 - acc: 0.4549 - val_loss: 1.3785 - val_acc: 0.4298\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 461s 640us/step - loss: 1.2961 - acc: 0.4600 - val_loss: 1.3843 - val_acc: 0.4310\n",
      "180000/180000 [==============================] - 28s 158us/step\n",
      "1000000/1000000 [==============================] - 151s 151us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4299088888888889"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]\n",
    "maxlen=200\n",
    "# for age\n",
    "num_classes = 10\n",
    "train_y = np_utils.to_categorical(train_user[\"age\"]-1)\n",
    "# # for gender\n",
    "# num_classes = 2\n",
    "# train_y = np_utils.to_categorical(train_user[\"gender\"]-1)\n",
    "\n",
    "test_set = [test_ctid, test_adid, test_atid, test_pdid, test_pdct, test_inds]\n",
    "predicted_train_age = np.zeros(train_y.shape)\n",
    "predicted_age = np.zeros((len(test_user),num_classes))\n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_ctid, train_y))\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    K.clear_session()\n",
    "    \n",
    "    X_train_ctid = train_ctid[train_idx]\n",
    "    X_train_adid = train_adid[train_idx]\n",
    "    X_train_atid = train_atid[train_idx]\n",
    "    X_train_pdid = train_pdid[train_idx]\n",
    "    X_train_pdct = train_pdct[train_idx]\n",
    "    X_train_inds = train_inds[train_idx]\n",
    "    X_val_train_ctid = train_ctid[valid_idx]\n",
    "    X_val_train_adid = train_adid[valid_idx]\n",
    "    X_val_train_atid = train_atid[valid_idx]\n",
    "    X_val_train_pdid = train_pdid[valid_idx]\n",
    "    X_val_train_pdct = train_pdct[valid_idx]\n",
    "    X_val_train_inds = train_inds[valid_idx]\n",
    "    \n",
    "    y_train = train_y[train_idx]\n",
    "    y_val = train_y[valid_idx]\n",
    "\n",
    "    X_train_set = [X_train_ctid, X_train_adid, X_train_atid, X_train_pdid, X_train_pdct, X_train_inds]\n",
    "    X_val_set = [X_val_train_ctid, X_val_train_adid, X_val_train_atid, X_val_train_pdid, X_val_train_pdct, X_val_train_inds]\n",
    "    \n",
    "    model = sim_lstm_atten(maxlen, num_classes)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])\n",
    "    print(\"MODEL COMPLIE...\")\n",
    "    model.summary()\n",
    "    \n",
    "    model.fit(X_train_set, y_train, batch_size=200, epochs=50, validation_data=(X_val_set, y_val), callbacks=cb, verbose=1)\n",
    "    pred_val_y = model.predict(X_val_set, batch_size=200, verbose=1)\n",
    "    pred_test_y = model.predict(test_set, batch_size=200, verbose=1)\n",
    "    predicted_train_age[valid_idx] = pred_val_y\n",
    "    predicted_age += pred_test_y / len(splits)\n",
    "    \n",
    "accuracy_score(np.argmax(train_y,axis=1),np.argmax(predicted_train_age,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            258         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,453,582\n",
      "Trainable params: 3,452,814\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:44:43 - loss: 1.1167 - acc: 0.5467"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.139393). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 478s 664us/step - loss: 0.2143 - acc: 0.9202 - val_loss: 0.1896 - val_acc: 0.9293\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 466s 648us/step - loss: 0.1928 - acc: 0.9288 - val_loss: 0.1833 - val_acc: 0.9322\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 464s 645us/step - loss: 0.1864 - acc: 0.9316 - val_loss: 0.1820 - val_acc: 0.9330\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 467s 648us/step - loss: 0.1812 - acc: 0.9336 - val_loss: 0.1814 - val_acc: 0.9332\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 469s 651us/step - loss: 0.1768 - acc: 0.9353 - val_loss: 0.1799 - val_acc: 0.9332\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 466s 647us/step - loss: 0.1729 - acc: 0.9372 - val_loss: 0.1833 - val_acc: 0.9319\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 469s 651us/step - loss: 0.1688 - acc: 0.9388 - val_loss: 0.1840 - val_acc: 0.9328\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 467s 649us/step - loss: 0.1645 - acc: 0.9405 - val_loss: 0.1825 - val_acc: 0.9332\n",
      "180000/180000 [==============================] - 29s 160us/step\n",
      "1000000/1000000 [==============================] - 151s 151us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            258         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,453,582\n",
      "Trainable params: 3,452,814\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:44:49 - loss: 1.1661 - acc: 0.4550"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.135528). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 491s 682us/step - loss: 0.2157 - acc: 0.9192 - val_loss: 0.1931 - val_acc: 0.9279\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 479s 665us/step - loss: 0.1928 - acc: 0.9288 - val_loss: 0.1885 - val_acc: 0.9302\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 478s 664us/step - loss: 0.1867 - acc: 0.9315 - val_loss: 0.1858 - val_acc: 0.9317\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 478s 663us/step - loss: 0.1817 - acc: 0.9333 - val_loss: 0.1848 - val_acc: 0.9317\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 480s 667us/step - loss: 0.1777 - acc: 0.9351 - val_loss: 0.1824 - val_acc: 0.9330\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 478s 664us/step - loss: 0.1740 - acc: 0.9367 - val_loss: 0.1829 - val_acc: 0.9330\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 480s 667us/step - loss: 0.1701 - acc: 0.9384 - val_loss: 0.1831 - val_acc: 0.9331\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 479s 665us/step - loss: 0.1661 - acc: 0.9398 - val_loss: 0.1867 - val_acc: 0.9313\n",
      "180000/180000 [==============================] - 30s 168us/step\n",
      "1000000/1000000 [==============================] - 162s 162us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            258         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,453,582\n",
      "Trainable params: 3,452,814\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:44:59 - loss: 0.8641 - acc: 0.6017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.139719). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 491s 682us/step - loss: 0.2142 - acc: 0.9202 - val_loss: 0.1953 - val_acc: 0.9279\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 480s 666us/step - loss: 0.1925 - acc: 0.9291 - val_loss: 0.1881 - val_acc: 0.9304\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 477s 662us/step - loss: 0.1861 - acc: 0.9317 - val_loss: 0.1844 - val_acc: 0.9320\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 492s 684us/step - loss: 0.1816 - acc: 0.9335 - val_loss: 0.1864 - val_acc: 0.9316\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 499s 693us/step - loss: 0.1777 - acc: 0.9352 - val_loss: 0.1857 - val_acc: 0.9321\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 500s 695us/step - loss: 0.1740 - acc: 0.9367 - val_loss: 0.1832 - val_acc: 0.9325\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 500s 695us/step - loss: 0.1701 - acc: 0.9382 - val_loss: 0.1865 - val_acc: 0.9310\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 500s 695us/step - loss: 0.1664 - acc: 0.9399 - val_loss: 0.1847 - val_acc: 0.9328\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 502s 697us/step - loss: 0.1621 - acc: 0.9415 - val_loss: 0.1892 - val_acc: 0.9320\n",
      "180000/180000 [==============================] - 32s 176us/step\n",
      "1000000/1000000 [==============================] - 167s 167us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            258         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,453,582\n",
      "Trainable params: 3,452,814\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:46:27 - loss: 1.0973 - acc: 0.4783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.140859). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 511s 710us/step - loss: 0.2148 - acc: 0.9201 - val_loss: 0.1976 - val_acc: 0.9265\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 502s 697us/step - loss: 0.1920 - acc: 0.9290 - val_loss: 0.1923 - val_acc: 0.9287\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 501s 696us/step - loss: 0.1861 - acc: 0.9318 - val_loss: 0.1889 - val_acc: 0.9303\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 499s 694us/step - loss: 0.1812 - acc: 0.9335 - val_loss: 0.1876 - val_acc: 0.9311\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 500s 695us/step - loss: 0.1773 - acc: 0.9350 - val_loss: 0.1854 - val_acc: 0.9323\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 505s 701us/step - loss: 0.1738 - acc: 0.9367 - val_loss: 0.1863 - val_acc: 0.9325\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 499s 693us/step - loss: 0.1699 - acc: 0.9381 - val_loss: 0.1847 - val_acc: 0.9321\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 500s 694us/step - loss: 0.1660 - acc: 0.9396 - val_loss: 0.1897 - val_acc: 0.9317\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 501s 695us/step - loss: 0.1620 - acc: 0.9414 - val_loss: 0.1883 - val_acc: 0.9313\n",
      "Epoch 10/50\n",
      "720000/720000 [==============================] - 500s 694us/step - loss: 0.1574 - acc: 0.9429 - val_loss: 0.1895 - val_acc: 0.9308\n",
      "180000/180000 [==============================] - 31s 172us/step\n",
      "1000000/1000000 [==============================] - 163s 163us/step\n",
      "MODEL COMPLIE...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 256)       439296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 256)       329472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 256)       329472      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 256)       329472      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 256)       329472      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 256)       329472      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 128)       123264      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 128)       123264      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 128)       123264      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 128)       123264      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 128)       123264      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 128)       123264      bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          257         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          129         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          257         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          129         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          257         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          129         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          257         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          129         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 256)          257         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          129         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 256)          257         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          129         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 attention_10[0][0]               \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          590080      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            258         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,453,582\n",
      "Trainable params: 3,452,814\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 180000 samples\n",
      "Epoch 1/50\n",
      "   600/720000 [..............................] - ETA: 3:44:00 - loss: 1.0661 - acc: 0.4917"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.141387). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720000/720000 [==============================] - 508s 705us/step - loss: 0.2154 - acc: 0.9195 - val_loss: 0.1961 - val_acc: 0.9272\n",
      "Epoch 2/50\n",
      "720000/720000 [==============================] - 496s 690us/step - loss: 0.1929 - acc: 0.9285 - val_loss: 0.1906 - val_acc: 0.9295\n",
      "Epoch 3/50\n",
      "720000/720000 [==============================] - 497s 690us/step - loss: 0.1864 - acc: 0.9314 - val_loss: 0.1861 - val_acc: 0.9313\n",
      "Epoch 4/50\n",
      "720000/720000 [==============================] - 495s 687us/step - loss: 0.1816 - acc: 0.9334 - val_loss: 0.1850 - val_acc: 0.9322\n",
      "Epoch 5/50\n",
      "720000/720000 [==============================] - 496s 689us/step - loss: 0.1777 - acc: 0.9351 - val_loss: 0.1891 - val_acc: 0.9297\n",
      "Epoch 6/50\n",
      "720000/720000 [==============================] - 497s 690us/step - loss: 0.1737 - acc: 0.9367 - val_loss: 0.1844 - val_acc: 0.9324\n",
      "Epoch 7/50\n",
      "720000/720000 [==============================] - 497s 690us/step - loss: 0.1701 - acc: 0.9382 - val_loss: 0.1845 - val_acc: 0.9323\n",
      "Epoch 8/50\n",
      "720000/720000 [==============================] - 496s 689us/step - loss: 0.1661 - acc: 0.9399 - val_loss: 0.1852 - val_acc: 0.9322\n",
      "Epoch 9/50\n",
      "720000/720000 [==============================] - 500s 695us/step - loss: 0.1620 - acc: 0.9415 - val_loss: 0.1911 - val_acc: 0.9309\n",
      "180000/180000 [==============================] - 30s 167us/step\n",
      "1000000/1000000 [==============================] - 157s 157us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9316455555555555"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]\n",
    "\n",
    "maxlen=200\n",
    "# for age\n",
    "num_classes = 2\n",
    "train_y = np_utils.to_categorical(train_user[\"gender\"]-1)\n",
    "# # for gender\n",
    "# num_classes = 2\n",
    "# train_y = np_utils.to_categorical(train_user[\"gender\"]-1)\n",
    "\n",
    "test_set = [test_ctid, test_adid, test_atid, test_pdid, test_pdct, test_inds]\n",
    "predicted_train_gender = np.zeros(train_y.shape)\n",
    "predicted_gender = np.zeros((len(test_user),num_classes))\n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_ctid, train_y))\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    K.clear_session()\n",
    "    \n",
    "    X_train_ctid = train_ctid[train_idx]\n",
    "    X_train_adid = train_adid[train_idx]\n",
    "    X_train_atid = train_atid[train_idx]\n",
    "    X_train_pdid = train_pdid[train_idx]\n",
    "    X_train_pdct = train_pdct[train_idx]\n",
    "    X_train_inds = train_inds[train_idx]\n",
    "    X_val_train_ctid = train_ctid[valid_idx]\n",
    "    X_val_train_adid = train_adid[valid_idx]\n",
    "    X_val_train_atid = train_atid[valid_idx]\n",
    "    X_val_train_pdid = train_pdid[valid_idx]\n",
    "    X_val_train_pdct = train_pdct[valid_idx]\n",
    "    X_val_train_inds = train_inds[valid_idx]\n",
    "    \n",
    "    y_train = train_y[train_idx]\n",
    "    y_val = train_y[valid_idx]\n",
    "\n",
    "    X_train_set = [X_train_ctid, X_train_adid, X_train_atid, X_train_pdid, X_train_pdct, X_train_inds]\n",
    "    X_val_set = [X_val_train_ctid, X_val_train_adid, X_val_train_atid, X_val_train_pdid, X_val_train_pdct, X_val_train_inds]\n",
    "    \n",
    "    model = sim_lstm_atten(maxlen, num_classes)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])\n",
    "    print(\"MODEL COMPLIE...\")\n",
    "    model.summary()\n",
    "    \n",
    "    model.fit(X_train_set, y_train, batch_size=200, epochs=50, validation_data=(X_val_set, y_val), callbacks=cb, verbose=1)\n",
    "    pred_val_y = model.predict(X_val_set, batch_size=200, verbose=1)\n",
    "    pred_test_y = model.predict(test_set, batch_size=200, verbose=1)\n",
    "    predicted_train_gender[valid_idx] = pred_val_y\n",
    "    predicted_gender += pred_test_y / len(splits)\n",
    "\n",
    "accuracy_score(np.argmax(train_y,axis=1),np.argmax(predicted_train_gender,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: \n",
    "# 1. add stats feature and prediction embedding\n",
    "# 2. optimize embedding\n",
    "# 3. adjust network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3615544444444443"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4299088888888889 + 0.9316455555555555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{UDDIR}/imd/seqidx_dic.pkl\", \"rb\") as f:\n",
    "    seqidx_dic = pickle.loads(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_ids = list(combinations(ID_SET, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_eq = True\n",
    "for cp in comb_ids:\n",
    "    if (seqidx_dic[cp[0]] != seqidx_dic[cp[1]]).sum() != 0:\n",
    "        print(cp)\n",
    "        is_eq = False\n",
    "        break\n",
    "assert is_eq, \"Must True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder index\n",
    "re_train_user = pd.DataFrame(index=seqidx_dic[\"creative_id\"][seqidx_dic[\"creative_id\"].isin(train_user[UID])])\n",
    "re_train_user = pd.merge(re_train_user, train_user, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder index\n",
    "re_test_user = pd.DataFrame(index=seqidx_dic[\"creative_id\"][~seqidx_dic[\"creative_id\"].isin(train_user[UID])])\n",
    "re_test_user = pd.merge(re_test_user, test_user, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np_utils.to_categorical(re_train_user[\"age\"]-1)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"creative_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_val = train_seq[col].values\n",
    "test_seq_val = test_seq[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_seq_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = tokenizer.texts_to_sequences(train_seq_val)\n",
    "test_X = tokenizer.texts_to_sequences(test_seq_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=35)\n",
    "test_X = pad_sequences(test_X, maxlen=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(train_X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(35,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_1 = Embedding(35, 300)(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #     lstm_1 = LSTM(256)\n",
    "# dense_1 = Dense(128, kernel_initializer='normal', activation='relu')(embed_1)\n",
    "# #     dense_1 = Dropout(0.5)(dense_1)\n",
    "# dense_2 = Dense(64, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "# #     dense_2 = Dropout(0.5)(dense_2)\n",
    "# dense_3 = Dense(32, kernel_initializer='normal', activation='relu')(dense_2)\n",
    "# #     dense_3 = Dropout(0.5)(dense_3)\n",
    "# out     = Dense(10,kernel_initializer='normal', activation='softmax')(dense_3)\n",
    "# Compile model\n",
    "# model = Model(inputs=input1, outputs = input1)\n",
    "# model.compile(loss ='categorical_crossentropy', optimizer=\"adadelta\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     train_X, \n",
    "#     train_y, \n",
    "#     batch_size=2048, \n",
    "#     epochs=10, \n",
    "# #     validation_data=([train_X], train_y), \n",
    "#     callbacks = cb, \n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1024\n",
    "model = Sequential()\n",
    "model.add(Embedding(2481135, output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(2481116, 64))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X[:1000], train_y[:1000], batch_size=2048, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = model.predict(train_X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "assert output_array.shape == (32, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_seq[col].values[:10000], train_seq[col].values[:10000], batch_size=2048, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_softmax= model.predict(test_X[:10000], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.argmax(res_softmax, axis=1)+1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "mms_tol_feat = mms.fit_transform(tol_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mms_tr_feat = mms_tol_feat[:len(train_feat)]\n",
    "mms_te_feat = mms_tol_feat[len(train_feat):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every category\n",
    "# pd.Series(np.argmax(train_y, axis=1)+1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "st_X_train, st_X_val, st_y_train, st_y_val = train_test_split(mms_tr_feat, train_y, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model(inshape=(st_X_train.shape[1],), outshape=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    st_X_train, \n",
    "    st_y_train, \n",
    "    batch_size=2048, \n",
    "    epochs=10, \n",
    "    validation_data=([st_X_val], st_y_val), \n",
    "    callbacks = cb, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(st_X_val, st_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_softmax= model.predict(test_feat, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_age = np.argmax(res_softmax, axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = model.predict(st_X_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 9000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 300 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"creative_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.DataFrame()\n",
    "test_X = pd.DataFrame()\n",
    "embedding_matrix = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\"]:\n",
    "    cur_train_X, cur_test_X, word_index = loadnprec(col, maxlen=maxlen)\n",
    "    cur_embedding_matrix = load_glove(col, word_index, max_features)\n",
    "    # concat\n",
    "    if len(train_X):\n",
    "        train_X = np.concatenate((train_X, cur_train_X), axis=1)\n",
    "    else:\n",
    "        train_X = cur_train_X\n",
    "    # concat\n",
    "    if len(test_X):\n",
    "        test_X = np.concatenate((test_X, cur_test_X), axis=1)\n",
    "    else:\n",
    "        test_X = cur_test_X\n",
    "    # concat\n",
    "    if len(embedding_matrix):\n",
    "        embedding_matrix = np.concatenate((embedding_matrix, cur_embedding_matrix), axis=1)\n",
    "    else:\n",
    "        embedding_matrix = cur_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = copy.deepcopy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = copy.deepcopy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_atten(max_features, embedding_matrix):\n",
    "    \n",
    "    inp_active = Input(shape=(maxlen,))\n",
    "    \n",
    "    # Stage ctid\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp_active)\n",
    "    x = Position_Embedding()(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    z = Conv1D(64, kernel_size = 1, kernel_initializer=initializers.he_uniform(seed=2020), activation = \"tanh\")(y)\n",
    "    \n",
    "    atten_1 = Attention(maxlen)(x) # skip connect\n",
    "    atten_2 = Attention(maxlen)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    max_pool1 = GlobalMaxPooling1D()(z)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2, 3, 5, 8]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    text_cnn = concatenate(convs, axis=1)\n",
    "\n",
    "    \n",
    "    # Stage adid\n",
    "    x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    x = Position_Embedding()(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    atten_3 = Attention(maxlen)(x)\n",
    "    atten_4 = Attention(maxlen)(y)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2, 4, 6, 10]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    text_cnn_1 = concatenate(convs, axis=1)\n",
    "    \n",
    "    # Stage atid\n",
    "    x = Embedding(u_max_features, embed_size, weights=[u_embedding_matrix], trainable=False)(inp_usage)\n",
    "    x = Position_Embedding()(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    atten_3 = Attention(maxlen)(x)\n",
    "    atten_4 = Attention(maxlen)(y)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2, 4, 6, 10]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    text_cnn_1 = concatenate(convs, axis=1)    \n",
    "    \n",
    "\n",
    "      \n",
    "    \n",
    "#     x = Dense(32)(aux)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool, max_pool1, text_cnn,])\n",
    "    conc = Dense(256)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    \n",
    "    conc = Dense(128)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "#     conc = PReLU()(conc)\n",
    "\n",
    "    outp = Dense(num_classes, activation=\"softmax\")(conc)    \n",
    "\n",
    "    model = Model(inputs=[inp_active], outputs=outp)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inshape = (train_X.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]\n",
    "\n",
    "train_meta = np.zeros(train_y.shape)\n",
    "test_meta = np.zeros((test_X.shape[0],num_classes))\n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_X, train_y))\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    K.clear_session()\n",
    "    X_train = train_X[train_idx]\n",
    "    X_val = train_X[valid_idx]\n",
    "    y_train = train_y[train_idx]\n",
    "    y_val = train_y[valid_idx]\n",
    "    \n",
    "#     Aux_train = aux_train[train_idx]|\n",
    "#     Aux_val = aux_train[valid_idx]\n",
    "    \n",
    "    model = model_lstm_atten(max_features, embedding_matrix)\n",
    "#     model = multi_gpu_model(model, gpus=8)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])\n",
    "    print(\"MODEL COMPLIE...\")\n",
    "#     lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "#     lookahead.inject(model) # add into model\n",
    "#     print(\"LOOKAHEAD INJECT...\")\n",
    "    model.summary()\n",
    "    model.fit([X_train], y_train, batch_size=1024, epochs=1, validation_data=([X_val], y_val), callbacks = cb, verbose=1)\n",
    "    pred_val_y = model.predict([X_val], batch_size=2048, verbose=1)\n",
    "    pred_test_y = model.predict([test_X], batch_size=2048, verbose=1)\n",
    "    train_meta[valid_idx] = pred_val_y\n",
    "    test_meta += pred_test_y / len(splits)\n",
    "\n",
    "print(accuracy_score(np.argmax(train_y,axis=1),np.argmax(train_meta,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(K.name_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(K.variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb_multi_age = lgb.Booster(model_file=f\"{UMDIR}/lgb_multi_age_20200511045531.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb_multi_gender = lgb.Booster(model_file=f\"{UMDIR}/lgb_multi_gender_20200511034408.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_user[[UID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res[\"predicted_age\"] = np.argmax(predicted_age, axis=1)+1\n",
    "# res[\"predicted_age\"] = predicted_age\n",
    "# res[\"predicted_age\"] = [list(x).index(max(x))+1 for x in model_lgb_multi_age.predict(test_feat, num_iteration=model_lgb_multi_age.best_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"] = np.argmax(predicted_gender, axis=1)+1\n",
    "# res[\"predicted_gender\"] = [list(x).index(max(x))+1 for x in model_lgb_multi_gender.predict(test_feat, num_iteration=model_lgb_multi_gender.best_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_suffix = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))\n",
    "res.to_csv(f\"{RESDIR}/res-{res_suffix}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     282570\n",
       "2     175742\n",
       "4     149807\n",
       "5     128458\n",
       "6     121039\n",
       "7      62868\n",
       "1      25733\n",
       "9      21920\n",
       "8      19686\n",
       "10     12177\n",
       "Name: predicted_age, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    675035\n",
       "2    324965\n",
       "Name: predicted_gender, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(f\"{RESDIR}/res-20200528143657.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     267709\n",
       "2     180465\n",
       "4     155289\n",
       "5     133065\n",
       "6     120413\n",
       "7      53172\n",
       "1      27688\n",
       "8      26677\n",
       "9      21253\n",
       "10     14269\n",
       "Name: predicted_age, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    675774\n",
       "2    324226\n",
       "Name: predicted_gender, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"] = tmp[\"predicted_gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"predicted_age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cent result to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ti import session\n",
    "ti_session = session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ti_session.upload_data(path=f\"{RESDIR}/res-20200515004850.csv\", bucket=\"etveritas-1252104022\", key_prefix=RESDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
