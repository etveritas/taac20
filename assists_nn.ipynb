{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if used pip install package\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install wget\n",
    "# !pip install gensim\n",
    "# !pip install catboost\n",
    "# !pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import wget\n",
    "import time\n",
    "import tarfile\n",
    "import zipfile\n",
    "import functools\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from itertools import product, combinations\n",
    "from scipy.special import comb, perm\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cbt\n",
    "from glove import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression,BayesianRidge,SGDClassifier,PassiveAggressiveClassifier,RidgeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC,NuSVC,SVC\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, LSTM, GRU  #, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import *\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import gensim\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# ray.init(object_store_memory=int(100e6))\n",
    "# import modin.pandas as pd\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, filename='default.log', stream=sys.stdout):\n",
    "        self.terminal = stream\n",
    "        self.log = open(filename, 'a')\n",
    "        \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        \n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.stdout = Logger(\"logs/default.log\", sys.stdout)\n",
    "# sys.stderr = Logger(\"logs/default_err.log\", sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDIR = \"data\"\n",
    "UDDIR = \"user_data\"\n",
    "UFEDIR = \"user_data/feat_data_v05\"\n",
    "UMDIR = \"user_data/model_data\"\n",
    "RESDIR = \"prediction_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UID = \"user_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (Only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/train_preliminary.zip\", out=DDIR)\n",
    "# test_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/test.zip\", out=DDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myunzip(filename):\n",
    "#     zFile = zipfile.ZipFile(filename, \"r\")\n",
    "#     for fileM in zFile.namelist(): \n",
    "#         zFile.extract(fileM, DDIR)\n",
    "#         print(fileM)\n",
    "#     zFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myunzip(train_fname)\n",
    "# myunzip(test_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bch_rencol(values, prefix=\"\", suffix=\"\"):\n",
    "    return list(map(lambda x: f\"{prefix}\"+\"_\".join(list(map(lambda y: str(y), x)))+f\"{suffix}\" \n",
    "                    if hasattr(x, \"__iter__\") and not isinstance(x, str) \n",
    "                    else f\"{prefix}\"+str(x)+f\"{suffix}\", values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynunique(values):\n",
    "    return values.nunique(dropna=False)\n",
    "def getidxmax(x):\n",
    "    return x.idxmax()[1]\n",
    "# for time series\n",
    "def at_len(x):\n",
    "    return len(x)\n",
    "\n",
    "def at_sum(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def at_max(x):\n",
    "    return np.max(x)\n",
    "\n",
    "def at_min(x):\n",
    "    return np.min(x)\n",
    "\n",
    "def at_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def at_range(x):\n",
    "    return at_max(x) - at_min(x)\n",
    "\n",
    "def at_nunq(x):\n",
    "    return len(set(x))\n",
    "\n",
    "def at_lenDrange(x):\n",
    "    return at_len(x)/(at_range(x)+1)\n",
    "\n",
    "def at_lenDnunq(x):\n",
    "    return at_len(x)/at_nunq(x)\n",
    "\n",
    "def at_percentile(n):\n",
    "    def at_percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    at_percentile_.__name__ = f\"at_percentile_{n}\"\n",
    "    return at_percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_SET = [\"sum\", \"max\", \"min\", \"mean\", \"std\"]\n",
    "OP_SET1 = [\"nunique\", \"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"skew\", at_percentile(0.25), at_percentile(0.75)]\n",
    "OP_SET2 = [\"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"skew\", at_percentile(0.25), at_percentile(0.75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_SET = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\", \"time\", \"click_times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting_level = 0\n",
    "is_start = None\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        self.history = [self.start]\n",
    "\n",
    "    def check(self, info):\n",
    "        current = time.time()\n",
    "        print(f\"[{info}] spend {current - self.history[-1]:0.2f} sec\")\n",
    "        self.history.append(current)\n",
    "\n",
    "def log(entry):\n",
    "    global nesting_level\n",
    "    space = \"-\" * (4 * nesting_level)\n",
    "    print(f\"{space}{entry}\")\n",
    "\n",
    "def timeit(method, start_log=None):\n",
    "    @functools.wraps(method)\n",
    "    def timed(*args, **kw):\n",
    "        global is_start\n",
    "        global nesting_level\n",
    "\n",
    "        if not is_start:\n",
    "            print()\n",
    "\n",
    "        is_start = True\n",
    "        log(f\"Start [{method.__name__}]:\" + (start_log if start_log else \"\"))\n",
    "        log(f'Start time: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        nesting_level += 1\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        end_time = time.time()\n",
    "\n",
    "        nesting_level -= 1\n",
    "        log(f\"End   [{method.__name__}]. Time elapsed: {end_time - start_time:0.2f} sec.\")\n",
    "        is_start = False\n",
    "\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training&Prediction by NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "\n",
    "    \"\"\"RAdam optimizer.\n",
    "\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay for each param.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n",
    "        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n",
    "        min_lr: float >= 0. Minimum learning rate after warmup.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n",
    "                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n",
    "        super(RAdam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.total_steps = K.variable(total_steps, name='total_steps')\n",
    "            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n",
    "            self.min_lr = K.variable(min_lr, name='min_lr')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.initial_weight_decay = weight_decay\n",
    "        self.initial_total_steps = total_steps\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.learning_rate\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        if self.initial_total_steps > 0:\n",
    "            warmup_steps = self.total_steps * self.warmup_proportion\n",
    "            decay_steps = self.total_steps - warmup_steps\n",
    "            lr = K.switch(\n",
    "                t <= warmup_steps,\n",
    "                lr * (t / warmup_steps),\n",
    "                lr * (1.0 - K.minimum(t, decay_steps) / decay_steps),\n",
    "            )\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        else:\n",
    "            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        beta_1_t = K.pow(self.beta_1, t)\n",
    "        beta_2_t = K.pow(self.beta_2, t)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            m_corr_t = m_t / (1.0 - beta_1_t)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t) + self.epsilon)\n",
    "\n",
    "            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                         (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                         sma_inf / sma_t)\n",
    "\n",
    "            p_t = K.switch(sma_t > 5, r_t * m_corr_t / v_corr_t, m_corr_t)\n",
    "\n",
    "            if self.initial_weight_decay > 0:\n",
    "                p_t += self.weight_decay * p\n",
    "\n",
    "            p_t = p - lr * p_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'learning_rate': float(K.get_value(self.learning_rate)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'decay': float(K.get_value(self.decay)),\n",
    "            'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': float(K.get_value(self.total_steps)),\n",
    "            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n",
    "            'min_lr': float(K.get_value(self.min_lr)),\n",
    "        }\n",
    "        base_config = super(RAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(object):\n",
    "    \"\"\"Add the [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) functionality for [keras](https://keras.io/).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5, alpha=0.5):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.count = 0\n",
    "\n",
    "    def inject(self, model):\n",
    "        \"\"\"Inject the Lookahead algorithm for the given model.\n",
    "        The following code is modified from keras's _make_train_function method.\n",
    "        See: https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497\n",
    "        \"\"\"\n",
    "        if not hasattr(model, 'train_function'):\n",
    "            raise RuntimeError('You must compile your model before using it.')\n",
    "\n",
    "        model._check_trainable_weights_consistency()\n",
    "\n",
    "        if model.train_function is None:\n",
    "            inputs = (model._feed_inputs +\n",
    "                      model._feed_targets +\n",
    "                      model._feed_sample_weights)\n",
    "            if model._uses_dynamic_learning_phase():\n",
    "                inputs += [K.learning_phase()]\n",
    "            fast_params = model._collected_trainable_weights\n",
    "\n",
    "            with K.name_scope('training'):\n",
    "                with K.name_scope(model.optimizer.__class__.__name__):\n",
    "                    training_updates = model.optimizer.get_updates(\n",
    "                        params=fast_params,\n",
    "                        loss=model.total_loss)\n",
    "                    slow_params = [K.variable(p) for p in fast_params]\n",
    "                fast_updates = (model.updates +\n",
    "                                training_updates +\n",
    "                                model.metrics_updates)\n",
    "\n",
    "                slow_updates, copy_updates = [], []\n",
    "                for p, q in zip(fast_params, slow_params):\n",
    "                    slow_updates.append(K.update(q, q + self.alpha * (p - q)))\n",
    "                    copy_updates.append(K.update(p, q))\n",
    "\n",
    "                # Gets loss and metrics. Updates weights at each call.\n",
    "                fast_train_function = K.function(\n",
    "                    inputs,\n",
    "                    [model.total_loss] + model.metrics_tensors,\n",
    "                    updates=fast_updates,\n",
    "                    name='fast_train_function',\n",
    "                    **model._function_kwargs)\n",
    "\n",
    "                def F(inputs):\n",
    "                    self.count += 1\n",
    "                    R = fast_train_function(inputs)\n",
    "                    if self.count % self.k == 0:\n",
    "                        K.batch_get_value(slow_updates)\n",
    "                        K.batch_get_value(copy_updates)\n",
    "                    return R\n",
    "                \n",
    "                model.train_function = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            # decoupled weight decay (2/4)\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay')\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "#     @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd  # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            # decoupled weight decay (4/4)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.learning_rate)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Position_Embedding(Layer): \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):        \n",
    "        self.size = size         \n",
    "        self.mode = mode       \n",
    "        super(Position_Embedding, self).__init__(**kwargs) \n",
    "\n",
    "    def call(self, x): \n",
    "        if (self.size == None) or (self.mode == 'sum'):            \n",
    "            self.size = int(x.shape[-1])        \n",
    "            batch_size, seq_len = K.shape(x)[0], K.shape(x)[1]        \n",
    "            position_j = 1. / K.pow(10000., \\\n",
    "                2 * K.arange(self.size / 2, dtype='float32') / self.size)        \n",
    "            position_j = K.expand_dims(position_j, 0)        \n",
    "            position_i = K.cumsum(K.ones_like(x[:, :, 0]), 1)-1     \n",
    "            position_i = K.expand_dims(position_i, 2)        \n",
    "            position_ij = K.dot(position_i, position_j)        \n",
    "            position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2) \n",
    "            if self.mode == 'sum': \n",
    "                return position_ij + x \n",
    "            elif self.mode == 'concat': \n",
    "                return K.concatenate([position_ij, x], 2) \n",
    "\n",
    "    def compute_output_shape(self, input_shape): \n",
    "        if self.mode == 'sum': \n",
    "            return input_shape \n",
    "        elif self.mode == 'concat': \n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(Layer): \n",
    "    def __init__(self, output_dim=30, activation=\"relu\",**kwargs): \n",
    "        self.output_dim = output_dim \n",
    "        self.activate = activations.get(activation)\n",
    "        super(FM, self).__init__(**kwargs) \n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.weight = self.add_weight(name='weight',shape=(input_shape[1], self.output_dim),initializer='glorot_uniform',trainable=True) \n",
    "        self.bias = self.add_weight(name='bias',shape=(self.output_dim,),initializer='zeros',trainable=True) \n",
    "        self.kernel = self.add_weight(name='kernel',shape=(input_shape[1], self.output_dim),initializer='glorot_uniform',trainable=True) \n",
    "        super(FM, self).build(input_shape) \n",
    "        \n",
    "    def call(self, x):\n",
    "        feature = K.dot(x,self.weight) + self.bias\n",
    "        a = K.pow(K.dot(x,self.kernel), 2)\n",
    "        b = K.dot(x, K.pow(self.kernel, 2))\n",
    "        cross = K.mean(a-b, 1, keepdims=True)*0.5\n",
    "        cross = K.repeat_elements(K.reshape(cross, (-1, 1)), self.output_dim, axis=-1) \n",
    "        return self.activate(feature + cross) \n",
    "    \n",
    "    def compute_output_shape(self, input_shape): \n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(inshape, outshape):\n",
    "    # create two models\n",
    "    input1 = Input(shape=inshape)\n",
    "#     lstm_1 = LSTM(256)\n",
    "    dense_1 = Dense(128, kernel_initializer='normal', activation='relu')(input1)\n",
    "#     dense_1 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(64, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "#     dense_2 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(32, kernel_initializer='normal', activation='relu')(dense_2)\n",
    "#     dense_3 = Dropout(0.5)(dense_3)\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(dense_3)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer=\"adadelta\", metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet(inshape, outshape):\n",
    "    # create two models\n",
    "    input1  = Input(shape=inshape)\n",
    "\n",
    "    dense_1 = Dense(256, kernel_initializer='normal', activation='relu')(input1)\n",
    "    #dense_1 = Dropout(0.6)(dense_1)\n",
    "    dense_2 = Dense(128, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "    #dense_2 = Dropout(0.6)(dense_2)\n",
    "    dense_2_x  = concatenate([dense_1,dense_2])\n",
    "    dense_3 = Dense(64, kernel_initializer='normal', activation='relu')(dense_2_x)\n",
    "    #dense_3 = Dropout(0.6)(dense_3)\n",
    "    dense_3_x  = concatenate([dense_1,dense_2,dense_3])\n",
    "    dense_4 = Dense(7, kernel_initializer='normal', activation='relu')(dense_3_x)\n",
    "    #dense_4 = Dropout(0.6)(dense_4)\n",
    "    dense_4_x  = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(dense_4_x)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FMNet(inshape, outshape):\n",
    "    # create two models\n",
    "    input1  = Input(shape=inshape)\n",
    "    #DNN_model_I\n",
    "    dense_1 = Dense(100, kernel_initializer='normal', activation='tanh')(input1)\n",
    "    dense_2 = Dense(150, kernel_initializer='normal', activation='tanh')(dense_1)\n",
    "    dense_3 = Dense(150, kernel_initializer='normal', activation='tanh')(dense_2)\n",
    "    dense_4 = Dense(100, kernel_initializer='normal', activation='tanh')(dense_3)\n",
    "    dense_5 = Dense(64, kernel_initializer='normal', activation='tanh')(dense_4)\n",
    "    #FM_model_II\n",
    "    FM_1    = FM(200)(input1)\n",
    "    FM_2    = FM(64)(FM_1)\n",
    "\n",
    "    x       = concatenate([dense_5,FM_2])\n",
    "    x_tmp   = Dense(32,kernel_initializer='normal', activation='softmax')(x)\n",
    "    out     = Dense(outshape,kernel_initializer='normal', activation='softmax')(x_tmp)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emded_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftAttention(object):\n",
    "    \"\"\"\n",
    "    Layer to compute local inference between two encoded sentences a and b.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        a = inputs[0]\n",
    "        b = inputs[1]\n",
    "\n",
    "        attention = Lambda(self._attention,\n",
    "                                        output_shape = self._attention_output_shape,\n",
    "                                        arguments = None)(inputs)\n",
    "\n",
    "        align_a = Lambda(self._soft_alignment,\n",
    "                                     output_shape = self._soft_alignment_output_shape,\n",
    "                                     arguments = None)([attention, a])\n",
    "        align_b = Lambda(self._soft_alignment,\n",
    "                                     output_shape = self._soft_alignment_output_shape,\n",
    "                                     arguments = None)([attention, b])\n",
    "\n",
    "        return align_a, align_b\n",
    "\n",
    "    def _attention(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute the attention between elements of two sentences with the dot\n",
    "        product.\n",
    "        Args:\n",
    "            inputs: A list containing two elements, one for the first sentence\n",
    "                    and one for the second, both encoded by a BiLSTM.\n",
    "        Returns:\n",
    "            A tensor containing the dot product (attention weights between the\n",
    "            elements of the two sentences).\n",
    "        \"\"\"\n",
    "        attn_weights = K.batch_dot(x=inputs[0],\n",
    "                                   y=K.permute_dimensions(inputs[1],\n",
    "                                                          pattern=(0, 2, 1)))\n",
    "        return K.permute_dimensions(attn_weights, (0, 2, 1))\n",
    "\n",
    "    def _attention_output_shape(self, inputs):\n",
    "        input_shape = inputs[0]\n",
    "        embedding_size = input_shape[1]\n",
    "        return (input_shape[0], embedding_size, embedding_size)\n",
    "\n",
    "    def _soft_alignment(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute the soft alignment between the elements of two sentences.\n",
    "        Args:\n",
    "            inputs: A list of two elements, the first is a tensor of attention\n",
    "                    weights, the second is the encoded sentence on which to\n",
    "                    compute the alignments.\n",
    "        Returns:\n",
    "            A tensor containing the alignments.\n",
    "        \"\"\"\n",
    "        attention = inputs[0]\n",
    "        sentence = inputs[1]\n",
    "\n",
    "        # Subtract the max. from the attention weights to avoid overflows.\n",
    "        exp = K.exp(attention - K.max(attention, axis=-1, keepdims=True))\n",
    "        exp_sum = K.sum(exp, axis=-1, keepdims=True)\n",
    "        softmax = exp / exp_sum\n",
    "\n",
    "        return K.batch_dot(softmax, sentence)\n",
    "\n",
    "    def _soft_alignment_output_shape(self, inputs):\n",
    "        attention_shape = inputs[0]\n",
    "        sentence_shape = inputs[1]\n",
    "        return (attention_shape[0], attention_shape[1], sentence_shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esim():\n",
    "\n",
    "    inp_a = Input(shape=(maxlen,))\n",
    "    inp_b = Input(shape=(maxlen,))\n",
    "    \n",
    "    # embedding layer\n",
    "    embedded_a = Embedding(max_features, embed_size, weights=[emb_matrix_dic[\"creative_id\"]], trainable=False)(inp_a)\n",
    "    embedded_b = Embedding(max_features, embed_size, weights=[emb_matrix_dic[\"ad_id\"]], trainable=False)(inp_b)\n",
    "    \n",
    "    # Encoding Layer\n",
    "    bilstm = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1))\n",
    "    \n",
    "    encoded_a = bilstm(embedded_a)\n",
    "    encoded_b = bilstm(embedded_b)\n",
    "    \n",
    "    # local inference layer \n",
    "    atten_a, atten_b = SoftAttention()([encoded_a, encoded_b])\n",
    "\n",
    "    sub_a_atten = Lambda(lambda x: x[0]-x[1])([encoded_a, atten_a])\n",
    "    sub_b_atten = Lambda(lambda x: x[0]-x[1])([encoded_b, atten_b])\n",
    "\n",
    "    mul_a_atten = Lambda(lambda x: x[0]*x[1])([encoded_a, atten_a])\n",
    "    mul_b_atten = Lambda(lambda x: x[0]*x[1])([encoded_b, atten_b])\n",
    "\n",
    "    m_a = concatenate([encoded_a, atten_a, sub_a_atten, mul_a_atten])\n",
    "    m_b = concatenate([encoded_b, atten_b, sub_b_atten, mul_b_atten])\n",
    "    \n",
    "    # Inference composition layer\n",
    "    composition_a = Bidirectional(LSTM(\n",
    "        128,\n",
    "        return_sequences=True,\n",
    "        dropout=0.1\n",
    "    ))(m_a)\n",
    "\n",
    "    avg_pool_a = GlobalAveragePooling1D()(composition_a)\n",
    "    max_pool_a = GlobalMaxPooling1D()(composition_a)\n",
    "\n",
    "    composition_b = Bidirectional(LSTM(\n",
    "        128,\n",
    "        return_sequences=True,\n",
    "        dropout=0.1\n",
    "    ))(m_b)\n",
    "\n",
    "    avg_pool_b = GlobalAveragePooling1D()(composition_b)\n",
    "    max_pool_b = GlobalMaxPooling1D()(composition_b)\n",
    "\n",
    "    pooled = concatenate([avg_pool_a, max_pool_a, avg_pool_b, max_pool_b])\n",
    "    pooled = Dropout(rate=0.1)(pooled)\n",
    "    \n",
    "    \n",
    "    conc = Dense(256)(pooled)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "\n",
    "    conc = Dense(128)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    # conc = PReLU()(conc)\n",
    "\n",
    "    if num_classes == 1:\n",
    "        outp = Dense(num_classes, activation=\"sigmoid\")(conc)\n",
    "    else:\n",
    "        outp = Dense(num_classes, activation=\"softmax\")(conc)\n",
    "\n",
    "    model = Model(inputs=[inp_a, inp_b], outputs=outp)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_transformer():\n",
    "    embed_dim = 256  # Embedding size for each token\n",
    "    num_heads = 6  # Number of attention heads\n",
    "    ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fname = sorted(os.listdir(UFEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()\n",
    "train_feat[UID] = train_user[UID]\n",
    "test_feat[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.set_index(UID, inplace=True)\n",
    "test_feat.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_IDSET = ID_SET[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "train_ftset_w2v = dict()\n",
    "test_ftset_w2v = dict()\n",
    "for cur_id in USE_IDSET:\n",
    "    fname = f\"w2v_avg_{cur_id}.pkl\"\n",
    "    print(\"current filename: \", fname)\n",
    "    cur_df = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "    train_ftset_w2v[cur_id] = pd.merge(train_feat, cur_df, how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "    test_ftset_w2v[cur_id] = pd.merge(test_feat, cur_df, how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_id in USE_IDSET:\n",
    "    assert len(train_ftset_w2v[cur_id]) == len(train_feat)\n",
    "    assert len(test_ftset_w2v[cur_id]) == len(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec\n",
    "train_ftset_d2v = dict()\n",
    "test_ftset_d2v = dict()\n",
    "for cur_id in USE_IDSET:\n",
    "    fname = f\"d2v_avg_{cur_id}.pkl\"\n",
    "    print(\"current filename: \", fname)\n",
    "    cur_df = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "    train_ftset_d2v[cur_id] = pd.merge(train_feat, cur_df, how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "    test_ftset_d2v[cur_id] = pd.merge(test_feat, cur_df, how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_id in USE_IDSET:\n",
    "    assert len(train_ftset_d2v[cur_id]) == len(train_feat)\n",
    "    assert len(test_ftset_d2v[cur_id]) == len(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove\n",
    "train_ftset_glv = dict()\n",
    "test_ftset_glv = dict()\n",
    "for cur_id in USE_IDSET:\n",
    "    fname = f\"glove_avg_{cur_id}.pkl\"\n",
    "    print(\"current filename: \", fname)\n",
    "    cur_df = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "    train_ftset_glv[cur_id] = pd.merge(train_feat, cur_df, how=\"left\", on=UID).values.reshape(-1, 1, 300)\n",
    "    test_ftset_glv[cur_id] = pd.merge(test_feat, cur_df, how=\"left\", on=UID).values.reshape(-1, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_id in USE_IDSET:\n",
    "    assert len(train_ftset_glv[cur_id]) == len(train_feat)\n",
    "    assert len(test_ftset_glv[cur_id]) == len(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf svd\n",
    "train_ftset_svd = dict()\n",
    "test_ftset_svd = dict()\n",
    "for cur_id in USE_IDSET:\n",
    "    fname = f\"tfidf_svd_{cur_id}.pkl\"\n",
    "    print(\"current filename: \", fname)\n",
    "    cur_df = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "    train_ftset_svd[cur_id] = pd.merge(train_feat, cur_df, how=\"left\", on=UID).values.reshape(-1, 1, 64)\n",
    "    test_ftset_svd[cur_id] = pd.merge(test_feat, cur_df, how=\"left\", on=UID).values.reshape(-1, 1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_id in USE_IDSET:\n",
    "    assert len(train_ftset_svd[cur_id]) == len(train_feat)\n",
    "    assert len(test_ftset_svd[cur_id]) == len(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pemb = pd.DataFrame()\n",
    "test_pemb = pd.DataFrame()\n",
    "train_pemb[UID] = train_user[UID]\n",
    "test_pemb[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"meta_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        gender_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_pemb = pd.merge(train_pemb, gender_agg_pred, how=\"left\", on=UID)\n",
    "        test_pemb = pd.merge(test_pemb, gender_agg_pred, how=\"left\", on=UID)\n",
    "        gender_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"tfidf_count_emb_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_tce = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_pemb = pd.merge(train_pemb, cur_tce, how=\"left\", on=UID)\n",
    "        test_pemb = pd.merge(test_pemb, cur_tce, how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats cate target encode\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_catemlb\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_pemb = pd.merge(train_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_catemlb\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_pemb = pd.merge(test_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats o1\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_pemb = pd.merge(train_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_pemb = pd.merge(test_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats getidxmax\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_o2_getidxmax\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_pemb = pd.merge(train_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_o2_getidxmax\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_pemb = pd.merge(test_pemb, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"tfidf_svd_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_tfidf_svd = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_pemb = pd.merge(train_pemb, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "        test_pemb = pd.merge(test_pemb, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "        cur_tfidf_svd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pemb.set_index(UID, inplace=True)\n",
    "test_pemb.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train_pemb.columns == train_pemb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna and inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnaninf(val):\n",
    "    return np.mean(val[(~np.isnan(val))&(~np.isinf(val))].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillmean(df):\n",
    "    for col in tqdm(df.columns):\n",
    "        if df[col].count() < len(df):\n",
    "            df[col] = df[col].replace([np.nan, np.inf], nnaninf(df[col]))\n",
    "    # check\n",
    "    for col in tqdm(df.columns):\n",
    "        if df[col].count() < len(df):\n",
    "            log(col)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pemb = fillmean(train_pemb)\n",
    "test_pemb = fillmean(test_pemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_pemb.values.max(), train_pemb.values.min())\n",
    "print(test_pemb.values.max(), test_pemb.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transmms(tr_df, te_df):\n",
    "    tol_df = pd.concat([tr_df, te_df])\n",
    "    mms = MinMaxScaler()\n",
    "    mms_tol_df = mms.fit_transform(tol_df)\n",
    "    mms_tr_df = mms_tol_df[:len(tr_df)]\n",
    "    mms_te_df = mms_tol_df[len(tr_df):]\n",
    "    \n",
    "    return mms_tr_df, mms_te_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms_tr_df, mms_te_df = transmms(train_pemb, test_pemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mms_tr_df.max(), mms_tr_df.min())\n",
    "print(mms_te_df.max(), mms_te_df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms_tr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms_te_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadnprec(col, max_feature=None, maxlen=None, train_order=train_user[UID].values, test_order=test_user[UID].values):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    \n",
    "    train_seq[col] = train_seq[col].apply(lambda x: \" \".join(x))\n",
    "    test_seq[col] = test_seq[col].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    train_seq.sort_index(inplace=True)\n",
    "    test_seq.sort_index(inplace=True)\n",
    "    \n",
    "    assert (train_seq.index.values != train_order).sum() == 0\n",
    "    assert (test_seq.index.values != test_order).sum() == 0\n",
    "    \n",
    "    train_X = train_seq[col].values\n",
    "    test_X = test_seq[col].values\n",
    "    \n",
    "    tol_X = np.concatenate([train_X, test_X])\n",
    "    \n",
    "    # Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features, filters=\"\")\n",
    "    tokenizer.fit_on_texts(list(tol_X))\n",
    "    \n",
    "    tol_X = tokenizer.texts_to_sequences(tol_X)\n",
    "\n",
    "    # Pad the sentences \n",
    "    tol_X = pad_sequences(tol_X, maxlen=maxlen)\n",
    "    \n",
    "    train_X = tol_X[:len(train_seq)]\n",
    "    test_X = tol_X[len(train_seq):]\n",
    "    \n",
    "    return train_X, test_X, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(col, word_index, max_features):\n",
    "    \n",
    "    EMBEDDING_FILE = (f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "    k = Glove.load(EMBEDDING_FILE)\n",
    "    \n",
    "    embeddings_index = []\n",
    "    for i in tqdm(k.dictionary.keys()):\n",
    "        embeddings_index.append((i,k.word_vectors[k.dictionary[i]]))\n",
    "\n",
    "    embeddings_index = dict(pd.DataFrame(embeddings_index).values)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v(col, word_index, max_features):    \n",
    "    EMBEDDING_FILE = (f\"{UMDIR}/vectors/w2v_{col}.model\")\n",
    "    embeddings_index = dict()\n",
    "    model = gensim.models.Word2Vec.load(EMBEDDING_FILE)\n",
    "    for word in model.wv.index2word:\n",
    "        embeddings_index[word] = model.wv[word]\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadnoh(col, max_feature=None, maxlen=None, train_order=train_user[UID].values, test_order=test_user[UID].values):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    \n",
    "    train_seq[col] = train_seq[col].apply(lambda x: \" \".join(x))\n",
    "    test_seq[col] = test_seq[col].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    train_seq.sort_index(inplace=True)\n",
    "    test_seq.sort_index(inplace=True)\n",
    "    \n",
    "    assert (train_seq.index.values != train_order).sum() == 0\n",
    "    assert (test_seq.index.values != test_order).sum() == 0\n",
    "    \n",
    "    train_X = train_seq[col].values\n",
    "    test_X = test_seq[col].values\n",
    "    \n",
    "    tol_X = np.concatenate([train_X, test_X])\n",
    "    \n",
    "    tol_X = [one_hot(d, n=max_feature) for d in tol_X]\n",
    "\n",
    "    # Pad the sentences \n",
    "    tol_X = pad_sequences(tol_X, maxlen=maxlen, padding='post')\n",
    "    \n",
    "    train_X = tol_X[:len(train_seq)]\n",
    "    test_X = tol_X[len(train_seq):]\n",
    "    \n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 32\n",
    "embed_size = 300\n",
    "max_features = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_IDSET = [\"creative_id\", \"ad_id\", \"advertiser_id\",]  #  \"product_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_textprec_dic = dict()\n",
    "test_textprec_dic = dict()\n",
    "w2v_wordidx_dic = dict()\n",
    "emb_matrix_dic = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cur_id in tqdm(USE_IDSET):\n",
    "#     train_textprec_dic[cur_id],  test_textprec_dic[cur_id] = loadnoh(cur_id, max_features, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3331: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      " 33%|███▎      | 1/3 [04:13<08:26, 253.10s/it]/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3331: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      " 67%|██████▋   | 2/3 [08:19<04:11, 251.06s/it]/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3331: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "100%|██████████| 3/3 [10:36<00:00, 212.01s/it]\n"
     ]
    }
   ],
   "source": [
    "for cur_id in tqdm(USE_IDSET):\n",
    "    train_textprec_dic[cur_id], test_textprec_dic[cur_id], w2v_wordidx_dic[cur_id] = loadnprec(cur_id, max_features, maxlen)\n",
    "    emb_matrix_dic[cur_id] = load_w2v(cur_id, w2v_wordidx_dic[cur_id], max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_wordidx_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_matrix_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{UDDIR}/imd/train_textprec_dic.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(train_textprec_dic), f)\n",
    "\n",
    "# with open(f\"{UDDIR}/imd/test_textprec_dic.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(test_textprec_dic), f)\n",
    "\n",
    "# with open(f\"{UDDIR}/imd/w2v_wordidx_dic.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(w2v_wordidx_dic), f)\n",
    "\n",
    "# with open(f\"{UDDIR}/imd/emb_matrix_dic.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(emb_matrix_dic), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{UDDIR}/imd/train_textprec_dic.pkl\", \"rb\") as f:\n",
    "#     train_textprec_dic = pickle.loads(pickle.load(f))\n",
    "    \n",
    "# with open(f\"{UDDIR}/imd/test_textprec_dic.pkl\", \"rb\") as f:\n",
    "#     test_textprec_dic = pickle.loads(pickle.load(f))\n",
    "\n",
    "# with open(f\"{UDDIR}/imd/w2v_wordidx_dic.pkl\", \"rb\") as f:\n",
    "#     w2v_wordidx_dic = pickle.loads(pickle.load(f))\n",
    "\n",
    "# with open(f\"{UDDIR}/imd/emb_matrix_dic.pkl\", \"rb\") as f:\n",
    "#     emb_matrix_dic = pickle.loads(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in emb_matrix_dic:\n",
    "    print(emb_matrix_dic[col].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_textprec_dic[\"creative_id\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix_dic[\"creative_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_lstm_atten(inp, maxlen, max_features, embed_size, embedding_matrix):\n",
    "    # Stage ctid\n",
    "    x = Embedding(len(embedding_matrix), embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "#     x = Position_Embedding()(x)\n",
    "#     x = SpatialDropout1D(0.1)(x)\n",
    "    y = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "#     y = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "#     z = Conv1D(64, kernel_size = 1, kernel_initializer=initializers.he_uniform(seed=2020), activation = \"tanh\")(y)\n",
    "\n",
    "#     atten_1 = Attention(maxlen)(x) # skip connect\n",
    "#     atten_2 = Attention(maxlen)(y)\n",
    "    avg_pool_1 = GlobalAveragePooling1D()(y)\n",
    "    max_pool_1 = GlobalMaxPooling1D()(y)\n",
    "#     max_pool1_1 = GlobalMaxPooling1D()(z)\n",
    "\n",
    "#     convs = []\n",
    "#     filter_sizes = [2, 3, 5, 8]\n",
    "#     for fsz in filter_sizes:\n",
    "#         l_conv = Conv1D(filters=maxlen, kernel_size=fsz, activation='relu')(y)\n",
    "#         l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "#         l_pool = Flatten()(l_pool)\n",
    "#         convs.append(l_pool)\n",
    "#     text_cnn = concatenate(convs, axis=1)\n",
    "\n",
    "    return avg_pool_1, max_pool_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_lstm_atten(maxlen, num_classes, max_features, embed_size, embm_dic):\n",
    "    \n",
    "    w2v_inp = dict()\n",
    "    \n",
    "    for cur_id in USE_IDSET:\n",
    "        w2v_inp[cur_id] = Input(shape=(maxlen, ))\n",
    "\n",
    "    w2v_nn = dict()\n",
    "        \n",
    "    for cur_id in USE_IDSET:\n",
    "        w2v_nn[cur_id] = list(cp_lstm_atten(w2v_inp[cur_id], maxlen, max_features, embed_size, embm_dic[cur_id]))\n",
    "    \n",
    "    inps = list(w2v_inp.values())\n",
    "    nnls = list(np.array(list(w2v_nn.values())).flatten())\n",
    "    \n",
    "    # Stage prediction embedding\n",
    "    inp_pemb = Input(shape=(mms_tr_df.shape[1], ))\n",
    "    aux_1 = Dense(32)(inp_pemb)\n",
    "#     aux_1 = Dense(32)(aux_1)\n",
    "#     aux_1 = BatchNormalization()(aux_1)\n",
    "\n",
    "#     all_nnls = nnls\n",
    "#     all_inps = inps\n",
    "\n",
    "    conc = concatenate(nnls+[aux_1])\n",
    "    conc = Dense(256)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(0.2)(conc)\n",
    "\n",
    "    conc = Dense(128)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    # conc = PReLU()(conc)\n",
    "\n",
    "    if num_classes == 1:\n",
    "        outp = Dense(num_classes, activation=\"sigmoid\")(conc)\n",
    "    else:\n",
    "        outp = Dense(num_classes, activation=\"softmax\")(conc)\n",
    "\n",
    "    model = Model(inputs=inps+[inp_pemb], outputs=outp)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]\n",
    "\n",
    "# for age\n",
    "num_classes = 10\n",
    "train_y = np_utils.to_categorical(train_user[\"age\"]-1)\n",
    "\n",
    "\n",
    "train_set =  list(train_textprec_dic.values())   # + [mms_tr_df, ]\n",
    "test_set = list(test_textprec_dic.values())   # + [mms_te_df, ]\n",
    "\n",
    "# train_set = list(train_textprec_dic.values())[:2]\n",
    "# test_set = list(test_textprec_dic.values())[:2]\n",
    "\n",
    "predicted_train_age = np.zeros(train_y.shape)\n",
    "predicted_age = np.zeros((len(test_user),num_classes))\n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_set[0], train_y))\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    if idx < 1:\n",
    "        K.clear_session()\n",
    "\n",
    "        X_train_set = list()\n",
    "        X_val_set = list()\n",
    "\n",
    "        for cur_train in train_set:\n",
    "            X_train_set.append(cur_train[train_idx])\n",
    "            X_val_set.append(cur_train[valid_idx])\n",
    "\n",
    "        y_train = train_y[train_idx]\n",
    "        y_val = train_y[valid_idx]\n",
    "\n",
    "\n",
    "        model = sim_lstm_atten(maxlen, num_classes, max_features, embed_size, emb_matrix_dic)\n",
    "#         model = esim()\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])\n",
    "        print(\"MODEL COMPLIE...\")\n",
    "        model.summary()\n",
    "\n",
    "        model.fit(X_train_set, y_train, batch_size=2048, epochs=50, validation_data=(X_val_set, y_val), callbacks=cb, verbose=1)\n",
    "        pred_val_y = model.predict(X_val_set, batch_size=2048, verbose=1, workers=20)\n",
    "        pred_test_y = model.predict(test_set, batch_size=2048, verbose=1, workers=20)\n",
    "        predicted_train_age[valid_idx] = pred_val_y\n",
    "        predicted_age += pred_test_y / len(splits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.argmax(predicted_train_age,axis=1)+1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print(accuracy_score(np.argmax(train_y[valid_idx],axis=1),np.argmax(predicted_train_age[valid_idx],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=32\n",
    "\n",
    "MODEL COMPLIE...\n",
    "Model: \"model_1\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_1 (InputLayer)            (None, 32)           0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_2 (InputLayer)            (None, 32)           0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_3 (InputLayer)            (None, 32)           0                                            \n",
    "__________________________________________________________________________________________________\n",
    "embedding_1 (Embedding)         (None, 32, 300)      15000000    input_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "embedding_2 (Embedding)         (None, 32, 300)      15000000    input_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "embedding_3 (Embedding)         (None, 32, 300)      15000000    input_3[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_1 (Bidirectional) (None, 32, 256)      439296      embedding_1[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_2 (Bidirectional) (None, 32, 256)      439296      embedding_2[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_3 (Bidirectional) (None, 32, 256)      439296      embedding_3[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_1[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_1[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_average_pooling1d_2 (Glo (None, 256)          0           bidirectional_2[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_2 (GlobalM (None, 256)          0           bidirectional_2[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_average_pooling1d_3 (Glo (None, 256)          0           bidirectional_3[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_3 (GlobalM (None, 256)          0           bidirectional_3[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "concatenate_1 (Concatenate)     (None, 1536)         0           global_average_pooling1d_1[0][0] \n",
    "                                                                 global_max_pooling1d_1[0][0]     \n",
    "                                                                 global_average_pooling1d_2[0][0] \n",
    "                                                                 global_max_pooling1d_2[0][0]     \n",
    "                                                                 global_average_pooling1d_3[0][0] \n",
    "                                                                 global_max_pooling1d_3[0][0]     \n",
    "__________________________________________________________________________________________________\n",
    "dense_1 (Dense)                 (None, 256)          393472      concatenate_1[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
    "__________________________________________________________________________________________________\n",
    "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "dense_3 (Dense)                 (None, 10)           1290        batch_normalization_2[0][0]      \n",
    "==================================================================================================\n",
    "Total params: 46,747,338\n",
    "Trainable params: 1,746,570\n",
    "Non-trainable params: 45,000,768\n",
    "__________________________________________________________________________________________________\n",
    "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
    "\n",
    "Train on 720000 samples, validate on 180000 samples\n",
    "Epoch 1/50\n",
    "720000/720000 [==============================] - 348s 483us/step - loss: 1.7897 - acc: 0.3306 - val_loss: 1.5208 - val_acc: 0.3814\n",
    "Epoch 2/50\n",
    "720000/720000 [==============================] - 344s 478us/step - loss: 1.4606 - acc: 0.4042 - val_loss: 1.4580 - val_acc: 0.4038\n",
    "Epoch 3/50\n",
    "720000/720000 [==============================] - 344s 478us/step - loss: 1.4093 - acc: 0.4189 - val_loss: 1.4105 - val_acc: 0.4233\n",
    "Epoch 4/50\n",
    "720000/720000 [==============================] - 344s 477us/step - loss: 1.3683 - acc: 0.4349 - val_loss: 1.4058 - val_acc: 0.4198\n",
    "Epoch 5/50\n",
    "720000/720000 [==============================] - 426s 592us/step - loss: 1.3509 - acc: 0.4417 - val_loss: 1.3754 - val_acc: 0.4296\n",
    "Epoch 6/50\n",
    "720000/720000 [==============================] - 520s 722us/step - loss: 1.3230 - acc: 0.4527 - val_loss: 1.4390 - val_acc: 0.4139\n",
    "Epoch 7/50\n",
    "720000/720000 [==============================] - 381s 530us/step - loss: 1.3041 - acc: 0.4606 - val_loss: 1.3911 - val_acc: 0.4310\n",
    "Epoch 8/50\n",
    "720000/720000 [==============================] - 429s 596us/step - loss: 1.2869 - acc: 0.4668 - val_loss: 1.4427 - val_acc: 0.4267\n",
    "180000/180000 [==============================] - 50s 280us/step\n",
    "1000000/1000000 [==============================] - 199s 199us/step\n",
    "                       \n",
    "                       \n",
    "0.42667777777777777\n",
    "0.038933333333333334\n",
    "0.03905\n",
    "0.03961666666666667\n",
    "0.039127777777777775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT_SEED = 2020\n",
    "cb = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "      CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)]\n",
    "\n",
    "# for gender\n",
    "num_classes = 1\n",
    "train_y = (train_user[\"gender\"]-1).values.reshape(-1, 1)\n",
    "\n",
    "train_set =  list(train_textprec_dic.values()) + [mms_tr_df, ]\n",
    "test_set = list(test_textprec_dic.values()) + [mms_te_df, ]\n",
    "\n",
    "\n",
    "predicted_train_gender = np.zeros(train_y.shape)\n",
    "predicted_gender = np.zeros((len(test_user),num_classes))\n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_set[0], train_y))\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    if idx < 1:\n",
    "        K.clear_session()\n",
    "\n",
    "        X_train_set = list()\n",
    "        X_val_set = list()\n",
    "\n",
    "        for cur_train in train_set:\n",
    "            X_train_set.append(cur_train[train_idx])\n",
    "            X_val_set.append(cur_train[valid_idx])\n",
    "\n",
    "        y_train = train_y[train_idx]\n",
    "        y_val = train_y[valid_idx]\n",
    "\n",
    "        model = sim_lstm_atten(maxlen, num_classes, max_features, embed_size, emb_matrix_dic)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=RAdam(lr=0.001), metrics=['acc'])\n",
    "        print(\"MODEL COMPLIE...\")\n",
    "        model.summary()\n",
    "\n",
    "        model.fit(X_train_set, y_train, batch_size=2048, epochs=50, validation_data=(X_val_set, y_val), callbacks=cb, verbose=1)\n",
    "        pred_val_y = model.predict(X_val_set, batch_size=2048, verbose=1)\n",
    "        pred_test_y = model.predict(test_set, batch_size=2048, verbose=1)\n",
    "        predicted_train_gender[valid_idx] = pred_val_y\n",
    "        predicted_gender += pred_test_y / len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print(accuracy_score((train_y[valid_idx] >= 0.5).astype(int),(predicted_train_gender[valid_idx] >= 0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.42667777777777777 + 0.9337722222222222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL COMPLIE...\n",
    "Model: \"model_1\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_1 (InputLayer)            (None, 32)           0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_2 (InputLayer)            (None, 32)           0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_3 (InputLayer)            (None, 32)           0                                            \n",
    "__________________________________________________________________________________________________\n",
    "embedding_1 (Embedding)         (None, 32, 300)      15000000    input_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "embedding_2 (Embedding)         (None, 32, 300)      15000000    input_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "embedding_3 (Embedding)         (None, 32, 300)      15000000    input_3[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_1 (Bidirectional) (None, 32, 256)      439296      embedding_1[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_2 (Bidirectional) (None, 32, 256)      439296      embedding_2[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_3 (Bidirectional) (None, 32, 256)      439296      embedding_3[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_1[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_1[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_average_pooling1d_2 (Glo (None, 256)          0           bidirectional_2[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_2 (GlobalM (None, 256)          0           bidirectional_2[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_average_pooling1d_3 (Glo (None, 256)          0           bidirectional_3[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_3 (GlobalM (None, 256)          0           bidirectional_3[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "concatenate_1 (Concatenate)     (None, 1536)         0           global_average_pooling1d_1[0][0] \n",
    "                                                                 global_max_pooling1d_1[0][0]     \n",
    "                                                                 global_average_pooling1d_2[0][0] \n",
    "                                                                 global_max_pooling1d_2[0][0]     \n",
    "                                                                 global_average_pooling1d_3[0][0] \n",
    "                                                                 global_max_pooling1d_3[0][0]     \n",
    "__________________________________________________________________________________________________\n",
    "dense_1 (Dense)                 (None, 256)          393472      concatenate_1[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n",
    "__________________________________________________________________________________________________\n",
    "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "dense_3 (Dense)                 (None, 1)            129         batch_normalization_2[0][0]      \n",
    "==================================================================================================\n",
    "Total params: 46,746,177\n",
    "Trainable params: 1,745,409\n",
    "Non-trainable params: 45,000,768\n",
    "__________________________________________________________________________________________________\n",
    "Train on 720000 samples, validate on 180000 samples\n",
    "Epoch 1/50\n",
    "720000/720000 [==============================] - 347s 481us/step - loss: 0.2723 - acc: 0.8965 - val_loss: 0.2192 - val_acc: 0.9195\n",
    "Epoch 2/50\n",
    "720000/720000 [==============================] - 346s 481us/step - loss: 0.1948 - acc: 0.9292 - val_loss: 0.1824 - val_acc: 0.9338\n",
    "Epoch 3/50\n",
    "720000/720000 [==============================] - 357s 496us/step - loss: 0.1809 - acc: 0.9341 - val_loss: 0.1829 - val_acc: 0.9337\n",
    "Epoch 4/50\n",
    "720000/720000 [==============================] - 368s 510us/step - loss: 0.1726 - acc: 0.9376 - val_loss: 0.1775 - val_acc: 0.9360\n",
    "Epoch 5/50\n",
    "720000/720000 [==============================] - 349s 485us/step - loss: 0.1692 - acc: 0.9387 - val_loss: 0.1813 - val_acc: 0.9344\n",
    "Epoch 6/50\n",
    "720000/720000 [==============================] - 490s 681us/step - loss: 0.1621 - acc: 0.9415 - val_loss: 0.2092 - val_acc: 0.9275\n",
    "Epoch 7/50\n",
    "720000/720000 [==============================] - 420s 583us/step - loss: 0.1562 - acc: 0.9435 - val_loss: 0.1835 - val_acc: 0.9338\n",
    "180000/180000 [==============================] - 35s 195us/step\n",
    "1000000/1000000 [==============================] - 192s 192us/step\n",
    "                       \n",
    "0.9337722222222222\n",
    "0.6710166666666667\n",
    "0.6699944444444444\n",
    "0.6692166666666667\n",
    "0.6689833333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb_multi_age = lgb.Booster(model_file=f\"{UMDIR}/lgb_multi_age_20200511045531.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb_multi_gender = lgb.Booster(model_file=f\"{UMDIR}/lgb_multi_gender_20200511034408.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_user[[UID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res[\"predicted_age\"] = np.argmax(predicted_age, axis=1)+1\n",
    "# res[\"predicted_age\"] = predicted_age\n",
    "# res[\"predicted_age\"] = [list(x).index(max(x))+1 for x in model_lgb_multi_age.predict(test_feat, num_iteration=model_lgb_multi_age.best_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"] = (predicted_gender >= 0.1).astype(int)+1\n",
    "# res[\"predicted_gender\"] = [list(x).index(max(x))+1 for x in model_lgb_multi_gender.predict(test_feat, num_iteration=model_lgb_multi_gender.best_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_suffix = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))\n",
    "res.to_csv(f\"{RESDIR}/res-{res_suffix}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(f\"{RESDIR}/res-20200605094137.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"] = tmp[\"predicted_gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"predicted_age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cent result to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ti import session\n",
    "ti_session = session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ti_session.upload_data(path=f\"{RESDIR}/res-20200515004850.csv\", bucket=\"etveritas-1252104022\", key_prefix=RESDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "767px",
    "left": "28px",
    "top": "132px",
    "width": "355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
