{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if used pip install package\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install wget\n",
    "# !pip install gensim\n",
    "# !pip install catboost\n",
    "# !pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import wget\n",
    "import time\n",
    "import tarfile\n",
    "import zipfile\n",
    "import functools\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cbt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "\n",
    "import gensim\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, filename='default.log', stream=sys.stdout):\n",
    "        self.terminal = stream\n",
    "        self.log = open(filename, 'a')\n",
    "        \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        \n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.stdout = Logger(\"logs/default.log\", sys.stdout)\n",
    "# sys.stderr = Logger(\"logs/default_err.log\", sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDIR = \"data\"\n",
    "UDDIR = \"user_data\"\n",
    "UFEDIR = \"user_data/feat_data_v05\"\n",
    "UMDIR = \"user_data/model_data\"\n",
    "RESDIR = \"prediction_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "UID = \"user_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (Only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/train_preliminary.zip\", out=DDIR)\n",
    "# test_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/test.zip\", out=DDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myunzip(filename):\n",
    "#     zFile = zipfile.ZipFile(filename, \"r\")\n",
    "#     for fileM in zFile.namelist(): \n",
    "#         zFile.extract(fileM, DDIR)\n",
    "#         print(fileM)\n",
    "#     zFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myunzip(train_fname)\n",
    "# myunzip(test_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bch_rencol(values, prefix=\"\", suffix=\"\"):\n",
    "    return list(map(lambda x: f\"{prefix}\"+\"_\".join(list(map(lambda y: str(y), x)))+f\"{suffix}\" \n",
    "                    if hasattr(x, \"__iter__\") and not isinstance(x, str) \n",
    "                    else f\"{prefix}\"+str(x)+f\"{suffix}\", values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynunique(values):\n",
    "    return values.nunique(dropna=False)\n",
    "def getidxmax(x):\n",
    "    return x.idxmax()[1]\n",
    "# for time series\n",
    "def at_len(x):\n",
    "    return len(x)\n",
    "\n",
    "def at_sum(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def at_max(x):\n",
    "    return np.max(x)\n",
    "\n",
    "def at_min(x):\n",
    "    return np.min(x)\n",
    "\n",
    "def at_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def at_range(x):\n",
    "    return at_max(x) - at_min(x)\n",
    "\n",
    "def at_nunq(x):\n",
    "    return len(set(x))\n",
    "\n",
    "def at_lenDrange(x):\n",
    "    return at_len(x)/(at_range(x)+1)\n",
    "\n",
    "def at_lenDnunq(x):\n",
    "    return at_len(x)/at_nunq(x)\n",
    "\n",
    "def at_percentile(n):\n",
    "    def at_percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    at_percentile_.__name__ = f\"at_percentile_{n}\"\n",
    "    return at_percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_SET = [\"sum\", \"max\", \"min\", \"mean\", \"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting_level = 0\n",
    "is_start = None\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        self.history = [self.start]\n",
    "\n",
    "    def check(self, info):\n",
    "        current = time.time()\n",
    "        print(f\"[{info}] spend {current - self.history[-1]:0.2f} sec\")\n",
    "        self.history.append(current)\n",
    "\n",
    "def log(entry):\n",
    "    global nesting_level\n",
    "    space = \"-\" * (4 * nesting_level)\n",
    "    print(f\"{space}{entry}\")\n",
    "\n",
    "def timeit(method, start_log=None):\n",
    "    @functools.wraps(method)\n",
    "    def timed(*args, **kw):\n",
    "        global is_start\n",
    "        global nesting_level\n",
    "\n",
    "        if not is_start:\n",
    "            print()\n",
    "\n",
    "        is_start = True\n",
    "        log(f\"Start [{method.__name__}]:\" + (start_log if start_log else \"\"))\n",
    "        log(f'Start time: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        nesting_level += 1\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        end_time = time.time()\n",
    "\n",
    "        nesting_level -= 1\n",
    "        log(f\"End   [{method.__name__}]. Time elapsed: {end_time - start_time:0.2f} sec.\")\n",
    "        is_start = False\n",
    "\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration (todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train data\n",
    "train_click_log = pd.read_csv(f\"{DDIR}/train_preliminary/click_log.csv\")\n",
    "train_ad = pd.read_csv(f\"{DDIR}/train_preliminary/ad.csv\")\n",
    "# tag\n",
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "test_click_log = pd.read_csv(f\"{DDIR}/test/click_log.csv\")\n",
    "test_ad = pd.read_csv(f\"{DDIR}/test/ad.csv\")\n",
    "# pd.DataFrame(np.sort(test_click_log[UID].unique()), columns=[UID]).to_csv(f\"{DDIR}/test/user.csv\", index=False)\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_click_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_click_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ad[\"product_id\"] = train_ad[\"product_id\"].replace(\"\\\\N\", -1).astype(int)\n",
    "train_ad[\"industry\"] = train_ad[\"industry\"].replace(\"\\\\N\", -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ad[\"product_id\"] = test_ad[\"product_id\"].replace(\"\\\\N\", -1).astype(int)\n",
    "test_ad[\"industry\"] = test_ad[\"industry\"].replace(\"\\\\N\", -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creative id in train (creative id is unique in train_ad)\n",
    "len(train_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creative id in test (creative id is unique in test_ad)\n",
    "len(test_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the same creative_id in train and test have same ad info\n",
    "insect1d = np.intersect1d(train_click_log.creative_id.unique(), test_click_log.creative_id.unique())\n",
    "print(\"Same creative id: \", insect1d.shape)\n",
    "print(\"Diff number: \", np.sum(train_ad[train_ad.creative_id.isin(insect1d)].values != test_ad[test_ad.creative_id.isin(insect1d)].values))\n",
    "# checked: they all have same ad info (result is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether click and ad have diff creative_id\n",
    "print(\"Diff list: \", np.setdiff1d(train_click_log.creative_id.unique(), train_ad.creative_id))\n",
    "print(\"Diff list: \", np.setdiff1d(train_ad.creative_id, train_click_log.creative_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether click and ad have diff creative_id\n",
    "print(\"Diff list: \", np.setdiff1d(test_click_log.creative_id.unique(), test_ad.creative_id))\n",
    "print(\"Diff list: \", np.setdiff1d(test_ad.creative_id, test_click_log.creative_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click time\n",
    "sns.lineplot(x=train_click_log.time.value_counts().index, y=train_click_log.time.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=test_click_log.time.value_counts().index, y=test_click_log.time.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_grouped = data.groupby(data.index)\n",
    "# results = Parallel(n_jobs=8)(delayed(key_func)(group) for name, group in data_grouped)\n",
    "# data = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ID sequence (Only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort by time, for time series\n",
    "# train_click_log.sort_values(by=\"time\", inplace=True)\n",
    "# test_click_log.sort_values(by=\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tol_train = pd.merge(train_click_log, train_ad, how=\"left\", on=\"creative_id\")\n",
    "# tol_test = pd.merge(test_click_log, test_ad, how=\"left\", on=\"creative_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_click_log, test_click_log\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeit\n",
    "# def gen_id_series(data, dtyp=\"train\"):\n",
    "#     for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "#         tmp = data.groupby([UID], sort=False)[[col]].agg(lambda x: [f\"word_{y}\" for y in x])\n",
    "#         tmp.columns = bch_rencol(tmp.columns)\n",
    "#         tmp.to_pickle(f\"{UDDIR}/imd/{dtyp}_{col}_seq.pkl\")\n",
    "#         tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_id_series(tol_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_id_series(tol_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_tfidf(col, nr_max=1, only_get_index=True):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    tol_seq[col] = tol_seq[col].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    if not only_get_index:\n",
    "        tfidf_enc = TfidfVectorizer(ngram_range=(1, nr_max))\n",
    "        tfidf_vec = tfidf_enc.fit_transform(tol_seq[col].values)\n",
    "        log(f\"TF-IDF shape: {tfidf_vec.shape}\")\n",
    "\n",
    "        # save sparse matrix\n",
    "        scipy.sparse.save_npz(f\"{UDDIR}/imd/sparse_{col}.npz\", tfidf_vec)\n",
    "    \n",
    "    return tol_seq.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_svd(col, index, prefix=\"tfidf\", n_cpt=64):\n",
    "    tfidf_vec = scipy.sparse.load_npz(f\"{UDDIR}/imd/sparse_{col}.npz\")\n",
    "    if tfidf_vec.shape[1] > n_cpt:\n",
    "        svd_enc = TruncatedSVD(n_components=n_cpt, n_iter=20, random_state=2020)\n",
    "        mode_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    else:\n",
    "        n_cpt = tfidf_vec.shape[1]\n",
    "        mode_svd = tfidf_vec.todense()\n",
    "    mode_svd = pd.DataFrame(mode_svd)\n",
    "    mode_svd.columns = [f\"{prefix}_svd_{col}_{i}\" for i in range(n_cpt)]\n",
    "    mode_svd.index = index\n",
    "    # save svd pkl\n",
    "    mode_svd.to_pickle(f\"{UFEDIR}/{prefix}_svd_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_idx_dic = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    tol_idx_dic[col] = copy.deepcopy(gen_tfidf(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    gen_svd(col, tol_idx_dic[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = None\n",
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    tfidf_vec = scipy.sparse.load_npz(f\"{UDDIR}/imd/sparse_{col}.npz\")\n",
    "    sparse_matrix = scipy.sparse.csr_matrix(scipy.sparse.hstack([sparse_matrix, tfidf_vec]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_w2v(col, vesize=200, win=20):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    if os.path.exists(f\"{UMDIR}/vectors/w2v_{col}.model\"):\n",
    "        model = gensim.models.Word2Vec.load(f\"{UMDIR}/vectors/w2v_{col}.model\")\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec(sentences=tol_seq[col], size=vesize, window=win, workers=32, sg=0, iter=10)\n",
    "        model.save(f\"{UMDIR}/vectors/w2v_{col}.model\")\n",
    "    \n",
    "    w2v_list = list()\n",
    "    \n",
    "    for it in tqdm(tol_seq[col]):\n",
    "        tmp = np.zeros(vesize)\n",
    "        cnt = 0\n",
    "        for wd in it:\n",
    "            cnt += 1\n",
    "            if wd in model:\n",
    "                tmp += model[wd]\n",
    "        w2v_list.append(list(tmp/cnt))\n",
    "    \n",
    "    w2v_avg = pd.DataFrame(w2v_list)\n",
    "    w2v_avg.index = tol_seq.index\n",
    "    w2v_avg.columns = [f\"w2v_avg_{col}_{i}\" for i in range(vesize)]\n",
    "    w2v_avg.to_pickle(f\"{UFEDIR}/w2v_avg_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    gen_w2v(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_d2v(col, vesize=200, win=20):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    tol_seq = pd.concat([train_seq, test_seq])\n",
    "    \n",
    "    train_seq = None\n",
    "    test_seq = None\n",
    "    \n",
    "    if os.path.exists(f\"{UMDIR}/vectors/d2v_{col}.model\"):\n",
    "        model = gensim.models.Doc2Vec.load(f\"{UMDIR}/vectors/d2v_{col}.model\")\n",
    "    else:\n",
    "        docs = [gensim.models.doc2vec.TaggedDocument(words=i[1],tags=[str(i[0])]) for i in tol_seq[col].reset_index().values]\n",
    "        model = gensim.models.Doc2Vec(documents=docs, size=vesize, window=win, workers=32, iter=10)\n",
    "        model.save(f\"{UMDIR}/vectors/d2v_{col}.model\")\n",
    "    \n",
    "    d2v_list = list()\n",
    "    for it, cps in tqdm(tol_seq[col].reset_index().values):\n",
    "#         if it in model.docvecs:\n",
    "        d2v_list.append(model.docvecs[str(it)])\n",
    "#         else:\n",
    "#             d2v_list.append(model.infer_vector(cps))\n",
    "\n",
    "    d2v_avg = pd.DataFrame(d2v_list)\n",
    "    d2v_avg.index = tol_seq.index\n",
    "    d2v_avg.columns = [f\"d2v_avg_{col}_{i}\" for i in range(vesize)]\n",
    "    d2v_avg = reduce_mem_usage(d2v_avg)\n",
    "    d2v_avg.to_pickle(f\"{UFEDIR}/d2v_avg_{col}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    gen_d2v(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_train = pd.merge(train_click_log, train_ad, how=\"left\", on=\"creative_id\")\n",
    "tol_test = pd.merge(test_click_log, test_ad, how=\"left\", on=\"creative_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_click_log, train_ad\n",
    "del test_click_log, test_ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category map to Label (use GroupFold) - a little overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gen_catemlb(train_data, test_data, tag, col):\n",
    "    train_use = pd.merge(train_data[[UID, col]], tag, how=\"left\", on=UID)\n",
    "    test_use = test_data[[UID, col]]\n",
    "    \n",
    "    nfolds = 5\n",
    "    kfold = GroupKFold(n_splits=nfolds)\n",
    "    \n",
    "    kf_map_df = pd.DataFrame()\n",
    "\n",
    "    re_train_use = pd.DataFrame()\n",
    "    re_test_use = pd.DataFrame()\n",
    "\n",
    "    for tr_idx, val_idx in kfold.split(train_use, groups=train_use[UID]):\n",
    "        tr_ucid, val_ucid = train_use.iloc[tr_idx], train_use.iloc[val_idx]\n",
    "        kf_map = tr_ucid.groupby([col])[[\"age\", \"gender\"]].agg([\"mean\"])\n",
    "        kf_map.columns = bch_rencol(kf_map.columns, prefix=f\"{col}_\")\n",
    "        # only use intersect between train and test\n",
    "        kf_map.drop(np.setdiff1d(kf_map.index.unique(), test_use[col].unique()), inplace=True)\n",
    "        # get kf_cum\n",
    "        if kf_map_df.empty:\n",
    "            kf_map_df = kf_map\n",
    "        else:\n",
    "            kf_map_df = (kf_map_df + kf_map).fillna(0)\n",
    "        \n",
    "        val_ucid = pd.merge(val_ucid, kf_map, how=\"left\", on=col)\n",
    "        re_train_use = pd.concat([re_train_use, val_ucid])\n",
    "    \n",
    "    kf_map_df = kf_map_df/nfolds\n",
    "    re_test_use = pd.merge(test_use, kf_map_df, how=\"left\", on=col)\n",
    "\n",
    "    assert len(train_use) == len(re_train_use)\n",
    "    assert len(test_use) == len(re_test_use)\n",
    "\n",
    "    train_use = None\n",
    "    test_use = None\n",
    "\n",
    "    tmp = re_train_use.groupby([UID], sort=False)[f\"{col}_age_mean\", f\"{col}_gender_mean\"].agg([\"sum\", \"max\", \"mean\", \"min\", \"std\"])\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    tmp.to_pickle(f\"{UFEDIR}/train_stats_{col}_catemlb.pkl\")\n",
    "    tmp = None\n",
    "\n",
    "    tmp = re_test_use.groupby([UID], sort=False)[f\"{col}_age_mean\", f\"{col}_gender_mean\"].agg([\"sum\", \"max\", \"mean\", \"min\", \"std\"])\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    tmp.to_pickle(f\"{UFEDIR}/test_stats_{col}_catemlb.pkl\")\n",
    "    tmp = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    gen_catemlb(tol_train, tol_test, train_user, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_gid = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_dic = {\n",
    "        UID: [\"count\"], \n",
    "        \"click_times\": [\"sum\", \"max\", \"mean\", \"std\", at_percentile(.75), ],\n",
    "        \"time\": [\"nunique\", at_range, at_lenDnunq, at_lenDrange, ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ops_dic:\n",
    "    tmp = tol_train.groupby([UID], sort=False)[[col]].agg(ops_dic[col])\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    tmp.to_pickle(f\"{UFEDIR}/train_stats_{col}.pkl\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_gid.check(\"Group by UID for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ops_dic:\n",
    "    tmp = tol_test.groupby([UID], sort=False)[[col]].agg(ops_dic[col])\n",
    "    tmp.columns = bch_rencol(tmp.columns)\n",
    "    tmp.to_pickle(f\"{UFEDIR}/test_stats_{col}.pkl\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_gid.check(\"Group by UID for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by UID and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_gidt = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tol_train.groupby([UID, \"time\"])[[\"time\"]].agg([\"count\"]).groupby([UID]).agg([\"sum\", \"max\", \"min\", \"mean\", \"std\"])\n",
    "tmp.columns = bch_rencol(tmp.columns)\n",
    "tmp.to_pickle(f\"{UFEDIR}/train_stats_time2{UID}.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tol_train.groupby([UID, \"time\"])[[\"click_times\"]].agg([\"sum\", \"max\", \"min\", \"mean\", \"std\"]).groupby([UID]).agg([\"sum\", \"max\", \"min\", \"mean\", \"std\"])\n",
    "tmp.columns = bch_rencol(tmp.columns)\n",
    "tmp.to_pickle(f\"{UFEDIR}/train_stats_click_times2{UID}.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_gidt.check(\"Group by columns to UID for train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tol_test.groupby([UID, \"time\"])[[\"time\"]].agg([\"count\"]).groupby([UID]).agg([\"sum\", \"max\", \"min\", \"mean\", \"std\"])\n",
    "tmp.columns = bch_rencol(tmp.columns)\n",
    "tmp.to_pickle(f\"{UFEDIR}/test_stats_time2{UID}.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tol_test.groupby([UID, \"time\"])[[\"click_times\"]].agg([\"sum\", \"max\", \"min\", \"mean\", \"std\"]).groupby([UID]).agg([\"sum\", \"max\", \"min\", \"mean\", \"std\"])\n",
    "tmp.columns = bch_rencol(tmp.columns)\n",
    "tmp.to_pickle(f\"{UFEDIR}/test_stats_click_times2{UID}.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_gidt.check(\"Group by columns to UID for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "tmp = tol_train.groupby([UID, \"product_category\"], sort=False)[[\"product_category\"]].agg([\"count\"]).unstack().fillna(0)\n",
    "tmp.columns = bch_rencol(tmp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_pickle(f\"{UFEDIR}/train_onehot.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "tmp = tol_test.groupby([UID, \"product_category\"], sort=False)[[\"product_category\"]].agg([\"count\"]).unstack().fillna(0)\n",
    "tmp.columns = bch_rencol(tmp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_pickle(f\"{UFEDIR}/test_onehot.pkl\")\n",
    "tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Train \n",
    "\n",
    "__weak learner prediction as features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_test.index = -tol_test.index -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.concat([tol_train, tol_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tol_train, tol_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data[f\"{UID}_count\"] = tol_data[[UID]].groupby([UID])[UID].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]:\n",
    "    tmp = tol_data[[UID, col]]\n",
    "    tol_data[f\"{UID}_{col}_std\"] = tmp.groupby([UID])[col].transform(\"std\")\n",
    "    tol_data[f\"{col}_{UID}_count\"] = tmp.groupby([col])[UID].transform(\"count\")\n",
    "    tol_data[f\"{col}_{UID}_std\"] = tmp.groupby([col])[UID].transform(\"std\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"time\", \"click_times\"]:\n",
    "    tmp = tol_data[[UID, col]]\n",
    "    tol_data[f\"{UID}_{col}_std\"] = tmp.groupby([UID])[col].transform(\"std\")\n",
    "    tol_data[f\"{UID}_{col}_mean\"] = tmp.groupby([UID])[col].transform(\"mean\")\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = reduce_mem_usage(tol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data.to_pickle(f\"{UDDIR}/imd/tol_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if have tol_data.pkl\n",
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")\n",
    "\n",
    "tol_data = pd.read_pickle(f\"{UDDIR}/imd/tol_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred))** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.merge(tol_data, train_user[[UID, \"age\"]], how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df_hist_train df_new_train df_hist_new_train to train 3 models\n",
    "train_df = tol_data[tol_data[\"age\"].notnull()]\n",
    "test_df = tol_data[tol_data[\"age\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [UID, \"age\"]\n",
    "cat_features = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in tol_data.columns if f not in drop_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits= 3\n",
    "folds = GroupKFold(n_splits=n_splits)\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "print ('feats:' + str(len(feats)))\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[\"age\"],groups=train_df[UID])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df[\"age\"].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[\"age\"].iloc[valid_idx] \n",
    "    \n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "               \"objective\" : \"regression\", \n",
    "               \"boosting\" : \"gbdt\", \n",
    "               \"metric\" : \"rmse\",  \n",
    "               \"max_depth\": 7, \n",
    "               \"num_leaves\" : 31, \n",
    "               \"max_bin\" : 255, \n",
    "               \"learning_rate\" : 0.1, \n",
    "               \"subsample\" : 0.8,\n",
    "               \"colsample_bytree\" : 0.8, \n",
    "               \"verbosity\": -1,\n",
    "               \"num_threads\" : -1,\n",
    "    }\n",
    "    \n",
    "\n",
    "    if n_fold >= 0:\n",
    "        evals_result = {}\n",
    "        dtrain = lgb.Dataset(\n",
    "            train_x, label=train_y,categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(\n",
    "            valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features)\n",
    "        bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=30000,\n",
    "            valid_sets=[dval], early_stopping_rounds=100, verbose_eval=20,)#feval = evalerror\n",
    "        \n",
    "        new_list = sorted(zip(feats, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:]\n",
    "        for item in new_list:\n",
    "            print (item) \n",
    "\n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "\n",
    "        sub_preds += bst.predict(test_df[feats], num_iteration=bst.best_iteration) / folds.n_splits # test_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = rmse(train_df[\"age\"],  oof_preds)\n",
    "print('Full OOF RMSE %.6f' % cv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df[[UID]]\n",
    "b = test_df[[UID]]\n",
    "\n",
    "a[\"age_pred\"] = oof_preds\n",
    "b[\"age_pred\"] = sub_preds\n",
    "\n",
    "a1 = a.groupby([UID])[\"age_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])\n",
    "b1 = b.groupby([UID])[\"age_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.append(b1).add_prefix(\"age_gkf_agg_pred_\").reset_index().sort_values(by=[UID]).reset_index(drop=True).to_pickle(f\"{UFEDIR}/meta_age_group_regeress.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_data = pd.merge(tol_data, train_user[[UID, \"gender\"]], how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df_hist_train df_new_train df_hist_new_train to train 3 models\n",
    "train_df = tol_data[tol_data[\"gender\"].notnull()]\n",
    "test_df = tol_data[tol_data[\"gender\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [UID, \"gender\"]\n",
    "cat_features = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in tol_data.columns if f not in drop_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits= 3\n",
    "folds = GroupKFold(n_splits=n_splits)\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "print ('feats:' + str(len(feats)))\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[\"gender\"],groups=train_df[UID])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df[\"gender\"].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[\"gender\"].iloc[valid_idx] \n",
    "    \n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "               \"objective\" : \"regression\", \n",
    "               \"boosting\" : \"gbdt\", \n",
    "               \"metric\" : \"rmse\",  \n",
    "               \"max_depth\": 7, \n",
    "               \"num_leaves\" : 31, \n",
    "               \"max_bin\" : 255, \n",
    "               \"learning_rate\" : 0.1, \n",
    "               \"subsample\" : 0.8,\n",
    "               \"colsample_bytree\" : 0.8, \n",
    "               \"verbosity\": -1,\n",
    "               \"num_threads\" : -1,\n",
    "    }\n",
    "    \n",
    "\n",
    "    if n_fold >= 0:\n",
    "        evals_result = {}\n",
    "        dtrain = lgb.Dataset(\n",
    "            train_x, label=train_y,categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(\n",
    "            valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features)\n",
    "        bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=30000,\n",
    "            valid_sets=[dval], early_stopping_rounds=100, verbose_eval=20,)#feval = evalerror\n",
    "        \n",
    "        new_list = sorted(zip(feats, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:]\n",
    "        for item in new_list:\n",
    "            print (item) \n",
    "\n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "\n",
    "        sub_preds += bst.predict(test_df[feats], num_iteration=bst.best_iteration) / folds.n_splits # test_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = rmse(train_df[\"gender\"],  oof_preds)\n",
    "print('Full OOF RMSE %.6f' % cv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df[[UID]]\n",
    "b = test_df[[UID]]\n",
    "\n",
    "a[\"gender_pred\"] = oof_preds\n",
    "b[\"gender_pred\"] = sub_preds\n",
    "\n",
    "a1 = a.groupby([UID])[\"gender_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])\n",
    "b1 = b.groupby([UID])[\"gender_pred\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.append(b1).add_prefix(\"gender_gkf_agg_pred_\").reset_index().sort_values(by=[UID]).reset_index(drop=True).to_pickle(f\"{UFEDIR}/meta_gender_group_regeress.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory (only once)\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"w2v_\") or fname.startswith(\"tfidf_svd_\") or fname.startswith(\"meta_age_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        reduce_mem_usage(pd.read_pickle(f\"{UFEDIR}/{fname}\")).to_pickle(f\"{UFEDIR}/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()\n",
    "train_feat[UID] = train_user[UID]\n",
    "test_feat[UID] = test_user[UID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fname = os.listdir(UFEDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  w2v_avg_advertiser_id.pkl\n",
      "current filename:  w2v_avg_industry.pkl\n",
      "current filename:  w2v_avg_ad_id.pkl\n",
      "current filename:  w2v_avg_product_id.pkl\n",
      "current filename:  w2v_avg_creative_id.pkl\n",
      "current filename:  w2v_avg_product_category.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"w2v_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_w2v = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_w2v, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_w2v, how=\"left\", on=UID)\n",
    "        cur_w2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not use now\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_feat = pd.merge(train_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_feat = pd.merge(test_feat, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  tfidf_svd_industry.pkl\n",
      "current filename:  tfidf_svd_advertiser_id.pkl\n",
      "current filename:  tfidf_svd_product_category.pkl\n",
      "current filename:  tfidf_svd_creative_id.pkl\n",
      "current filename:  tfidf_svd_product_id.pkl\n",
      "current filename:  tfidf_svd_ad_id.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"tfidf_svd_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_tfidf_svd = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "        cur_tfidf_svd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  meta_age_group_regeress.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"meta_age_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        age_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, age_agg_pred, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, age_agg_pred, how=\"left\", on=UID)\n",
    "        age_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  d2v_avg_product_id.pkl\n",
      "current filename:  d2v_avg_ad_id.pkl\n",
      "current filename:  d2v_avg_advertiser_id.pkl\n",
      "current filename:  d2v_avg_industry.pkl\n",
      "current filename:  d2v_avg_product_category.pkl\n",
      "current filename:  d2v_avg_creative_id.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"d2v_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_d2v = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_feat = pd.merge(train_feat, cur_d2v, how=\"left\", on=UID)\n",
    "        test_feat = pd.merge(test_feat, cur_d2v, how=\"left\", on=UID)\n",
    "        cur_d2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make sure feat and user(target) have same order\n",
    "# if true --> sum == 0\n",
    "np.sum(train_feat[UID] != train_user[UID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 2744)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2744)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_feat[[UID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4724.12109375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat.memory_usage().sum() / 1024**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5249.0234375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feat.memory_usage().sum() / 1024**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat.to_pickle(f\"{UDDIR}/feat_ing/train_feat_tol_v05.pkl\")\n",
    "# test_feat.to_pickle(f\"{UDDIR}/feat_ing/test_feat_tol_v05.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat = pd.read_pickle(f\"{UDDIR}/feat_ing/train_feat_tol_v05.pkl\")\n",
    "# test_feat = pd.read_pickle(f\"{UDDIR}/feat_ing/test_feat_tol_v05.pkl\")\n",
    "# train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat.drop([col for col in train_feat.columns if col.find(\"creative_id_gender_\") != -1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feat.drop([col for col in test_feat.columns if col.find(\"creative_id_gender_\") != -1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training&Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat.drop(UID, axis=1, inplace=True)\n",
    "test_feat.drop(UID, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_feat_tr, train_feat_val, train_tag_tr, train_tag_val = train_test_split(train_feat, train_user, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_tr.drop(UID, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_val.drop(UID, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbds_train_tr_age = lgb.Dataset(train_feat_tr, train_tag_tr[\"age\"]-1)\n",
    "lgbds_train_val_age = lgb.Dataset(train_feat_val, train_tag_val[\"age\"]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_age = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 10,\n",
    "    \"metric\": [\"multi_logloss\", \"multi_error\"],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"seed\": 2020,\n",
    "    \"n_jobs\": -1,\n",
    "    \"min_child_weight\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 5,     \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb_multi_age_off = lgb.train(params_age, lgbds_train_tr_age, num_boost_round=1000, valid_sets=[lgbds_train_val_age], verbose_eval=50, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_age_prob = model_lgb_multi_age_off.predict(train_feat_val, num_iteration=model_lgb_multi_age_off.best_iteration)\n",
    "train_val_age_pred = [list(x).index(max(x))+1 for x in train_val_age_prob]\n",
    "age_acy = accuracy_score(train_val_age_pred, train_tag_val[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbds_train_age = lgb.Dataset(train_feat, train_user[\"age\"]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_age = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 10,\n",
    "    \"metric\": [\"multi_logloss\", \"multi_error\"],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"seed\": 2020,\n",
    "    \"n_jobs\": -1,\n",
    "    \"min_child_weight\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 5,     \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb_multi_age = lgb.train(params_age, lgbds_train_age, num_boost_round=1000, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndt = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))\n",
    "model_lgb_multi_age.save_model(f\"{UMDIR}/lgb_multi_age_{ndt}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_mage = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_age = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 10,\n",
    "    \"metric\": [\"multi_logloss\", \"multi_error\"],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"seed\": 2020,\n",
    "    \"n_jobs\": -1,\n",
    "    \"min_child_weight\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 5,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "result_proba = []\n",
    "age_feat_import_df = pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 1.5947\tvalid_0's multi_error: 0.639867\n",
      "[100]\tvalid_0's multi_logloss: 1.52649\tvalid_0's multi_error: 0.621544\n",
      "[150]\tvalid_0's multi_logloss: 1.49443\tvalid_0's multi_error: 0.611422\n",
      "[200]\tvalid_0's multi_logloss: 1.47452\tvalid_0's multi_error: 0.60565\n",
      "[250]\tvalid_0's multi_logloss: 1.46076\tvalid_0's multi_error: 0.600861\n",
      "[300]\tvalid_0's multi_logloss: 1.4505\tvalid_0's multi_error: 0.597217\n",
      "[350]\tvalid_0's multi_logloss: 1.44274\tvalid_0's multi_error: 0.595122\n",
      "[400]\tvalid_0's multi_logloss: 1.43658\tvalid_0's multi_error: 0.593644\n",
      "[450]\tvalid_0's multi_logloss: 1.43152\tvalid_0's multi_error: 0.591378\n",
      "[500]\tvalid_0's multi_logloss: 1.42752\tvalid_0's multi_error: 0.5901\n",
      "[550]\tvalid_0's multi_logloss: 1.42403\tvalid_0's multi_error: 0.588744\n",
      "[600]\tvalid_0's multi_logloss: 1.42146\tvalid_0's multi_error: 0.588372\n",
      "[650]\tvalid_0's multi_logloss: 1.41933\tvalid_0's multi_error: 0.587667\n",
      "[700]\tvalid_0's multi_logloss: 1.41749\tvalid_0's multi_error: 0.587028\n",
      "[750]\tvalid_0's multi_logloss: 1.41589\tvalid_0's multi_error: 0.586494\n",
      "[800]\tvalid_0's multi_logloss: 1.41465\tvalid_0's multi_error: 0.585989\n",
      "[850]\tvalid_0's multi_logloss: 1.41357\tvalid_0's multi_error: 0.585444\n",
      "[900]\tvalid_0's multi_logloss: 1.41271\tvalid_0's multi_error: 0.585728\n",
      "[950]\tvalid_0's multi_logloss: 1.41222\tvalid_0's multi_error: 0.58545\n",
      "[1000]\tvalid_0's multi_logloss: 1.41173\tvalid_0's multi_error: 0.585606\n",
      "Early stopping, best iteration is:\n",
      "[906]\tvalid_0's multi_logloss: 1.41261\tvalid_0's multi_error: 0.585422\n",
      "current validation score:  0.41457777777777777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 1.5972\tvalid_0's multi_error: 0.640628\n",
      "[100]\tvalid_0's multi_logloss: 1.52925\tvalid_0's multi_error: 0.621944\n",
      "[150]\tvalid_0's multi_logloss: 1.49725\tvalid_0's multi_error: 0.611872\n",
      "[200]\tvalid_0's multi_logloss: 1.47764\tvalid_0's multi_error: 0.606228\n",
      "[250]\tvalid_0's multi_logloss: 1.46412\tvalid_0's multi_error: 0.601211\n",
      "[300]\tvalid_0's multi_logloss: 1.45418\tvalid_0's multi_error: 0.598672\n",
      "[350]\tvalid_0's multi_logloss: 1.44649\tvalid_0's multi_error: 0.595339\n",
      "[400]\tvalid_0's multi_logloss: 1.44031\tvalid_0's multi_error: 0.593567\n",
      "[450]\tvalid_0's multi_logloss: 1.4354\tvalid_0's multi_error: 0.591756\n",
      "[500]\tvalid_0's multi_logloss: 1.43149\tvalid_0's multi_error: 0.590878\n",
      "[550]\tvalid_0's multi_logloss: 1.42818\tvalid_0's multi_error: 0.589622\n",
      "[600]\tvalid_0's multi_logloss: 1.42552\tvalid_0's multi_error: 0.588667\n",
      "[650]\tvalid_0's multi_logloss: 1.42335\tvalid_0's multi_error: 0.588311\n",
      "[700]\tvalid_0's multi_logloss: 1.42167\tvalid_0's multi_error: 0.587\n",
      "[750]\tvalid_0's multi_logloss: 1.42016\tvalid_0's multi_error: 0.586139\n",
      "[800]\tvalid_0's multi_logloss: 1.41899\tvalid_0's multi_error: 0.586178\n",
      "[850]\tvalid_0's multi_logloss: 1.41819\tvalid_0's multi_error: 0.585778\n",
      "[900]\tvalid_0's multi_logloss: 1.41739\tvalid_0's multi_error: 0.585261\n",
      "[950]\tvalid_0's multi_logloss: 1.41662\tvalid_0's multi_error: 0.585167\n",
      "[1000]\tvalid_0's multi_logloss: 1.41622\tvalid_0's multi_error: 0.585539\n",
      "[1050]\tvalid_0's multi_logloss: 1.41571\tvalid_0's multi_error: 0.585228\n",
      "Early stopping, best iteration is:\n",
      "[978]\tvalid_0's multi_logloss: 1.41639\tvalid_0's multi_error: 0.58495\n",
      "current validation score:  0.41505\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 1.59628\tvalid_0's multi_error: 0.638506\n",
      "[100]\tvalid_0's multi_logloss: 1.52838\tvalid_0's multi_error: 0.620911\n",
      "[150]\tvalid_0's multi_logloss: 1.49639\tvalid_0's multi_error: 0.611417\n",
      "[200]\tvalid_0's multi_logloss: 1.47658\tvalid_0's multi_error: 0.605306\n",
      "[250]\tvalid_0's multi_logloss: 1.46307\tvalid_0's multi_error: 0.601778\n",
      "[300]\tvalid_0's multi_logloss: 1.45303\tvalid_0's multi_error: 0.598328\n",
      "[350]\tvalid_0's multi_logloss: 1.44549\tvalid_0's multi_error: 0.596206\n",
      "[400]\tvalid_0's multi_logloss: 1.43925\tvalid_0's multi_error: 0.59405\n",
      "[450]\tvalid_0's multi_logloss: 1.43435\tvalid_0's multi_error: 0.592561\n",
      "[500]\tvalid_0's multi_logloss: 1.43036\tvalid_0's multi_error: 0.591539\n",
      "[550]\tvalid_0's multi_logloss: 1.42741\tvalid_0's multi_error: 0.590844\n",
      "[600]\tvalid_0's multi_logloss: 1.42473\tvalid_0's multi_error: 0.5899\n",
      "[650]\tvalid_0's multi_logloss: 1.4227\tvalid_0's multi_error: 0.588878\n",
      "[700]\tvalid_0's multi_logloss: 1.42097\tvalid_0's multi_error: 0.588894\n",
      "[750]\tvalid_0's multi_logloss: 1.41969\tvalid_0's multi_error: 0.588311\n",
      "[800]\tvalid_0's multi_logloss: 1.41839\tvalid_0's multi_error: 0.588022\n",
      "[850]\tvalid_0's multi_logloss: 1.41738\tvalid_0's multi_error: 0.587906\n",
      "[900]\tvalid_0's multi_logloss: 1.41665\tvalid_0's multi_error: 0.587956\n",
      "[950]\tvalid_0's multi_logloss: 1.41596\tvalid_0's multi_error: 0.587578\n",
      "[1000]\tvalid_0's multi_logloss: 1.41547\tvalid_0's multi_error: 0.587333\n",
      "[1050]\tvalid_0's multi_logloss: 1.41488\tvalid_0's multi_error: 0.586794\n",
      "[1100]\tvalid_0's multi_logloss: 1.4146\tvalid_0's multi_error: 0.586533\n",
      "[1150]\tvalid_0's multi_logloss: 1.41445\tvalid_0's multi_error: 0.586517\n",
      "Early stopping, best iteration is:\n",
      "[1084]\tvalid_0's multi_logloss: 1.41465\tvalid_0's multi_error: 0.586278\n",
      "current validation score:  0.4137222222222222\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 1.5958\tvalid_0's multi_error: 0.640156\n",
      "[100]\tvalid_0's multi_logloss: 1.52812\tvalid_0's multi_error: 0.622628\n",
      "[150]\tvalid_0's multi_logloss: 1.49652\tvalid_0's multi_error: 0.613289\n",
      "[200]\tvalid_0's multi_logloss: 1.47679\tvalid_0's multi_error: 0.607956\n",
      "[250]\tvalid_0's multi_logloss: 1.46314\tvalid_0's multi_error: 0.603483\n",
      "[300]\tvalid_0's multi_logloss: 1.45311\tvalid_0's multi_error: 0.600133\n",
      "[350]\tvalid_0's multi_logloss: 1.44528\tvalid_0's multi_error: 0.597878\n",
      "[400]\tvalid_0's multi_logloss: 1.43923\tvalid_0's multi_error: 0.595522\n",
      "[450]\tvalid_0's multi_logloss: 1.43442\tvalid_0's multi_error: 0.593806\n",
      "[500]\tvalid_0's multi_logloss: 1.43067\tvalid_0's multi_error: 0.592194\n",
      "[550]\tvalid_0's multi_logloss: 1.42741\tvalid_0's multi_error: 0.591317\n",
      "[600]\tvalid_0's multi_logloss: 1.42497\tvalid_0's multi_error: 0.590072\n",
      "[650]\tvalid_0's multi_logloss: 1.42279\tvalid_0's multi_error: 0.58935\n",
      "[700]\tvalid_0's multi_logloss: 1.42109\tvalid_0's multi_error: 0.589167\n",
      "[750]\tvalid_0's multi_logloss: 1.41965\tvalid_0's multi_error: 0.58855\n",
      "[800]\tvalid_0's multi_logloss: 1.41842\tvalid_0's multi_error: 0.588439\n",
      "[850]\tvalid_0's multi_logloss: 1.4175\tvalid_0's multi_error: 0.588017\n",
      "[900]\tvalid_0's multi_logloss: 1.41675\tvalid_0's multi_error: 0.587583\n",
      "[950]\tvalid_0's multi_logloss: 1.41605\tvalid_0's multi_error: 0.587494\n",
      "[1000]\tvalid_0's multi_logloss: 1.41544\tvalid_0's multi_error: 0.587294\n",
      "Early stopping, best iteration is:\n",
      "[927]\tvalid_0's multi_logloss: 1.41638\tvalid_0's multi_error: 0.587089\n",
      "current validation score:  0.4129111111111111\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 1.59466\tvalid_0's multi_error: 0.637806\n",
      "[100]\tvalid_0's multi_logloss: 1.52624\tvalid_0's multi_error: 0.619589\n",
      "[150]\tvalid_0's multi_logloss: 1.49414\tvalid_0's multi_error: 0.60935\n",
      "[200]\tvalid_0's multi_logloss: 1.47404\tvalid_0's multi_error: 0.603356\n",
      "[250]\tvalid_0's multi_logloss: 1.46024\tvalid_0's multi_error: 0.598606\n",
      "[300]\tvalid_0's multi_logloss: 1.4499\tvalid_0's multi_error: 0.595383\n",
      "[350]\tvalid_0's multi_logloss: 1.44186\tvalid_0's multi_error: 0.592622\n",
      "[400]\tvalid_0's multi_logloss: 1.43572\tvalid_0's multi_error: 0.590817\n",
      "[450]\tvalid_0's multi_logloss: 1.43052\tvalid_0's multi_error: 0.588894\n",
      "[500]\tvalid_0's multi_logloss: 1.42651\tvalid_0's multi_error: 0.587267\n",
      "[550]\tvalid_0's multi_logloss: 1.42319\tvalid_0's multi_error: 0.586022\n",
      "[600]\tvalid_0's multi_logloss: 1.42055\tvalid_0's multi_error: 0.585161\n",
      "[650]\tvalid_0's multi_logloss: 1.41837\tvalid_0's multi_error: 0.584467\n",
      "[700]\tvalid_0's multi_logloss: 1.41652\tvalid_0's multi_error: 0.584444\n",
      "[750]\tvalid_0's multi_logloss: 1.41521\tvalid_0's multi_error: 0.584044\n",
      "[800]\tvalid_0's multi_logloss: 1.41384\tvalid_0's multi_error: 0.583361\n",
      "[850]\tvalid_0's multi_logloss: 1.4129\tvalid_0's multi_error: 0.583394\n",
      "[900]\tvalid_0's multi_logloss: 1.412\tvalid_0's multi_error: 0.583061\n",
      "[950]\tvalid_0's multi_logloss: 1.41145\tvalid_0's multi_error: 0.582406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_0's multi_logloss: 1.41085\tvalid_0's multi_error: 0.582978\n",
      "Early stopping, best iteration is:\n",
      "[936]\tvalid_0's multi_logloss: 1.41162\tvalid_0's multi_error: 0.581967\n",
      "current validation score:  0.4180333333333333\n",
      "accuracy score:  0.4148588888888889\n"
     ]
    }
   ],
   "source": [
    "for cur_fold, (tr_idx, val_idx) in enumerate(kfold.split(train_feat, train_user[\"age\"]-1)):\n",
    "    tr_x = tr_y = val_x = val_y = None\n",
    "    tr_x, tr_y, val_x, val_y = train_feat.iloc[tr_idx], train_user[\"age\"].iloc[tr_idx]-1, train_feat.iloc[val_idx], train_user[\"age\"].iloc[val_idx]-1\n",
    "    train_set = lgb.Dataset(tr_x, tr_y)\n",
    "    val_set = lgb.Dataset(val_x, val_y)\n",
    "    lgb_model = lgb.train(params_age, train_set,\n",
    "                          valid_sets=[val_set], early_stopping_rounds=100, num_boost_round=40000, verbose_eval=50)\n",
    "    # save feature importance\n",
    "    tmp = pd.DataFrame(lgb_model.feature_name(), columns=[\"feature_name\"])\n",
    "    tmp[f\"feature_importance_fold{cur_fold}\"] = lgb_model.feature_importance()\n",
    "    if age_feat_import_df.empty:\n",
    "        age_feat_import_df = tmp\n",
    "    else:\n",
    "        age_feat_import_df = pd.merge(age_feat_import_df, tmp, how=\"left\", on=\"feature_name\")\n",
    "    # get valid pred\n",
    "    val_pred = np.argmax(lgb_model.predict(\n",
    "        val_x, num_iteration=lgb_model.best_iteration), axis=1)\n",
    "    val_score = accuracy_score(val_pred, val_y)\n",
    "    # get test pred\n",
    "    result_proba.append(lgb_model.predict(\n",
    "        test_feat, num_iteration=lgb_model.best_iteration))\n",
    "    scores.append(val_score)\n",
    "    print(\"current validation score: \", val_score)\n",
    "    gc.collect()\n",
    "print(\"accuracy score: \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_age = np.argmax(np.mean(result_proba, axis=0), axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K-Fold train costs] spend 24908.36 sec\n"
     ]
    }
   ],
   "source": [
    "tm_mage.check(\"K-Fold train costs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training until validation scores don't improve for 100 rounds\n",
    "[50]\tvalid_0's multi_error: 0.649072\n",
    "[100]\tvalid_0's multi_error: 0.632817\n",
    "[150]\tvalid_0's multi_error: 0.623339\n",
    "[200]\tvalid_0's multi_error: 0.617433\n",
    "[250]\tvalid_0's multi_error: 0.613378\n",
    "[300]\tvalid_0's multi_error: 0.611011\n",
    "[350]\tvalid_0's multi_error: 0.609172\n",
    "[400]\tvalid_0's multi_error: 0.606422\n",
    "[450]\tvalid_0's multi_error: 0.605522\n",
    "[500]\tvalid_0's multi_error: 0.604956\n",
    "[550]\tvalid_0's multi_error: 0.603811\n",
    "[600]\tvalid_0's multi_error: 0.603089\n",
    "[650]\tvalid_0's multi_error: 0.602733\n",
    "[700]\tvalid_0's multi_error: 0.601961\n",
    "[750]\tvalid_0's multi_error: 0.601689\n",
    "[800]\tvalid_0's multi_error: 0.601744\n",
    "[850]\tvalid_0's multi_error: 0.601217\n",
    "[900]\tvalid_0's multi_error: 0.600606\n",
    "[950]\tvalid_0's multi_error: 0.600294\n",
    "[1000]\tvalid_0's multi_error: 0.600333\n",
    "[1050]\tvalid_0's multi_error: 0.600011\n",
    "[1100]\tvalid_0's multi_error: 0.600339\n",
    "Early stopping, best iteration is:\n",
    "[1035]\tvalid_0's multi_error: 0.599733\n",
    "current validation score:  0.40026666666666666\n",
    "Training until validation scores don't improve for 100 rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbds_train_tr_gender = lgb.Dataset(train_feat_tr, train_tag_tr[\"gender\"]-1)\n",
    "lgbds_train_val_gender = lgb.Dataset(train_feat_val, train_tag_val[\"gender\"]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gender = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 2,\n",
    "    \"metric\": [\"multi_logloss\", \"multi_error\"],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"seed\": 2020,\n",
    "    \"n_jobs\": -1,\n",
    "    \"min_child_weight\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb_multi_gender_off = lgb.train(params_gender, lgbds_train_tr_gender, num_boost_round=1000, valid_sets=[lgbds_train_val_gender], verbose_eval=50, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_gender_prob = model_lgb_multi_gender_off.predict(train_feat_val, num_iteration=model_lgb_multi_gender_off.best_iteration)\n",
    "train_val_gender_pred = [list(x).index(max(x))+1 for x in train_val_gender_prob]\n",
    "gender_acy = accuracy_score(train_val_gender_pred, train_tag_val[\"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbds_train_gender = lgb.Dataset(train_feat, train_user[\"gender\"]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gender = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 2,\n",
    "    \"metric\": [\"multi_logloss\", \"multi_error\"],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"seed\": 2020,\n",
    "    \"n_jobs\": -1,\n",
    "    \"min_child_weight\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb_multi_gender = lgb.train(params_gender, lgbds_train_gender, num_boost_round=1000, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndt = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))\n",
    "model_lgb_multi_gender.save_model(f\"{UMDIR}/lgb_multi_gender_{ndt}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_mgender = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gender = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 2,\n",
    "    \"metric\": [\"multi_logloss\", \"multi_error\"],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"seed\": 2020,\n",
    "    \"n_jobs\": -1,\n",
    "    \"min_child_weight\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_scores = []\n",
    "gender_result_proba = []\n",
    "gender_feat_import_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.236098\tvalid_0's multi_error: 0.0866611\n",
      "[100]\tvalid_0's multi_logloss: 0.208996\tvalid_0's multi_error: 0.0774611\n",
      "[150]\tvalid_0's multi_logloss: 0.199907\tvalid_0's multi_error: 0.0736611\n",
      "[200]\tvalid_0's multi_logloss: 0.194856\tvalid_0's multi_error: 0.0719778\n",
      "[250]\tvalid_0's multi_logloss: 0.191612\tvalid_0's multi_error: 0.0701944\n",
      "[300]\tvalid_0's multi_logloss: 0.18943\tvalid_0's multi_error: 0.0693111\n",
      "[350]\tvalid_0's multi_logloss: 0.187819\tvalid_0's multi_error: 0.0686\n",
      "[400]\tvalid_0's multi_logloss: 0.186607\tvalid_0's multi_error: 0.0680889\n",
      "[450]\tvalid_0's multi_logloss: 0.185802\tvalid_0's multi_error: 0.0675611\n",
      "[500]\tvalid_0's multi_logloss: 0.185229\tvalid_0's multi_error: 0.0675556\n",
      "[550]\tvalid_0's multi_logloss: 0.184861\tvalid_0's multi_error: 0.0673\n",
      "[600]\tvalid_0's multi_logloss: 0.184523\tvalid_0's multi_error: 0.0672722\n",
      "[650]\tvalid_0's multi_logloss: 0.184291\tvalid_0's multi_error: 0.0670778\n",
      "[700]\tvalid_0's multi_logloss: 0.18404\tvalid_0's multi_error: 0.0669889\n",
      "[750]\tvalid_0's multi_logloss: 0.183925\tvalid_0's multi_error: 0.067\n",
      "[800]\tvalid_0's multi_logloss: 0.183833\tvalid_0's multi_error: 0.0668389\n",
      "[850]\tvalid_0's multi_logloss: 0.183766\tvalid_0's multi_error: 0.0670278\n",
      "Early stopping, best iteration is:\n",
      "[782]\tvalid_0's multi_logloss: 0.183858\tvalid_0's multi_error: 0.0667167\n",
      "current validation score: 0.9332833333333334\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.238371\tvalid_0's multi_error: 0.0883889\n",
      "[100]\tvalid_0's multi_logloss: 0.212296\tvalid_0's multi_error: 0.0786056\n",
      "[150]\tvalid_0's multi_logloss: 0.203237\tvalid_0's multi_error: 0.0751278\n",
      "[200]\tvalid_0's multi_logloss: 0.19832\tvalid_0's multi_error: 0.0728556\n",
      "[250]\tvalid_0's multi_logloss: 0.195143\tvalid_0's multi_error: 0.0716389\n",
      "[300]\tvalid_0's multi_logloss: 0.193009\tvalid_0's multi_error: 0.0708611\n",
      "[350]\tvalid_0's multi_logloss: 0.191504\tvalid_0's multi_error: 0.0701222\n",
      "[400]\tvalid_0's multi_logloss: 0.19029\tvalid_0's multi_error: 0.0697611\n",
      "[450]\tvalid_0's multi_logloss: 0.189493\tvalid_0's multi_error: 0.0693722\n",
      "[500]\tvalid_0's multi_logloss: 0.189015\tvalid_0's multi_error: 0.0692278\n",
      "[550]\tvalid_0's multi_logloss: 0.188682\tvalid_0's multi_error: 0.06905\n",
      "[600]\tvalid_0's multi_logloss: 0.188383\tvalid_0's multi_error: 0.06875\n",
      "[650]\tvalid_0's multi_logloss: 0.188141\tvalid_0's multi_error: 0.0688056\n",
      "[700]\tvalid_0's multi_logloss: 0.187995\tvalid_0's multi_error: 0.0685833\n",
      "[750]\tvalid_0's multi_logloss: 0.187866\tvalid_0's multi_error: 0.0686222\n",
      "[800]\tvalid_0's multi_logloss: 0.187741\tvalid_0's multi_error: 0.0686111\n",
      "[850]\tvalid_0's multi_logloss: 0.187641\tvalid_0's multi_error: 0.0685222\n",
      "[900]\tvalid_0's multi_logloss: 0.187559\tvalid_0's multi_error: 0.0683278\n",
      "[950]\tvalid_0's multi_logloss: 0.18746\tvalid_0's multi_error: 0.0681056\n",
      "[1000]\tvalid_0's multi_logloss: 0.187371\tvalid_0's multi_error: 0.0680944\n",
      "[1050]\tvalid_0's multi_logloss: 0.187357\tvalid_0's multi_error: 0.0681\n",
      "Early stopping, best iteration is:\n",
      "[983]\tvalid_0's multi_logloss: 0.187404\tvalid_0's multi_error: 0.0679389\n",
      "current validation score: 0.9320611111111111\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.236494\tvalid_0's multi_error: 0.0863778\n",
      "[100]\tvalid_0's multi_logloss: 0.209797\tvalid_0's multi_error: 0.0774611\n",
      "[150]\tvalid_0's multi_logloss: 0.200215\tvalid_0's multi_error: 0.0736278\n",
      "[200]\tvalid_0's multi_logloss: 0.19506\tvalid_0's multi_error: 0.0711556\n",
      "[250]\tvalid_0's multi_logloss: 0.191687\tvalid_0's multi_error: 0.0696278\n",
      "[300]\tvalid_0's multi_logloss: 0.189571\tvalid_0's multi_error: 0.0687722\n",
      "[350]\tvalid_0's multi_logloss: 0.187976\tvalid_0's multi_error: 0.0679778\n",
      "[400]\tvalid_0's multi_logloss: 0.18682\tvalid_0's multi_error: 0.0675944\n",
      "[450]\tvalid_0's multi_logloss: 0.185937\tvalid_0's multi_error: 0.0672944\n",
      "[500]\tvalid_0's multi_logloss: 0.185324\tvalid_0's multi_error: 0.06695\n",
      "[550]\tvalid_0's multi_logloss: 0.184986\tvalid_0's multi_error: 0.0669889\n",
      "[600]\tvalid_0's multi_logloss: 0.184593\tvalid_0's multi_error: 0.0668\n",
      "[650]\tvalid_0's multi_logloss: 0.184319\tvalid_0's multi_error: 0.0666889\n",
      "[700]\tvalid_0's multi_logloss: 0.184075\tvalid_0's multi_error: 0.0665611\n",
      "[750]\tvalid_0's multi_logloss: 0.183889\tvalid_0's multi_error: 0.0664889\n",
      "[800]\tvalid_0's multi_logloss: 0.183669\tvalid_0's multi_error: 0.0666444\n",
      "Early stopping, best iteration is:\n",
      "[736]\tvalid_0's multi_logloss: 0.183939\tvalid_0's multi_error: 0.0664444\n",
      "current validation score: 0.9335555555555556\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.239271\tvalid_0's multi_error: 0.0887611\n",
      "[100]\tvalid_0's multi_logloss: 0.212977\tvalid_0's multi_error: 0.0796278\n",
      "[150]\tvalid_0's multi_logloss: 0.203863\tvalid_0's multi_error: 0.0754722\n",
      "[200]\tvalid_0's multi_logloss: 0.199003\tvalid_0's multi_error: 0.0736556\n",
      "[250]\tvalid_0's multi_logloss: 0.195729\tvalid_0's multi_error: 0.0720889\n",
      "[300]\tvalid_0's multi_logloss: 0.193497\tvalid_0's multi_error: 0.0709889\n",
      "[350]\tvalid_0's multi_logloss: 0.191941\tvalid_0's multi_error: 0.0702833\n",
      "[400]\tvalid_0's multi_logloss: 0.190795\tvalid_0's multi_error: 0.0697111\n",
      "[450]\tvalid_0's multi_logloss: 0.189888\tvalid_0's multi_error: 0.0693\n",
      "[500]\tvalid_0's multi_logloss: 0.189335\tvalid_0's multi_error: 0.0691944\n",
      "[550]\tvalid_0's multi_logloss: 0.188984\tvalid_0's multi_error: 0.0689278\n",
      "[600]\tvalid_0's multi_logloss: 0.188723\tvalid_0's multi_error: 0.0688333\n",
      "[650]\tvalid_0's multi_logloss: 0.18857\tvalid_0's multi_error: 0.0687056\n",
      "[700]\tvalid_0's multi_logloss: 0.188413\tvalid_0's multi_error: 0.0686444\n",
      "[750]\tvalid_0's multi_logloss: 0.188265\tvalid_0's multi_error: 0.0688889\n",
      "Early stopping, best iteration is:\n",
      "[679]\tvalid_0's multi_logloss: 0.188454\tvalid_0's multi_error: 0.0686167\n",
      "current validation score: 0.9313833333333333\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.238985\tvalid_0's multi_error: 0.0881833\n",
      "[100]\tvalid_0's multi_logloss: 0.212285\tvalid_0's multi_error: 0.0787278\n",
      "[150]\tvalid_0's multi_logloss: 0.202932\tvalid_0's multi_error: 0.0747611\n",
      "[200]\tvalid_0's multi_logloss: 0.197834\tvalid_0's multi_error: 0.0728556\n",
      "[250]\tvalid_0's multi_logloss: 0.194499\tvalid_0's multi_error: 0.0714278\n",
      "[300]\tvalid_0's multi_logloss: 0.192192\tvalid_0's multi_error: 0.0703778\n",
      "[350]\tvalid_0's multi_logloss: 0.190439\tvalid_0's multi_error: 0.0694889\n",
      "[400]\tvalid_0's multi_logloss: 0.189213\tvalid_0's multi_error: 0.0687833\n",
      "[450]\tvalid_0's multi_logloss: 0.188282\tvalid_0's multi_error: 0.0682778\n",
      "[500]\tvalid_0's multi_logloss: 0.187627\tvalid_0's multi_error: 0.0679667\n",
      "[550]\tvalid_0's multi_logloss: 0.187201\tvalid_0's multi_error: 0.0676833\n",
      "[600]\tvalid_0's multi_logloss: 0.186981\tvalid_0's multi_error: 0.0675444\n",
      "[650]\tvalid_0's multi_logloss: 0.186757\tvalid_0's multi_error: 0.0674667\n",
      "[700]\tvalid_0's multi_logloss: 0.186573\tvalid_0's multi_error: 0.0673222\n",
      "[750]\tvalid_0's multi_logloss: 0.186352\tvalid_0's multi_error: 0.0672833\n",
      "[800]\tvalid_0's multi_logloss: 0.186206\tvalid_0's multi_error: 0.0673556\n",
      "[850]\tvalid_0's multi_logloss: 0.186169\tvalid_0's multi_error: 0.0671833\n",
      "[900]\tvalid_0's multi_logloss: 0.185985\tvalid_0's multi_error: 0.0671222\n",
      "[950]\tvalid_0's multi_logloss: 0.185908\tvalid_0's multi_error: 0.0672389\n",
      "[1000]\tvalid_0's multi_logloss: 0.185855\tvalid_0's multi_error: 0.0670222\n",
      "[1050]\tvalid_0's multi_logloss: 0.185828\tvalid_0's multi_error: 0.0670056\n",
      "[1100]\tvalid_0's multi_logloss: 0.185755\tvalid_0's multi_error: 0.06705\n",
      "Early stopping, best iteration is:\n",
      "[1040]\tvalid_0's multi_logloss: 0.18585\tvalid_0's multi_error: 0.06695\n",
      "current validation score: 0.93305\n",
      "accuracy score: 0.9326666666666666\n"
     ]
    }
   ],
   "source": [
    "for cur_fold, (tr_idx, val_idx) in enumerate(kfold.split(train_feat, train_user[\"gender\"]-1)):\n",
    "    tr_x = tr_y = val_x = val_y = None\n",
    "    tr_x, tr_y, val_x, val_y = train_feat.iloc[tr_idx], train_user[\"gender\"].iloc[tr_idx]-1, train_feat.iloc[val_idx], train_user[\"gender\"].iloc[val_idx]-1\n",
    "    train_set = lgb.Dataset(tr_x, tr_y)\n",
    "    val_set = lgb.Dataset(val_x, val_y)\n",
    "    lgb_model = lgb.train(params_gender, train_set,\n",
    "                          valid_sets=[val_set], early_stopping_rounds=100, num_boost_round=40000, verbose_eval=50)\n",
    "    # save feature importance\n",
    "    tmp = pd.DataFrame(lgb_model.feature_name(), columns=[\"feature_name\"])\n",
    "    tmp[f\"feature_importance_fold{cur_fold}\"] = lgb_model.feature_importance()\n",
    "    if gender_feat_import_df.empty:\n",
    "        gender_feat_import_df = tmp\n",
    "    else:\n",
    "        gender_feat_import_df = pd.merge(gender_feat_import_df, tmp, how=\"left\", on=\"feature_name\")\n",
    "    # get valid pred\n",
    "    val_pred = np.argmax(lgb_model.predict(\n",
    "        val_x, num_iteration=lgb_model.best_iteration), axis=1)\n",
    "    val_score = accuracy_score(val_pred, val_y)\n",
    "    # get test pred\n",
    "    gender_result_proba.append(lgb_model.predict(\n",
    "        test_feat, num_iteration=lgb_model.best_iteration))\n",
    "    gender_scores.append(val_score)\n",
    "    log(f\"current validation score: {val_score}\")\n",
    "    gc.collect()\n",
    "log(f\"accuracy score: {np.mean(gender_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_gender = np.argmax(np.mean(gender_result_proba, axis=0), axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K-Fold train costs] spend 4706.51 sec\n"
     ]
    }
   ],
   "source": [
    "tm_mgender.check(\"K-Fold train costs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training until validation scores don't improve for 100 rounds\n",
    "[50]\tvalid_0's multi_error: 0.0912667\n",
    "[100]\tvalid_0's multi_error: 0.08375\n",
    "[150]\tvalid_0's multi_error: 0.0806111\n",
    "[200]\tvalid_0's multi_error: 0.07855\n",
    "[250]\tvalid_0's multi_error: 0.0770611\n",
    "[300]\tvalid_0's multi_error: 0.07595\n",
    "[350]\tvalid_0's multi_error: 0.0752611\n",
    "[400]\tvalid_0's multi_error: 0.0747556\n",
    "[450]\tvalid_0's multi_error: 0.0741667\n",
    "[500]\tvalid_0's multi_error: 0.0737611\n",
    "[550]\tvalid_0's multi_error: 0.0736278\n",
    "[600]\tvalid_0's multi_error: 0.0735278\n",
    "[650]\tvalid_0's multi_error: 0.0733167\n",
    "[700]\tvalid_0's multi_error: 0.0730722\n",
    "[750]\tvalid_0's multi_error: 0.0729056\n",
    "[800]\tvalid_0's multi_error: 0.0729278\n",
    "[850]\tvalid_0's multi_error: 0.0728111\n",
    "[900]\tvalid_0's multi_error: 0.0726611\n",
    "[950]\tvalid_0's multi_error: 0.0726889\n",
    "[1000]\tvalid_0's multi_error: 0.0726556\n",
    "[1050]\tvalid_0's multi_error: 0.0725389\n",
    "[1100]\tvalid_0's multi_error: 0.0724444\n",
    "[1150]\tvalid_0's multi_error: 0.0723111\n",
    "[1200]\tvalid_0's multi_error: 0.0722722\n",
    "Early stopping, best iteration is:\n",
    "[1149]\tvalid_0's multi_error: 0.07225\n",
    "current validation score: 0.92775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(gender_scores) + np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb_multi_age = lgb.Booster(model_file=f\"{UMDIR}/lgb_multi_age_20200511045531.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb_multi_gender = lgb.Booster(model_file=f\"{UMDIR}/lgb_multi_gender_20200511034408.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model_lgb_multi_gender, max_num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model_lgb_multi_age, max_num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "res[\"predicted_age\"] = predicted_age\n",
    "# res[\"predicted_age\"] = [list(x).index(max(x))+1 for x in model_lgb_multi_age.predict(test_feat, num_iteration=model_lgb_multi_age.best_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "res[\"predicted_gender\"] = predicted_gender\n",
    "# res[\"predicted_gender\"] = [list(x).index(max(x))+1 for x in model_lgb_multi_gender.predict(test_feat, num_iteration=model_lgb_multi_gender.best_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_suffix = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))\n",
    "res.to_csv(f\"{RESDIR}/res-{res_suffix}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     303724\n",
       "2     164119\n",
       "4     144075\n",
       "5     142473\n",
       "6     118908\n",
       "7      54847\n",
       "1      23404\n",
       "8      22225\n",
       "9      16292\n",
       "10      9933\n",
       "Name: predicted_age, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    679154\n",
       "2    320846\n",
       "Name: predicted_gender, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(f\"{RESDIR}/res-20200516155349.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     305843\n",
       "2     165322\n",
       "4     141417\n",
       "5     140297\n",
       "6     116020\n",
       "7      55586\n",
       "1      24162\n",
       "8      23163\n",
       "9      17309\n",
       "10     10881\n",
       "Name: predicted_age, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    678320\n",
       "2    321680\n",
       "Name: predicted_gender, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cent result to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ti import session\n",
    "ti_session = session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ti_session.upload_data(path=f\"{RESDIR}/res-20200515004850.csv\", bucket=\"etveritas-1252104022\", key_prefix=RESDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
