{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if used pip install package\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install wget\n",
    "# !pip install gensim\n",
    "# !pip install catboost\n",
    "# !pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import wget\n",
    "import time\n",
    "import tarfile\n",
    "import zipfile\n",
    "import functools\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from itertools import product, combinations\n",
    "from scipy.special import comb, perm\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cbt\n",
    "from glove import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression,BayesianRidge,SGDClassifier,PassiveAggressiveClassifier,RidgeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC,NuSVC,SVC\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# import keras\n",
    "# import keras.backend as K\n",
    "# from keras import layers\n",
    "\n",
    "\n",
    "import gensim\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# ray.init(object_store_memory=int(100e6))\n",
    "# import modin.pandas as pd\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, filename='default.log', stream=sys.stdout):\n",
    "        self.terminal = stream\n",
    "        self.log = open(filename, 'a')\n",
    "        \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        \n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.stdout = Logger(\"logs/default.log\", sys.stdout)\n",
    "# sys.stderr = Logger(\"logs/default_err.log\", sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDIR = \"data\"\n",
    "UDDIR = \"user_data\"\n",
    "UFEDIR = \"user_data/feat_data_v05\"\n",
    "UMDIR = \"user_data/model_data\"\n",
    "RESDIR = \"prediction_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UID = \"user_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (Only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/train_preliminary.zip\", out=DDIR)\n",
    "# test_fname = wget.download(\"https://tesla-ap-shanghai-1256322946.cos.ap-shanghai.myqcloud.com/cephfs/tesla_common/deeplearning/dataset/algo_contest/test.zip\", out=DDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myunzip(filename):\n",
    "#     zFile = zipfile.ZipFile(filename, \"r\")\n",
    "#     for fileM in zFile.namelist(): \n",
    "#         zFile.extract(fileM, DDIR)\n",
    "#         print(fileM)\n",
    "#     zFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myunzip(train_fname)\n",
    "# myunzip(test_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bch_rencol(values, prefix=\"\", suffix=\"\"):\n",
    "    return list(map(lambda x: f\"{prefix}\"+\"_\".join(list(map(lambda y: str(y), x)))+f\"{suffix}\" \n",
    "                    if hasattr(x, \"__iter__\") and not isinstance(x, str) \n",
    "                    else f\"{prefix}\"+str(x)+f\"{suffix}\", values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynunique(values):\n",
    "    return values.nunique(dropna=False)\n",
    "def getidxmax(x):\n",
    "    return x.idxmax()[1]\n",
    "# for time series\n",
    "def at_len(x):\n",
    "    return len(x)\n",
    "\n",
    "def at_sum(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def at_max(x):\n",
    "    return np.max(x)\n",
    "\n",
    "def at_min(x):\n",
    "    return np.min(x)\n",
    "\n",
    "def at_mean(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def at_range(x):\n",
    "    return at_max(x) - at_min(x)\n",
    "\n",
    "def at_nunq(x):\n",
    "    return len(set(x))\n",
    "\n",
    "def at_lenDrange(x):\n",
    "    return at_len(x)/(at_range(x)+1)\n",
    "\n",
    "def at_lenDnunq(x):\n",
    "    return at_len(x)/at_nunq(x)\n",
    "\n",
    "def at_percentile(n):\n",
    "    def at_percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    at_percentile_.__name__ = f\"at_percentile_{n}\"\n",
    "    return at_percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_SET = [\"sum\", \"max\", \"min\", \"mean\", \"std\"]\n",
    "OP_SET1 = [\"nunique\", \"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"skew\", at_percentile(0.25), at_percentile(0.75)]\n",
    "OP_SET2 = [\"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"skew\", at_percentile(0.25), at_percentile(0.75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_SET = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\", \"time\", \"click_times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting_level = 0\n",
    "is_start = None\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        self.history = [self.start]\n",
    "\n",
    "    def check(self, info):\n",
    "        current = time.time()\n",
    "        print(f\"[{info}] spend {current - self.history[-1]:0.2f} sec\")\n",
    "        self.history.append(current)\n",
    "\n",
    "def log(entry):\n",
    "    global nesting_level\n",
    "    space = \"-\" * (4 * nesting_level)\n",
    "    print(f\"{space}{entry}\")\n",
    "\n",
    "def timeit(method, start_log=None):\n",
    "    @functools.wraps(method)\n",
    "    def timed(*args, **kw):\n",
    "        global is_start\n",
    "        global nesting_level\n",
    "\n",
    "        if not is_start:\n",
    "            print()\n",
    "\n",
    "        is_start = True\n",
    "        log(f\"Start [{method.__name__}]:\" + (start_log if start_log else \"\"))\n",
    "        log(f'Start time: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        nesting_level += 1\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        end_time = time.time()\n",
    "\n",
    "        nesting_level -= 1\n",
    "        log(f\"End   [{method.__name__}]. Time elapsed: {end_time - start_time:0.2f} sec.\")\n",
    "        is_start = False\n",
    "\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fname = sorted(os.listdir(UFEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = pd.read_csv(f\"{DDIR}/train_preliminary/user.csv\")\n",
    "test_user = pd.read_csv(f\"{DDIR}/test/user.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadnprec(col, max_features=None, maxlen=None, train_order=train_user[UID].values, test_order=test_user[UID].values):\n",
    "    train_seq = pd.read_pickle(f\"{UDDIR}/imd/train_{col}_seq.pkl\")\n",
    "    test_seq = pd.read_pickle(f\"{UDDIR}/imd/test_{col}_seq.pkl\")\n",
    "    \n",
    "    train_seq[col] = train_seq[col].apply(lambda x: \" \".join(x))\n",
    "    test_seq[col] = test_seq[col].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    train_seq.sort_index(inplace=True)\n",
    "    test_seq.sort_index(inplace=True)\n",
    "    \n",
    "    assert (train_seq.index.values != train_order).sum() == 0\n",
    "    assert (test_seq.index.values != test_order).sum() == 0\n",
    "    \n",
    "    train_X = train_seq[col].values\n",
    "    test_X = test_seq[col].values\n",
    "    \n",
    "    tol_X = np.concatenate([train_X, test_X])\n",
    "    \n",
    "    # Tokenize the sentences\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(tol_X))\n",
    "    \n",
    "    tol_X = tokenizer.texts_to_sequences(tol_X)\n",
    "\n",
    "    # Pad the sentences \n",
    "    tol_X = keras.preprocessing.sequence.pad_sequences(tol_X, maxlen=maxlen, padding=\"post\")\n",
    "    \n",
    "    train_X = tol_X[:len(train_seq)]\n",
    "    test_X = tol_X[len(train_seq):]\n",
    "    \n",
    "    return train_X, test_X, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(col, word_index, max_features):\n",
    "    \n",
    "    EMBEDDING_FILE = (f\"{UMDIR}/vectors/glove_{col}.model\")\n",
    "    k = Glove.load(EMBEDDING_FILE)\n",
    "    \n",
    "    embeddings_index = []\n",
    "    for i in tqdm(k.dictionary.keys()):\n",
    "        embeddings_index.append((i,k.word_vectors[k.dictionary[i]]))\n",
    "\n",
    "    embeddings_index = dict(pd.DataFrame(embeddings_index).values)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v(col, word_index, max_features):    \n",
    "    EMBEDDING_FILE = (f\"{UMDIR}/vectors/w2v512/w2v_{col}.model\")\n",
    "    embeddings_index = dict()\n",
    "    model = gensim.models.Word2Vec.load(EMBEDDING_FILE)\n",
    "    for word in model.wv.index2word:\n",
    "        embeddings_index[word] = model.wv[word]\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_index)+1)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_IDS = [\"creative_id\", \"ad_id\", \"product_id\", \"product_category\", \"advertiser_id\", \"industry\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 128\n",
    "embed_size = 512\n",
    "max_features = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_textprec_dic = dict()\n",
    "# test_textprec_dic = dict()\n",
    "# w2v_wordidx_dic = dict()\n",
    "# emb_matrix_dic = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(2020)\n",
    "# for cur_id in tqdm(USE_IDS):\n",
    "#     train_textprec_dic[cur_id], test_textprec_dic[cur_id], w2v_wordidx_dic[cur_id] = loadnprec(cur_id, max_features, maxlen)\n",
    "#     emb_matrix_dic[cur_id] = load_w2v(cur_id, w2v_wordidx_dic[cur_id], max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{UDDIR}/imd/train_textprec_dic_w2v{embed_size}_seq{maxlen}.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(train_textprec_dic), f)\n",
    "\n",
    "# with open(f\"{UDDIR}/imd/test_textprec_dic_w2v{embed_size}_seq{maxlen}.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(test_textprec_dic), f)\n",
    "\n",
    "# with open(f\"{UDDIR}/imd/w2v_wordidx_dic_w2v{embed_size}_seq{maxlen}.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(w2v_wordidx_dic), f)\n",
    "\n",
    "# with open(f\"{UDDIR}/imd/emb_matrix_dic_w2v{embed_size}_seq{maxlen}.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pickle.dumps(emb_matrix_dic), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{UDDIR}/imd/train_textprec_dic_w2v{embed_size}_seq{maxlen}.pkl\", \"rb\") as f:\n",
    "    train_textprec_dic = pickle.loads(pickle.load(f))\n",
    "    \n",
    "with open(f\"{UDDIR}/imd/test_textprec_dic_w2v{embed_size}_seq{maxlen}.pkl\", \"rb\") as f:\n",
    "    test_textprec_dic = pickle.loads(pickle.load(f))\n",
    "\n",
    "with open(f\"{UDDIR}/imd/w2v_wordidx_dic_w2v{embed_size}_seq{maxlen}.pkl\", \"rb\") as f:\n",
    "    w2v_wordidx_dic = pickle.loads(pickle.load(f))\n",
    "\n",
    "with open(f\"{UDDIR}/imd/emb_matrix_dic_w2v{embed_size}_seq{maxlen}.pkl\", \"rb\") as f:\n",
    "    emb_matrix_dic = pickle.loads(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fname = sorted(os.listdir(UFEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aux = pd.DataFrame(train_user[UID])\n",
    "test_aux = pd.DataFrame(test_user[UID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  tfidf_count_emb_age_all.pkl\n",
      "current filename:  tfidf_count_emb_gender_all.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"tfidf_count_emb_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        gender_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_aux = pd.merge(train_aux, gender_agg_pred, how=\"left\", on=UID)\n",
    "        test_aux = pd.merge(test_aux, gender_agg_pred, how=\"left\", on=UID)\n",
    "        gender_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  meta_age_group_regeress.pkl\n",
      "current filename:  meta_gender_group_regeress.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"meta_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        gender_agg_pred = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_aux = pd.merge(train_aux, gender_agg_pred, how=\"left\", on=UID)\n",
    "        test_aux = pd.merge(test_aux, gender_agg_pred, how=\"left\", on=UID)\n",
    "        gender_agg_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats cate target encode\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_catemlb\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_aux = pd.merge(train_aux, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_catemlb\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_aux = pd.merge(test_aux, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  test_stats_o1.pkl\n",
      "current filename:  train_stats_o1.pkl\n"
     ]
    }
   ],
   "source": [
    "# stats o1\n",
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"train_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        train_aux = pd.merge(train_aux, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "    elif fname.startswith(\"test_stats_o1\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        test_aux = pd.merge(test_aux, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats getidxmax\n",
    "# for fname in feat_fname:\n",
    "#     if fname.startswith(\"train_stats_o2_getidxmax\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         train_aux = pd.merge(train_aux, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)\n",
    "#     elif fname.startswith(\"test_stats_o2_getidxmax\"):\n",
    "#         print(\"current filename: \", fname)\n",
    "#         test_aux = pd.merge(test_aux, pd.read_pickle(f\"{UFEDIR}/{fname}\"), how=\"left\", on=UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current filename:  tfidf_svd_ad_id.pkl\n",
      "current filename:  tfidf_svd_advertiser_id.pkl\n",
      "current filename:  tfidf_svd_creative_id.pkl\n",
      "current filename:  tfidf_svd_industry.pkl\n",
      "current filename:  tfidf_svd_product_category.pkl\n",
      "current filename:  tfidf_svd_product_id.pkl\n"
     ]
    }
   ],
   "source": [
    "for fname in feat_fname:\n",
    "    if fname.startswith(\"tfidf_svd_\"):\n",
    "        print(\"current filename: \", fname)\n",
    "        cur_tfidf_svd = pd.read_pickle(f\"{UFEDIR}/{fname}\")\n",
    "        train_aux = pd.merge(train_aux, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "        test_aux = pd.merge(test_aux, cur_tfidf_svd, how=\"left\", on=UID)\n",
    "        cur_tfidf_svd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aux.set_index(UID, inplace=True)\n",
    "test_aux.set_index(UID, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_aux.columns == test_aux.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnaninf(val):\n",
    "    return np.mean(val[(~np.isnan(val))&(~np.isinf(val))].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillmean(df):\n",
    "    for col in tqdm(df.columns):\n",
    "        if df[col].count() < len(df):\n",
    "            df[col] = df[col].replace([np.nan, np.inf], nnaninf(df[col]))\n",
    "    # check\n",
    "    for col in tqdm(df.columns):\n",
    "        if df[col].count() < len(df):\n",
    "            log(col)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillmean_abs(df):\n",
    "    for col in tqdm(df.columns):\n",
    "        u = at_percentile(75)(df[col])\n",
    "        l = at_percentile(25)(df[col])\n",
    "        iqr = u - l\n",
    "        up_bound = u + 1.5*iqr\n",
    "        low_bound = l - 1.5*iqr\n",
    "        \n",
    "        df[col][df[col] < low_bound] = np.mean(df[col].values)\n",
    "        df[col][df[col] > up_bound] = np.mean(df[col].values)\n",
    "    # check\n",
    "    for col in tqdm(df.columns):\n",
    "        if df[col].count() < len(df):\n",
    "            log(col)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 485/485 [00:01<00:00, 340.78it/s]\n",
      "100%|██████████| 485/485 [00:01<00:00, 373.21it/s]\n",
      "100%|██████████| 485/485 [00:02<00:00, 240.21it/s]\n",
      "100%|██████████| 485/485 [00:01<00:00, 337.53it/s]\n"
     ]
    }
   ],
   "source": [
    "train_aux = fillmean(train_aux)\n",
    "test_aux = fillmean(test_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 485/485 [00:36<00:00, 13.36it/s]\n",
      "100%|██████████| 485/485 [00:01<00:00, 364.76it/s]\n",
      "100%|██████████| 485/485 [00:40<00:00, 12.05it/s]\n",
      "100%|██████████| 485/485 [00:01<00:00, 334.31it/s]\n"
     ]
    }
   ],
   "source": [
    "train_aux = fillmean_abs(train_aux)\n",
    "test_aux = fillmean_abs(test_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transmms(tr_df, te_df):\n",
    "    tol_df = pd.concat([tr_df, te_df])\n",
    "    mms = MinMaxScaler()\n",
    "    mms_tol_df = mms.fit_transform(tol_df)\n",
    "    mms_tr_df = mms_tol_df[:len(tr_df)]\n",
    "    mms_te_df = mms_tol_df[len(tr_df):]\n",
    "    \n",
    "    return mms_tr_df, mms_te_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_aux_scal, test_aux_scal = transmms(train_aux, test_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0000000000000002, 0.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aux_scal.max(), train_aux_scal.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0000000000000004, 0.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_aux_scal.max(), test_aux_scal.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_aux, test_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftAttention(object):\n",
    "    \"\"\"\n",
    "    Layer to compute local inference between two encoded sentences a and b.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        a = inputs[0]\n",
    "        b = inputs[1]\n",
    "\n",
    "        attention = layers.Lambda(self._attention,\n",
    "                                        output_shape = self._attention_output_shape,\n",
    "                                        arguments = None)(inputs)\n",
    "\n",
    "        align_a = layers.Lambda(self._soft_alignment,\n",
    "                                     output_shape = self._soft_alignment_output_shape,\n",
    "                                     arguments = None)([attention, a])\n",
    "        align_b = layers.Lambda(self._soft_alignment,\n",
    "                                     output_shape = self._soft_alignment_output_shape,\n",
    "                                     arguments = None)([attention, b])\n",
    "\n",
    "        return align_a, align_b\n",
    "\n",
    "    def _attention(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute the attention between elements of two sentences with the dot\n",
    "        product.\n",
    "        Args:\n",
    "            inputs: A list containing two elements, one for the first sentence\n",
    "                    and one for the second, both encoded by a BiLSTM.\n",
    "        Returns:\n",
    "            A tensor containing the dot product (attention weights between the\n",
    "            elements of the two sentences).\n",
    "        \"\"\"\n",
    "        attn_weights = K.batch_dot(x=inputs[0],\n",
    "                                   y=K.permute_dimensions(inputs[1],\n",
    "                                                          pattern=(0, 2, 1)))\n",
    "        return K.permute_dimensions(attn_weights, (0, 2, 1))\n",
    "\n",
    "    def _attention_output_shape(self, inputs):\n",
    "        input_shape = inputs[0]\n",
    "        embedding_size = input_shape[1]\n",
    "        return (input_shape[0], embedding_size, embedding_size)\n",
    "\n",
    "    def _soft_alignment(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute the soft alignment between the elements of two sentences.\n",
    "        Args:\n",
    "            inputs: A list of two elements, the first is a tensor of attention\n",
    "                    weights, the second is the encoded sentence on which to\n",
    "                    compute the alignments.\n",
    "        Returns:\n",
    "            A tensor containing the alignments.\n",
    "        \"\"\"\n",
    "        attention = inputs[0]\n",
    "        sentence = inputs[1]\n",
    "\n",
    "        # Subtract the max. from the attention weights to avoid overflows.\n",
    "        exp = K.exp(attention - K.max(attention, axis=-1, keepdims=True))\n",
    "        exp_sum = K.sum(exp, axis=-1, keepdims=True)\n",
    "        softmax = exp / exp_sum\n",
    "\n",
    "        return K.batch_dot(softmax, sentence)\n",
    "\n",
    "    def _soft_alignment_output_shape(self, inputs):\n",
    "        attention_shape = inputs[0]\n",
    "        sentence_shape = inputs[1]\n",
    "        return (attention_shape[0], attention_shape[1], sentence_shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esim(id1, id2):\n",
    "\n",
    "    inp_a = layers.Input(shape=(maxlen,))\n",
    "    inp_b = layers.Input(shape=(maxlen,))\n",
    "    \n",
    "    # embedding layer\n",
    "    embedded_a = layers.Embedding(len(emb_matrix_dic[id1]), embed_size, weights=[emb_matrix_dic[id1]], trainable=False)(inp_a)\n",
    "    embedded_b = layers.Embedding(len(emb_matrix_dic[id2]), embed_size, weights=[emb_matrix_dic[id2]], trainable=False)(inp_b)\n",
    "    \n",
    "    # Encoding Layer\n",
    "    bilstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.1))\n",
    "    \n",
    "    encoded_a = bilstm(embedded_a)\n",
    "    encoded_b = bilstm(embedded_b)\n",
    "    \n",
    "    # local inference layer \n",
    "    atten_a, atten_b = SoftAttention()([encoded_a, encoded_b])\n",
    "\n",
    "    sub_a_atten = layers.Lambda(lambda x: x[0]-x[1])([encoded_a, atten_a])\n",
    "    sub_b_atten = layers.Lambda(lambda x: x[0]-x[1])([encoded_b, atten_b])\n",
    "\n",
    "    mul_a_atten = layers.Lambda(lambda x: x[0]*x[1])([encoded_a, atten_a])\n",
    "    mul_b_atten = layers.Lambda(lambda x: x[0]*x[1])([encoded_b, atten_b])\n",
    "\n",
    "    m_a = layers.concatenate([encoded_a, atten_a, sub_a_atten, mul_a_atten])\n",
    "    m_b = layers.concatenate([encoded_b, atten_b, sub_b_atten, mul_b_atten])\n",
    "    \n",
    "    # Inference composition layer\n",
    "    composition_a = layers.Bidirectional(layers.LSTM(\n",
    "        128,\n",
    "        return_sequences=True,\n",
    "        dropout=0.1\n",
    "    ))(m_a)\n",
    "\n",
    "    avg_pool_a = layers.GlobalAveragePooling1D()(composition_a)\n",
    "    max_pool_a = layers.GlobalMaxPooling1D()(composition_a)\n",
    "\n",
    "    composition_b = layers.Bidirectional(layers.LSTM(\n",
    "        128,\n",
    "        return_sequences=True,\n",
    "        dropout=0.1\n",
    "    ))(m_b)\n",
    "\n",
    "    avg_pool_b = layers.GlobalAveragePooling1D()(composition_b)\n",
    "    max_pool_b = layers.GlobalMaxPooling1D()(composition_b)\n",
    "\n",
    "    pooled = layers.concatenate([avg_pool_a, max_pool_a, avg_pool_b, max_pool_b])\n",
    "    pooled = layers.Dropout(rate=0.1)(pooled)\n",
    "    \n",
    "    \n",
    "    conc = layers.Dense(256)(pooled)\n",
    "    conc = layers.BatchNormalization()(conc)\n",
    "    conc = layers.PReLU()(conc)\n",
    "    conc = layers.Dropout(0.2)(conc)\n",
    "\n",
    "    conc = layers.Dense(128)(conc)\n",
    "    conc = layers.BatchNormalization()(conc)\n",
    "    # conc = PReLU()(conc)\n",
    "\n",
    "    if num_classes == 1:\n",
    "        outp = layers.Dense(num_classes, activation=\"sigmoid\")(conc)\n",
    "    else:\n",
    "        outp = layers.Dense(num_classes, activation=\"softmax\")(conc)\n",
    "\n",
    "    model = keras.Model(inputs=[inp_a, inp_b], outputs=outp)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = \"creative_id\"\n",
    "id2 = \"ad_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = esim(id1, id2)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001), metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y = keras.utils.to_categorical(train_user[\"age\"]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [train_textprec_dic[id1], train_textprec_dic[id2]]\n",
    "test_set = [test_textprec_dic[id1], test_textprec_dic[id2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tr0, train_X_val0, train_X_tr1, train_X_val1, train_y_tr, train_y_val= train_test_split(train_set[0], train_set[1], train_y, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cb = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([train_X_tr0, train_X_tr1], train_y_tr, validation_data=([train_X_val0,train_X_val1], train_y_val), epochs=8, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['creative_id',\n",
       " 'ad_id',\n",
       " 'product_id',\n",
       " 'product_category',\n",
       " 'advertiser_id',\n",
       " 'industry']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = \"creative_id\"\n",
    "id2 = \"ad_id\"\n",
    "id3 = \"advertiser_id\"\n",
    "id4 = \"product_id\"\n",
    "id5 = \"industry\"\n",
    "# id6 = \"product_category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_1 = layers.Input(shape=(maxlen,))\n",
    "inp_2 = layers.Input(shape=(maxlen,))\n",
    "inp_3 = layers.Input(shape=(maxlen,))\n",
    "inp_4 = layers.Input(shape=(maxlen,))\n",
    "inp_5 = layers.Input(shape=(maxlen,))\n",
    "# inp_6 = layers.Input(shape=(maxlen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_1 = layers.Embedding(input_dim=len(emb_matrix_dic[id1]), output_dim=embed_size, weights=[emb_matrix_dic[id1]], trainable=False)(inp_1)\n",
    "embed_2 = layers.Embedding(input_dim=len(emb_matrix_dic[id2]), output_dim=embed_size, weights=[emb_matrix_dic[id2]], trainable=False)(inp_2)\n",
    "embed_3 = layers.Embedding(input_dim=len(emb_matrix_dic[id3]), output_dim=embed_size, weights=[emb_matrix_dic[id3]], trainable=False)(inp_3)\n",
    "embed_4 = layers.Embedding(input_dim=len(emb_matrix_dic[id4]), output_dim=embed_size, weights=[emb_matrix_dic[id4]], trainable=False)(inp_4)\n",
    "embed_5 = layers.Embedding(input_dim=len(emb_matrix_dic[id5]), output_dim=embed_size, weights=[emb_matrix_dic[id5]], trainable=False)(inp_5)\n",
    "# embed_6 = layers.Embedding(input_dim=len(emb_matrix_dic[id6]), output_dim=embed_size, weights=[emb_matrix_dic[id6]], trainable=False)(inp_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(embed_1)\n",
    "bilstm_1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(bilstm_1)\n",
    "bilstm_2 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(embed_2)\n",
    "bilstm_2 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(bilstm_2)\n",
    "bilstm_3 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(embed_3)\n",
    "bilstm_3 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(bilstm_3)\n",
    "bilstm_4 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(embed_4)\n",
    "bilstm_4 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(bilstm_4)\n",
    "bilstm_5 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(embed_5)\n",
    "bilstm_5 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(bilstm_5)\n",
    "# bilstm_6 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(embed_6)\n",
    "# bilstm_6 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(bilstm_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool_1 = layers.GlobalMaxPooling1D()(bilstm_1)\n",
    "maxpool_2 = layers.GlobalMaxPooling1D()(bilstm_2)\n",
    "maxpool_3 = layers.GlobalMaxPooling1D()(bilstm_3)\n",
    "maxpool_4 = layers.GlobalMaxPooling1D()(bilstm_4)\n",
    "maxpool_5 = layers.GlobalMaxPooling1D()(bilstm_5)\n",
    "# maxpool_6 = layers.GlobalMaxPooling1D()(bilstm_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atten_1 = Attention(64)(bilstm_1)\n",
    "# atten_2 = Attention(64)(bilstm_2)\n",
    "# atten_3 = Attention(64)(bilstm_3)\n",
    "# atten_4 = Attention(64)(bilstm_4)\n",
    "# atten_5 = Attention(64)(bilstm_5)\n",
    "# atten_6 = Attention(64)(bilstm_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_aux = layers.Input(shape=(train_aux_scal.shape[1], ))\n",
    "aux_1 = layers.Dense(32)(inp_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_list = [maxpool_1, maxpool_2, maxpool_3, maxpool_4, maxpool_5, aux_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = layers.concatenate(conc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = layers.Dense(256)(conc)\n",
    "conc = layers.BatchNormalization()(conc)\n",
    "conc = layers.PReLU()(conc)\n",
    "conc = layers.Dropout(0.2)(conc)\n",
    "\n",
    "conc = layers.Dense(128)(conc)\n",
    "conc = layers.BatchNormalization()(conc)\n",
    "\n",
    "out_age = layers.Dense(10, activation=\"softmax\", name=\"out_age\")(conc)\n",
    "out_gender = layers.Dense(1, activation=\"sigmoid\", name=\"out_gender\")(conc)\n",
    "\n",
    "model = keras.Model(inputs=[inp_1, inp_2, inp_3, inp_4, inp_5, inp_aux], outputs=[out_age, out_gender])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=0.0005\n",
    "model.compile(\n",
    "    loss={\"out_age\": \"categorical_crossentropy\", \"out_gender\": \"binary_crossentropy\"},\n",
    "    loss_weights={\"out_age\": 1, \"out_gender\": 1},\n",
    "    optimizer=tfa.optimizers.RectifiedAdam(lr=0.0005), \n",
    "    metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 128, 512)     25600000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 128, 512)     25600000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 128, 512)     25600000    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 128, 512)     19997696    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 128, 512)     170496      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 128, 256)     656384      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128, 256)     656384      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 128, 256)     656384      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 128, 256)     656384      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 128, 256)     656384      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 256)     394240      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 128, 256)     394240      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 128, 256)     394240      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 128, 256)     394240      bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 128, 256)     394240      bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 485)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 256)          0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           15552       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1312)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          336128      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu (PReLU)                 (None, 256)          256         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_age (Dense)                 (None, 10)           1290        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "out_gender (Dense)              (None, 1)            129         batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 102,609,099\n",
      "Trainable params: 5,640,139\n",
      "Non-trainable params: 96,968,960\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tfa.optimizers.RectifiedAdam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [train_textprec_dic[id1], train_textprec_dic[id2], train_textprec_dic[id3], train_textprec_dic[id4], train_textprec_dic[id5], train_aux_scal]\n",
    "test_set = [test_textprec_dic[id1], test_textprec_dic[id2], test_textprec_dic[id3], test_textprec_dic[id4], test_textprec_dic[id5], test_aux_scal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_age = keras.utils.to_categorical(train_user[\"age\"]-1)\n",
    "train_y_gender = (train_user[\"gender\"]-1).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_set = train_test_split(*train_set, train_y_age, train_y_gender, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tr = res_set[:-4:2]\n",
    "train_X_val = res_set[1:-4:2]\n",
    "train_y_tr_age = res_set[-4]\n",
    "train_y_val_age = res_set[-3]\n",
    "\n",
    "train_y_tr_gender = res_set[-2]\n",
    "train_y_val_gender = res_set[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "elsp = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "filepath = f\"{UMDIR}/weights_improvement_fold.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [elsp,]  # checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  752/22500 [>.............................] - ETA: 55:28 - loss: 2.3385 - out_age_loss: 2.0190 - out_gender_loss: 0.3194 - out_age_acc: 0.2924 - out_gender_acc: 0.8787"
     ]
    }
   ],
   "source": [
    "model.fit(train_X_tr, [train_y_tr_age, train_y_tr_gender], validation_data=(train_X_val, [train_y_val_age, train_y_val_gender]), epochs=3, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/20\n",
    "22500/22500 [==============================] - 2096s 93ms/step - loss: 1.6319 - out_age_loss: 1.4304 - out_gender_loss: 0.2015 - out_age_acc: 0.4164 - out_gender_acc: 0.9272 - val_loss: 1.4971 - val_out_age_loss: 1.3261 - val_out_gender_loss: 0.1710 - val_out_age_acc: 0.4511 - val_out_gender_acc: 0.9396\n",
    "Epoch 2/20\n",
    "22500/22500 [==============================] - 2092s 93ms/step - loss: 1.5176 - out_age_loss: 1.3389 - out_gender_loss: 0.1787 - out_age_acc: 0.4474 - out_gender_acc: 0.9367 - val_loss: 1.4781 - val_out_age_loss: 1.3116 - val_out_gender_loss: 0.1666 - val_out_age_acc: 0.4571 - val_out_gender_acc: 0.9408\n",
    "Epoch 3/20\n",
    "22500/22500 [==============================] - 2094s 93ms/step - loss: 1.4770 - out_age_loss: 1.3061 - out_gender_loss: 0.1709 - out_age_acc: 0.4607 - out_gender_acc: 0.9398 - val_loss: 1.4576 - val_out_age_loss: 1.2924 - val_out_gender_loss: 0.1652 - val_out_age_acc: 0.4647 - val_out_gender_acc: 0.9412\n",
    "Epoch 4/20\n",
    "22500/22500 [==============================] - 2099s 93ms/step - loss: 1.4410 - out_age_loss: 1.2765 - out_gender_loss: 0.1645 - out_age_acc: 0.4714 - out_gender_acc: 0.9421 - val_loss: 1.4564 - val_out_age_loss: 1.2911 - val_out_gender_loss: 0.1652 - val_out_age_acc: 0.4643 - val_out_gender_acc: 0.9415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{UMDIR}/5inp_2bilstm_model_emb512_seq128.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.model.load_model(filepath)\n",
    "# pred_res = model.predict(test_set, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_X_val, [train_y_val_age, train_y_val_gender])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res = model.predict(test_set, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4671 + 0.9411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.472 + 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers.modeling_tf_bert import (BertConfig, TFBertModel,\n",
    "                                           TFBertEmbeddings, TFBertMainLayer,\n",
    "                                           TFBertEncoder, TFBertAttention, \n",
    "                                           TFBertSelfAttention, TFBertLayer, TFBertPooler,\n",
    "                                           TFBertIntermediate, TFBertOutput,\n",
    "                                           TFBertForSequenceClassification, TFBertSelfOutput\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cts = pd.read_pickle(f\"{UDDIR}/imd/train_click_times_seq.pkl\")\n",
    "test_cts = pd.read_pickle(f\"{UDDIR}/imd/test_click_times_seq.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = keras.preprocessing.sequence.pad_sequences(train_cts[\"click_times\"].values, maxlen=maxlen, padding=\"post\")\n",
    "test_mask = keras.preprocessing.sequence.pad_sequences(test_cts[\"click_times\"].values, maxlen=maxlen, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position ids\n",
    "train_pos_ids = dict()\n",
    "for col in train_textprec_dic:\n",
    "    lines, cols = train_textprec_dic[col].shape\n",
    "    train_pos_ids[col] = np.array([list(range(cols))]*lines)\n",
    "\n",
    "test_pos_ids = dict()\n",
    "for col in test_textprec_dic:\n",
    "    lines, cols = test_textprec_dic[col].shape\n",
    "    test_pos_ids[col] = np.array([list(range(cols))]*lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token type ids\n",
    "train_token_ids = dict()\n",
    "for col in train_textprec_dic:\n",
    "    lines, cols = train_textprec_dic[col].shape\n",
    "    train_token_ids[col] = np.array([[0]*cols]*lines)\n",
    "\n",
    "test_token_ids = dict()\n",
    "for col in test_textprec_dic:\n",
    "    lines, cols = test_textprec_dic[col].shape\n",
    "    test_token_ids[col] = np.array([[0]*cols]*lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = \"creative_id\"\n",
    "id2 = \"ad_id\"\n",
    "id3 = \"advertiser_id\"\n",
    "id4 = \"product_id\"\n",
    "id5 = \"industry\"\n",
    "# id6 = \"product_category\"\n",
    "TFBS = [id1, id2, id4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## My own transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initializer(initializer_range=0.02):\n",
    "    \"\"\"Creates a `tf.initializers.truncated_normal` with the given range.\n",
    "    Args:\n",
    "        initializer_range: float, initializer range for stddev.\n",
    "    Returns:\n",
    "        TruncatedNormal initializer with stddev = `initializer_range`.\n",
    "    \"\"\"\n",
    "    return keras.initializers.TruncatedNormal(stddev=initializer_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_bool_to_primitive(bool_variable, default_tensor_to_true=False):\n",
    "    \"\"\"Function arguments can be inserted as boolean tensor\n",
    "        and bool variables to cope with keras serialization\n",
    "        we need to cast `output_attentions` to correct bool\n",
    "        if it is a tensor\n",
    "\n",
    "    Args:\n",
    "        default_tensor_to_true: bool, if tensor should default to True\n",
    "        in case tensor has no numpy attribute\n",
    "    \"\"\"\n",
    "    # if bool variable is tensor and has numpy value\n",
    "    if tf.is_tensor(bool_variable):\n",
    "        if hasattr(bool_variable, \"numpy\"):\n",
    "            return bool(bool_variable.numpy())\n",
    "        elif default_tensor_to_true:\n",
    "            return True\n",
    "\n",
    "    # else variable is bool\n",
    "    return bool_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\" Gaussian Error Linear Unit.\n",
    "    Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\n",
    "    return x * cdf\n",
    "\n",
    "\n",
    "def gelu_new(x):\n",
    "    \"\"\"Gaussian Error Linear Unit.\n",
    "    This is a smoother version of the RELU.\n",
    "    Original paper: https://arxiv.org/abs/1606.08415\n",
    "    Args:\n",
    "        x: float Tensor to perform activation.\n",
    "    Returns:\n",
    "        `x` with the GELU activation applied.\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * tf.sigmoid(x)\n",
    "\n",
    "ACT2FN = {\n",
    "    \"gelu\": tf.keras.layers.Activation(gelu),\n",
    "    \"relu\": tf.keras.activations.relu,\n",
    "    \"swish\": tf.keras.layers.Activation(swish),\n",
    "    \"gelu_new\": tf.keras.layers.Activation(gelu_new),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # 将 sin 应用于数组中的偶数索引（indices）；2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 将 cos 应用于数组中的奇数索引；2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyTFBertEmbedding(emb_matrix, config):\n",
    "    # input ids embedding\n",
    "    inp_ids = layers.Input(shape=(maxlen,))\n",
    "    inputs_embeds = layers.Embedding(input_dim=len(emb_matrix), output_dim=embed_size, weights=[emb_matrix], trainable=False)(inp_ids)\n",
    "    \n",
    "    # input position Embedding\n",
    "    inp_pos = layers.Input(shape=(maxlen,))\n",
    "    position_embeds = layers.Embedding(input_dim=config.max_position_embeddings, output_dim=embed_size, weights=positional_encoding(config.max_position_embeddings, embed_size), trainable=False)(inp_pos)\n",
    "    \n",
    "    # input token type Embedding\n",
    "    inp_token_type = layers.Input(shape=(maxlen,))\n",
    "    token_type_embeds = layers.Embedding(input_dim=config.type_vocab_size, output_dim=embed_size, embeddings_initializer=get_initializer(config.initializer_range))(inp_token_type)\n",
    "    \n",
    "    embeds = inputs_embeds + position_embeds + token_type_embeds\n",
    "    embeds = layers.LayerNormalization(epsilon=config.layer_norm_eps)(embeds)\n",
    "    embeds = layers.Dropout(config.hidden_dropout_prob)(embeds)\n",
    "    \n",
    "    ret = {\"inputs\": [inp_ids, inp_pos, inp_token_type],\n",
    "           \"embeddings\": embeds\n",
    "          }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder include encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range)\n",
    "        )\n",
    "        self.key = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range)\n",
    "        )\n",
    "        self.value = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range)\n",
    "        )\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
    "\n",
    "        batch_size = shape_list(hidden_states)[0]\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = tf.matmul(\n",
    "            query_layer, key_layer, transpose_b=True\n",
    "        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n",
    "        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\n",
    "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs, training=training)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = tf.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer, (batch_size, -1, self.all_head_size)\n",
    "        )  # (batch_size, seq_len_q, all_head_size)\n",
    "\n",
    "        outputs = (\n",
    "            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyTFBertAttention(inputs, config, training=True):\n",
    "    input_tensor, attention_mask, head_mask, output_attentions = inputs\n",
    "    self_attention = TFBertSelfAttention(config)\n",
    "    dense_output = TFBertSelfOutput(config)\n",
    "    \n",
    "    self_outputs = self_attention(\n",
    "        [input_tensor, attention_mask, head_mask, output_attentions], training=training\n",
    "    )\n",
    "    attention_output = dense_output([self_outputs[0], input_tensor], training=training)\n",
    "    outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyTFBertLayer(inputs, config, training=True):\n",
    "    hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
    "    \n",
    "    intermediate = TFBertIntermediate(config)\n",
    "    bert_output = TFBertOutput(config)\n",
    "    \n",
    "    attention_outputs = MyTFBertAttention([hidden_states, attention_mask, head_mask, output_attentions], config, training=training)\n",
    "    attention_output = attention_outputs[0]\n",
    "    \n",
    "    intermediate_output = intermediate(attention_output)\n",
    "    layer_output = bert_output([intermediate_output, attention_output], training=training)\n",
    "    outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyTFBertEncoder(inputs, config, training=True):\n",
    "    hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
    "    output_hidden_states = config.output_hidden_states\n",
    "    all_hidden_states = ()\n",
    "    all_attentions = ()\n",
    "    \n",
    "    for i in range(config.num_hidden_layers):\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        layer_outputs = MyTFBertLayer([hidden_states, attention_mask, head_mask[i], output_attentions], config, training=training)\n",
    "        hidden_states = layer_outputs[0]\n",
    "\n",
    "        if cast_bool_to_primitive(output_attentions) is True:\n",
    "            all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "    # Add last layer\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    outputs = (hidden_states,)\n",
    "    if output_hidden_states:\n",
    "        outputs = outputs + (all_hidden_states,)\n",
    "    if cast_bool_to_primitive(output_attentions) is True:\n",
    "        outputs = outputs + (all_attentions,)\n",
    "    return outputs  # outputs, (hidden states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyBertModel(pre_embeds):\n",
    "    config = BertConfig(\n",
    "        intermediate_size=256,\n",
    "        max_position_embeddings=maxlen,\n",
    "        num_attention_heads=5,\n",
    "        num_hidden_layers=6,\n",
    "        type_vocab_size=1,\n",
    "        vocab_size=len(pre_embeds),\n",
    "        hidden_size=embed_size\n",
    "    )\n",
    "    retbyemb = MyTFBertEmbedding(pre_embeds, config)\n",
    "    \n",
    "    inps = retbyemb[\"inputs\"]\n",
    "    embeds = retbyemb[\"embeddings\"]\n",
    "    \n",
    "    inp_atten_mask = layers.Input(shape=(1, 1, maxlen))\n",
    "    head_mask = [None] * config.num_hidden_layers\n",
    "    inputs = embeds, inp_atten_mask, head_mask, config.output_attentions\n",
    "    outputs = MyTFBertEncoder(inputs, config)\n",
    "    \n",
    "    ret = {\"inputs\": inps+[inp_atten_mask],\n",
    "           \"layers\": outputs\n",
    "          }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_1 = MyBertModel(emb_matrix_dic[id1])\n",
    "inps_1 = ret_1[\"inputs\"]\n",
    "bert_layer_1 = ret_1[\"layers\"]\n",
    "\n",
    "ret_2 = MyBertModel(emb_matrix_dic[id2])\n",
    "inps_2 = ret_2[\"inputs\"]\n",
    "bert_layer_2 = ret_2[\"layers\"]\n",
    "\n",
    "ret_3 = MyBertModel(emb_matrix_dic[id3])\n",
    "inps_3 = ret_3[\"inputs\"]\n",
    "bert_layer_3 = ret_3[\"layers\"]\n",
    "\n",
    "# ret_4 = MyBertModel(emb_matrix_dic[id4])\n",
    "# inps_4 = ret_4[\"inputs\"]\n",
    "# bert_layer_4 = ret_4[\"layers\"]\n",
    "\n",
    "# ret_5 = MyBertModel(emb_matrix_dic[id5])\n",
    "# inps_5 = ret_5[\"inputs\"]\n",
    "# bert_layer_5 = ret_5[\"layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_1 = layers.LSTM(128, return_sequences=True)(bert_layer_1[0])\n",
    "bilstm_2 = layers.LSTM(128, return_sequences=True)(bert_layer_2[0])\n",
    "bilstm_3 = layers.LSTM(128, return_sequences=True)(bert_layer_3[0])\n",
    "# bilstm_4 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(bert_layer_4[0])\n",
    "# bilstm_5 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(bert_layer_5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool_1 = layers.GlobalMaxPooling1D()(bilstm_1)\n",
    "maxpool_2 = layers.GlobalMaxPooling1D()(bilstm_2)\n",
    "maxpool_3 = layers.GlobalMaxPooling1D()(bilstm_3)\n",
    "# maxpool_4 = layers.GlobalMaxPooling1D()(bilstm_4)\n",
    "# maxpool_5 = layers.GlobalMaxPooling1D()(bilstm_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_list = [maxpool_1, maxpool_2, maxpool_3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = layers.concatenate(conc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conc = keras.layers.Dense(256)(conc)\n",
    "conc = keras.layers.BatchNormalization()(conc)\n",
    "conc = keras.layers.PReLU()(conc)\n",
    "conc = keras.layers.Dropout(0.2)(conc)\n",
    "\n",
    "conc = keras.layers.Dense(128)(conc)\n",
    "conc = keras.layers.BatchNormalization()(conc)\n",
    "\n",
    "out_age = keras.layers.Dense(10, activation=\"softmax\", name=\"out_age\")(conc)\n",
    "out_gender = keras.layers.Dense(1, activation=\"sigmoid\", name=\"out_gender\")(conc)\n",
    "\n",
    "model = keras.Model(inputs=inps_1+inps_2+inps_3, outputs=[out_age, out_gender])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss={\"out_age\": \"categorical_crossentropy\", \"out_gender\": \"binary_crossentropy\"},\n",
    "    loss_weights={\"out_age\": 1, \"out_gender\": 1},\n",
    "    optimizer=tfa.optimizers.RectifiedAdam(lr=0.001), \n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids, pos, token, mask\n",
    "train_set = list()\n",
    "test_set = list()\n",
    "for col in TFBS:\n",
    "    train_set += [train_textprec_dic[col], train_pos_ids[col], train_token_ids[col], train_mask.reshape(-1,1,1,maxlen)]\n",
    "    test_set += [test_textprec_dic[col], test_pos_ids[col], test_token_ids[col], test_mask.reshape(-1,1,1,maxlen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = [train_txtprec, train_txtprec, train_mask, train_mask.reshape(-1,1,1,maxlen)]\n",
    "# test_set = [test_txtprec, test_txtprec, train_mask, test_mask.reshape(-1,1,1,maxlen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_age = keras.utils.to_categorical(train_user[\"age\"]-1)\n",
    "train_y_gender = (train_user[\"gender\"]-1).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_set = train_test_split(*train_set, train_y_age, train_y_gender, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tr = res_set[:-4:2]\n",
    "train_X_val = res_set[1:-4:2]\n",
    "train_y_tr_age = res_set[-4]\n",
    "train_y_val_age = res_set[-3]\n",
    "\n",
    "train_y_tr_gender = res_set[-2]\n",
    "train_y_val_gender = res_set[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elsp = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "filepath = f\"{UMDIR}/weights_improvement_fold.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [elsp,]  # checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_X_tr, [train_y_tr_age, train_y_tr_gender], validation_data=(train_X_val, [train_y_val_age, train_y_val_gender]), epochs=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Transfomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position encoding\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # 将 sin 应用于数组中的偶数索引（indices）；2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 将 cos 应用于数组中的奇数索引；2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, emded_dim, embedding_matrix):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=len(embedding_matrix), output_dim=emded_dim, weights=[embedding_matrix])\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n",
    "        self.emded_dim = emded_dim\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "#         positions = positional_encoding(self.maxlen, self.emded_dim)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 64\n",
    "embed_size = 300\n",
    "max_features = 50000\n",
    "\n",
    "num_heads = 6  # Number of attention heads\n",
    "ff_dim = 64\n",
    "\n",
    "num_enc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = \"creative_id\"\n",
    "id2 = \"ad_id\"\n",
    "id3 = \"advertiser_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_1 = layers.Input(shape=(maxlen,))\n",
    "inp_2 = layers.Input(shape=(maxlen,))\n",
    "inp_3 = layers.Input(shape=(maxlen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_1 = TokenAndPositionEmbedding(maxlen, embed_size, emb_matrix_dic[id1])(inp_1)\n",
    "embed_2 = TokenAndPositionEmbedding(maxlen, embed_size, emb_matrix_dic[id1])(inp_2)\n",
    "embed_3 = TokenAndPositionEmbedding(maxlen, embed_size, emb_matrix_dic[id1])(inp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_1 = list()\n",
    "transf_2 = list()\n",
    "transf_3 = list()\n",
    "for i in range(num_enc):\n",
    "    if i == 0:\n",
    "        transf_1.append(TransformerBlock(embed_size, num_heads, ff_dim)(embed_1))\n",
    "        transf_2.append(TransformerBlock(embed_size, num_heads, ff_dim)(embed_2))\n",
    "        transf_3.append(TransformerBlock(embed_size, num_heads, ff_dim)(embed_3))\n",
    "    else:\n",
    "        transf_1.append(TransformerBlock(embed_size, num_heads, ff_dim)(transf_1[-1]))\n",
    "        transf_2.append(TransformerBlock(embed_size, num_heads, ff_dim)(transf_2[-1]))\n",
    "        transf_3.append(TransformerBlock(embed_size, num_heads, ff_dim)(transf_3[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilstm_1 = list()\n",
    "# bilstm_2 = list()\n",
    "# bilstm_3 = list()\n",
    "# for i in range(num_enc):\n",
    "#     bilstm_1.append(layers.LSTM(128, return_sequences=True)(transf_1[i]))\n",
    "#     bilstm_2.append(layers.LSTM(128, return_sequences=True)(transf_2[i]))\n",
    "#     bilstm_3.append(layers.LSTM(128, return_sequences=True)(transf_3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_1 = list()\n",
    "avgpool_2 = list()\n",
    "avgpool_3 = list()\n",
    "for i in range(num_enc):\n",
    "    avgpool_1.append(layers.GlobalAveragePooling1D()(transf_1[i]))\n",
    "    avgpool_2.append(layers.GlobalAveragePooling1D()(transf_2[i]))\n",
    "    avgpool_3.append(layers.GlobalAveragePooling1D()(transf_3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transf_1 = TransformerBlock(embed_size, num_heads, ff_dim)(embed_1)\n",
    "# transf_2 = TransformerBlock(embed_size, num_heads, ff_dim)(embed_2)\n",
    "# transf_3 = TransformerBlock(embed_size, num_heads, ff_dim)(embed_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avgpool_1 = layers.GlobalAveragePooling1D()(transf_1)\n",
    "# avgpool_2 = layers.GlobalAveragePooling1D()(transf_2)\n",
    "# avgpool_3 = layers.GlobalAveragePooling1D()(transf_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conc = layers.concatenate([avgpool_1, avgpool_2, avgpool_3])\n",
    "conc = layers.concatenate(avgpool_1+avgpool_2+avgpool_3)\n",
    "\n",
    "conc = layers.Dense(256)(conc)\n",
    "conc = layers.BatchNormalization()(conc)\n",
    "conc = layers.PReLU()(conc)\n",
    "conc = layers.Dropout(0.2)(conc)\n",
    "\n",
    "conc = layers.Dense(128)(conc)\n",
    "conc = layers.BatchNormalization()(conc)\n",
    "\n",
    "out_age = layers.Dense(10, activation=\"softmax\", name=\"out_age\")(conc)\n",
    "out_gender = layers.Dense(1, activation=\"sigmoid\", name=\"out_gender\")(conc)\n",
    "\n",
    "model = keras.Model(inputs=[inp_1, inp_2, inp_3,], outputs=[out_age, out_gender])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss={\"out_age\": \"categorical_crossentropy\", \"out_gender\": \"binary_crossentropy\"},\n",
    "    loss_weights={\"out_age\": 1, \"out_gender\": 1},\n",
    "    optimizer=tfa.optimizers.RectifiedAdam(learning_rate=0.01), \n",
    "    metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [train_textprec_dic[id1], train_textprec_dic[id2], train_textprec_dic[id3]]\n",
    "test_set = [test_textprec_dic[id1], test_textprec_dic[id2], test_textprec_dic[id3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_age = keras.utils.to_categorical(train_user[\"age\"]-1)\n",
    "train_y_gender = (train_user[\"gender\"]-1).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_set = train_test_split(*train_set, train_y_age, train_y_gender, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tr = res_set[:-4:2]\n",
    "train_X_val = res_set[1:-4:2]\n",
    "train_y_tr_age = res_set[-4]\n",
    "train_y_val_age = res_set[-3]\n",
    "\n",
    "train_y_tr_gender = res_set[-2]\n",
    "train_y_val_gender = res_set[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elsp = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "# filepath = f\"{UMDIR}/weights_improvement_fold.hdf5\"\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [elsp] #, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_X_tr, [train_y_tr_age, train_y_tr_gender], validation_data=(train_X_val, [train_y_val_age, train_y_val_gender]), epochs=5, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_num=10\n",
    "\n",
    "# weights = [1.0]*class_num\n",
    "\n",
    "\n",
    "\n",
    "# def search_weight(valid_y, raw_prob, init_weight=[1.0]*class_num, step=0.001):\n",
    "\n",
    "#     weight = init_weight.copy()\n",
    "\n",
    "#     f_best = accuracy_score(y_true=valid_y, y_pred=raw_prob.argmax(\n",
    "\n",
    "#         axis=1))\n",
    "\n",
    "#     flag_score = 0\n",
    "\n",
    "#     round_num = 1\n",
    "\n",
    "#     while(flag_score != f_best):\n",
    "\n",
    "#         print(\"round: \", round_num)\n",
    "\n",
    "#         round_num += 1\n",
    "\n",
    "#         flag_score = f_best\n",
    "\n",
    "#         for c in range(class_num):\n",
    "\n",
    "#             for n_w in range(0, 2000,10):\n",
    "\n",
    "#                 num = n_w * step\n",
    "\n",
    "#                 new_weight = weight.copy()\n",
    "\n",
    "#                 new_weight[c] = num\n",
    "\n",
    "\n",
    "\n",
    "#                 prob_df = raw_prob.copy()\n",
    "\n",
    "#                 prob_df = prob_df * np.array(new_weight)\n",
    "\n",
    "\n",
    "\n",
    "#                 f = accuracy_score(y_true=valid_y, y_pred=prob_df.argmax(\n",
    "\n",
    "#                     axis=1))\n",
    "\n",
    "#                 if f > f_best:\n",
    "\n",
    "#                     weight = new_weight.copy()\n",
    "\n",
    "#                     f_best = f\n",
    "\n",
    "#                     print(f)\n",
    "\n",
    "#     return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_user[[UID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res[\"predicted_age\"] = np.argmax(pred_res[0], axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"] = (pred_res[1] >= 0.5).astype(int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_suffix = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(int(time.time())))\n",
    "res.to_csv(f\"{RESDIR}/res-{res_suffix}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predicted_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cent result to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ti import session\n",
    "ti_session = session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ti_session.upload_data(path=f\"{RESDIR}/res-20200515004850.csv\", bucket=\"etveritas-1252104022\", key_prefix=RESDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "767px",
    "left": "28px",
    "top": "132px",
    "width": "355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
